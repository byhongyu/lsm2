{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoJ2L0H_j79I"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import tempfile\n",
        "import warnings\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google3.learning.deepmind.xmanager2.client import xmanager_api\n",
        "from google3.pyglib import gfile\n",
        "from google3.pyglib.function_utils import memoize\n",
        "from matplotlib import font_manager\n",
        "\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.lines as mlines\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib.ticker import FixedLocator  # Import for the fix\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from matplotlib.ticker import LogLocator\n",
        "from matplotlib.ticker import FuncFormatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcU8MDG2j7_8"
      },
      "outputs": [],
      "source": [
        "#@title Google Sans Import\n",
        "\n",
        "# Import Google font family\n",
        "_GOOGLE_SANS_PATH = (\n",
        "    'google3/third_party/googlefonts/api/googlerestricted/googlesans/'\n",
        ")\n",
        "\n",
        "@memoize.Memoize()\n",
        "def import_google3_fonts(font_path: str) -\u003e None:\n",
        "  \"\"\"Import fonts stored in google3 into Matplotlib for use in Colab.\n",
        "\n",
        "  Args:\n",
        "    font_path: google3 path to either a directory that contains .ttf fonts or to\n",
        "      a specific .ttf font file.\n",
        "  \"\"\"\n",
        "  if gfile.IsDirectory(font_path):\n",
        "    # Create a temp directory as a destination for copied font files.\n",
        "    tmp_dir = tempfile.mkdtemp()\n",
        "    # Copy font files from google3 to temp dir.\n",
        "    gfile.RecursivelyCopyDir(font_path, tmp_dir, overwrite=True)\n",
        "    # Add font files in directory to matplotlib font_manager.\n",
        "    font_files = font_manager.findSystemFonts(fontpaths=tmp_dir)\n",
        "  else:\n",
        "    # Assume the path points to a file if it's not a directory.\n",
        "    # Copy ttf file from google3 to temp location.\n",
        "    tmp_file = tempfile.NamedTemporaryFile(suffix='.ttf')\n",
        "    tmp_file.close()\n",
        "    gfile.Copy(font_path, tmp_file.name)\n",
        "    font_files = [tmp_file.name]\n",
        "\n",
        "  # Add fonts to default font manager.\n",
        "  for font_file in font_files:\n",
        "    font_manager.fontManager.addfont(font_file)\n",
        "\n",
        "\n",
        "def import_default_google_fonts() -\u003e None:\n",
        "  \"\"\"Register a set of default fonts (Roboto, Google Sans) with Matplotlib.\"\"\"\n",
        "  # Prepend google_src to google3 paths.\n",
        "  import_google3_fonts(os.path.join('/google_src/head/depot', _GOOGLE_SANS_PATH))\n",
        "\n",
        "\n",
        "# Import and register Google fonts with Matplotlib so we can use them.\n",
        "import_default_google_fonts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hzh4DG0fCRQO"
      },
      "outputs": [],
      "source": [
        "#@title Set up Plot Settings\n",
        "\n",
        "pd.set_option('display.max_rows', None)  # Show all rows\n",
        "pd.set_option('display.max_columns', None)  # Show all columns\n",
        "# Suppress specific warning\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
        "\n",
        "xm_client = xmanager_api.XManagerApi(xm_deployment_env='alphabet')\n",
        "MEDIUM_SIZE = 12\n",
        "mpl.rcParams.update({\n",
        "    'font.size': MEDIUM_SIZE,\n",
        "    'axes.labelsize': MEDIUM_SIZE,\n",
        "    'axes.titlesize': MEDIUM_SIZE,\n",
        "})\n",
        "plt.rcParams['font.size'] = MEDIUM_SIZE\n",
        "plt.rcParams['axes.linewidth'] = 1\n",
        "plt.rcParams['axes.edgecolor'] = '#777777'\n",
        "plt.rcParams['axes.facecolor'] = '#FFFFFF'\n",
        "plt.rc('font', size=MEDIUM_SIZE)  # controls default text sizes\n",
        "plt.rc('axes', titlesize=MEDIUM_SIZE)  # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=MEDIUM_SIZE)  # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=MEDIUM_SIZE)  # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=MEDIUM_SIZE-5)  # legend fontsize\n",
        "plt.rc('figure', titlesize=MEDIUM_SIZE)  # fontsize of the figure title\n",
        "\n",
        "elegant_palette = sns.color_palette('muted')\n",
        "mpl.rcParams['font.family'] = 'Google Sans'\n",
        "\n",
        "def log_float_formatter(y, pos):\n",
        "    return f'{y:.2f}'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBQR_rQhVhd4"
      },
      "outputs": [],
      "source": [
        "def read_xm_metrics(example_xid, metric_name, unit_id, lowest=True):\n",
        "  experiment = xm_client.get_experiment(example_xid)\n",
        "  work_unit = experiment.get_work_unit(unit_id)\n",
        "  all_series = work_unit.list_measurement_series()\n",
        "  # Read measurement series metadata.\n",
        "  for series in all_series:\n",
        "    if series.label == metric_name:\n",
        "      # Read measurement points data.\n",
        "      all_measurements = []\n",
        "      for measurement in series.measurements:\n",
        "        all_measurements.append(measurement.objective_value)\n",
        "      if lowest:\n",
        "        return min(all_measurements)\n",
        "      else:\n",
        "        return all_measurements\n",
        "\n",
        "\n",
        "def add_min_columns(df):\n",
        "  # Function to calculate the minimum value in each list\n",
        "  def min_of_list(lst):\n",
        "    return min(lst)\n",
        "\n",
        "  # Calculate minimum values and add as new columns\n",
        "  df['min_valid_mean_absolute_error_all'] = df[\n",
        "      'valid_mean_absolute_error_all'\n",
        "  ].apply(min_of_list)\n",
        "  df['min_valid_mean_absolute_error_masked'] = df[\n",
        "      'valid_mean_absolute_error_masked'\n",
        "  ].apply(min_of_list)\n",
        "  df['min_valid_mean_squared_error_all'] = df[\n",
        "      'valid_mean_squared_error_all'\n",
        "  ].apply(min_of_list)\n",
        "  df['min_valid_mean_squared_error_masked'] = df[\n",
        "      'valid_mean_squared_error_masked'\n",
        "  ].apply(min_of_list)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def process_string_metric(input_string):\n",
        "  # Define the mapping of long error names to their abbreviations\n",
        "  error_map = {'mean_absolute_error': 'mae', 'mean_squared_error': 'mse'}\n",
        "\n",
        "  # Replace the errors in the string using the map\n",
        "  for long_error, short_error in error_map.items():\n",
        "    input_string = re.sub(long_error, short_error, input_string)\n",
        "\n",
        "  # Remove 'valid_' and replace '/' with '_'\n",
        "  input_string = input_string.replace('valid_', '').replace('/', '_')\n",
        "\n",
        "  return input_string\n",
        "\n",
        "\n",
        "def generate_percentiled_numbers(max_value, percentiles):\n",
        "  \"\"\"Generate a list of integer numbers based on the given percentiles of the maximum value.\n",
        "\n",
        "  Parameters:\n",
        "  max_value (int): The maximum value to base the percentages on.\n",
        "  percentiles (list of float): A list of percentiles (0-100) to calculate.\n",
        "\n",
        "  Returns:\n",
        "  list of int: A list of integers corresponding to the given percentiles.\n",
        "  \"\"\"\n",
        "  return [round(max_value * (p / 100))-1 for p in percentiles]\n",
        "\n",
        "# Custom formatter function to display y-ticks as floats\n",
        "def log_float_formatter(y, pos):\n",
        "    return f'{y:.2f}'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6m9nh51kpZY"
      },
      "outputs": [],
      "source": [
        "# @title Data Scaling\n",
        "\n",
        "# Get unique learning rates\n",
        "\n",
        "\n",
        "xm_id_dict = {  # Model Size, ParamSize, PatchSize\n",
        "    124248449: ['Tiny', 2.21, '10x5'],\n",
        "    124248804: ['ExtraSmall', 7.3, '10x5'],\n",
        "    # 124142001: ['Small', 24.6, '10x5'],\n",
        "    124248847: ['Base', 110.74, '10x5'],\n",
        "    # 125769633: ['Large', 328.1, '10x5'],\n",
        "}\n",
        "\n",
        "compute_metrics = [\n",
        "    'core_hours_TPU v5 lite',\n",
        "    'examples_seen'\n",
        "]\n",
        "\n",
        "\n",
        "metric_names = [\n",
        "    # 'valid_mean_absolute_error_masked',\n",
        "    'valid_mean_squared_error_masked',\n",
        "    # 'forecast_0.2_eval/valid_mean_absolute_error_masked',\n",
        "    'forecast_0.2_eval/valid_mean_squared_error_masked',\n",
        "    # 'imputation_0.2_eval/valid_mean_absolute_error_masked',\n",
        "    'imputation_0.2_eval/valid_mean_squared_error_masked',\n",
        "\n",
        "]\n",
        "\n",
        "xm_exp_dict = collections.defaultdict(list)\n",
        "for key, values in xm_id_dict.items():\n",
        "  xm_id = key\n",
        "  model_size = values[0]\n",
        "  param_size = values[1]\n",
        "  patch_size = values[2]\n",
        "  experiment = xm_client.get_experiment(xm_id)\n",
        "  num_of_units = experiment.get_num_work_units()\n",
        "  for id in range(num_of_units):\n",
        "    real_id = id + 1\n",
        "    work_unit = experiment.get_work_unit(real_id)\n",
        "    key_list = work_unit.parameters.keys()\n",
        "    xm_exp_dict['unit_id'].append(id)\n",
        "    xm_exp_dict['xm_id'].append(xm_id)\n",
        "    xm_exp_dict['Param Size'].append(param_size)\n",
        "    xm_exp_dict['Model Size'].append(model_size)\n",
        "    xm_exp_dict['Patch Size'].append(patch_size)\n",
        "    for param_name in key_list:\n",
        "      xm_exp_dict[param_name].append(work_unit.parameters[param_name])\n",
        "    for metric in metric_names + compute_metrics:\n",
        "      xm_exp_dict[metric].append(\n",
        "          read_xm_metrics(xm_id, metric, real_id, lowest=False)\n",
        "      )\n",
        "default_df = pd.DataFrame(xm_exp_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbE-twsX7KTf"
      },
      "outputs": [],
      "source": [
        "df = default_df[default_df['config.dataset_configs.train_num_samples'] == 1321235]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtiU_mE4aCMt"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGp5SQWe7dWK"
      },
      "outputs": [],
      "source": [
        "custom_model_params_map = {\n",
        "    'Base': 110000000,\n",
        "    'ExtraSmall': 7000000,\n",
        "    'Tiny': 2000000,\n",
        "    'Large': 328000000,\n",
        "    'Small': 2400000,\n",
        "}\n",
        "df['model_params'] = df['Model Size'].map(custom_model_params_map)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sapAAmJ37qOe"
      },
      "outputs": [],
      "source": [
        "example_seen_list = df['examples_seen'].values\n",
        "valid_mse_list = df['valid_mean_squared_error_masked'].values\n",
        "model_params = df['model_params'].values\n",
        "model_names = {\n",
        "  2000000: 'ViT-2M',\n",
        "  7000000: 'ViT-7M',\n",
        "  110000000: 'ViT-110M'\n",
        "}\n",
        "# 3182BD, 6BAED6, 9ECAE1\n",
        "colors = {\n",
        "  110000000: '#3182BD',\n",
        "  7000000: '#6BAED6',\n",
        "  2000000: '#9ECAE1'\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 1, figsize=(3.3, 3), sharex=True, dpi=100)\n",
        "axes.set(xscale=\"log\", yscale=\"log\")\n",
        "# Define four distinct colors for the lines\n",
        "# colors = plt.cm.tab20c()\n",
        "\n",
        "# Loop through each row in the DataFrame and plot its corresponding line\n",
        "for i in range(len(df)):\n",
        "    samples_have_seen = np.array(example_seen_list[i], dtype=float)\n",
        "    valid_mse = np.array(valid_mse_list[i], dtype=float)\n",
        "    # Use modulo to cycle through the colors if there are more than 4 lines\n",
        "    color = plt.cm.tab20c(i)\n",
        "    plt.plot(\n",
        "        samples_have_seen[:],\n",
        "        valid_mse[1:],  # Skip the first point if needed\n",
        "        # label=f'{model_params[i]}',\n",
        "        label=model_names[model_params[i]],\n",
        "        color=colors[model_params[i]],\n",
        "    )\n",
        "\n",
        "# Set labels and title\n",
        "axes.yaxis.set_major_locator(LogLocator(base=10.0, subs=np.arange(1, 10), numticks=10))\n",
        "axes.yaxis.set_major_formatter(FuncFormatter(log_float_formatter))\n",
        "axes.xaxis.set_major_locator(LogLocator(base=10.0, subs=np.arange(1, 10), numticks=10))\n",
        "plt.xlabel('Samples Have Seen')\n",
        "plt.ylabel('Test Loss')\n",
        "plt.legend(frameon=False, fontsize=MEDIUM_SIZE-2)\n",
        "# Add the legend\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/tmp/model_efficiency_lsm.pdf\", bbox_inches='tight', format=\"pdf\")\n",
        "plt.show()\n",
        "%download_file /tmp/model_efficiency_lsm.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPQaFILOlE3o"
      },
      "outputs": [],
      "source": [
        "example_seen_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uYqxVR9bVit"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "lsm_model_efficiency_figure.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/iclr_figures/lsm_model_efficient_figures.ipynb",
          "timestamp": 1726865844262
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/iclr_figures/lsm_scaling_gen_tasks_figures.ipynb",
          "timestamp": 1726172749523
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/iclr_figures/lsm_scaling_gen_dis_tasks_figures.ipynb",
          "timestamp": 1726002208609
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/lsm_scaling_analys_final_iclr.ipynb?workspaceId=xliucs:scaling_analysis::citc",
          "timestamp": 1725479472885
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/lsm_scaling_analys_final_iclr.ipynb",
          "timestamp": 1723509130272
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/lsm_scaling_analys_data_size.ipynb?workspaceId=xliucs:scaling_analysis_new_pretrain::citc",
          "timestamp": 1723419354747
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/lsm_scaling_analys_data_size.ipynb",
          "timestamp": 1723071326500
        },
        {
          "file_id": "1Q3nbnc5dYAV6pyHWeKKXRVbxMwBPPQuB",
          "timestamp": 1719441633704
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/lsm_scaling_analys_data.ipynb",
          "timestamp": 1719435410567
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/lsm_scaling_analysis_parameter_sweep.ipynb?workspaceId=xliucs:lsm_notebooks_scaling_june22::citc",
          "timestamp": 1719177613159
        },
        {
          "file_id": "1avNG7EtMynPa2U-GHj2nd4nWWdGzcFq3",
          "timestamp": 1719177577595
        },
        {
          "file_id": "1WdhekZ_TCf-uOIWufr8RjKRHUcB0jzoZ",
          "timestamp": 1719170535908
        },
        {
          "file_id": "1rMS2cnwRlpP6NSfAA_mDuXFnbLEc6UnM",
          "timestamp": 1718735304309
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
