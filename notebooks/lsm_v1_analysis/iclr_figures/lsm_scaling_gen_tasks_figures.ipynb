{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2FpX1V_-Ts1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import tempfile\n",
        "import warnings\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google3.learning.deepmind.xmanager2.client import xmanager_api\n",
        "from google3.pyglib import gfile\n",
        "from google3.pyglib.function_utils import memoize\n",
        "from matplotlib import font_manager\n",
        "\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.lines as mlines\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib.ticker import FixedLocator  # Import for the fix\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from matplotlib.ticker import LogLocator\n",
        "from matplotlib.ticker import FuncFormatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qsPPQ4v-YKO"
      },
      "outputs": [],
      "source": [
        "#@title Google Sans Import\n",
        "\n",
        "# Import Google font family\n",
        "_GOOGLE_SANS_PATH = (\n",
        "    'google3/third_party/googlefonts/api/googlerestricted/googlesans/'\n",
        ")\n",
        "\n",
        "@memoize.Memoize()\n",
        "def import_google3_fonts(font_path: str) -\u003e None:\n",
        "  \"\"\"Import fonts stored in google3 into Matplotlib for use in Colab.\n",
        "\n",
        "  Args:\n",
        "    font_path: google3 path to either a directory that contains .ttf fonts or to\n",
        "      a specific .ttf font file.\n",
        "  \"\"\"\n",
        "  if gfile.IsDirectory(font_path):\n",
        "    # Create a temp directory as a destination for copied font files.\n",
        "    tmp_dir = tempfile.mkdtemp()\n",
        "    # Copy font files from google3 to temp dir.\n",
        "    gfile.RecursivelyCopyDir(font_path, tmp_dir, overwrite=True)\n",
        "    # Add font files in directory to matplotlib font_manager.\n",
        "    font_files = font_manager.findSystemFonts(fontpaths=tmp_dir)\n",
        "  else:\n",
        "    # Assume the path points to a file if it's not a directory.\n",
        "    # Copy ttf file from google3 to temp location.\n",
        "    tmp_file = tempfile.NamedTemporaryFile(suffix='.ttf')\n",
        "    tmp_file.close()\n",
        "    gfile.Copy(font_path, tmp_file.name)\n",
        "    font_files = [tmp_file.name]\n",
        "\n",
        "  # Add fonts to default font manager.\n",
        "  for font_file in font_files:\n",
        "    font_manager.fontManager.addfont(font_file)\n",
        "\n",
        "\n",
        "def import_default_google_fonts() -\u003e None:\n",
        "  \"\"\"Register a set of default fonts (Roboto, Google Sans) with Matplotlib.\"\"\"\n",
        "  # Prepend google_src to google3 paths.\n",
        "  import_google3_fonts(os.path.join('/google_src/head/depot', _GOOGLE_SANS_PATH))\n",
        "\n",
        "\n",
        "# Import and register Google fonts with Matplotlib so we can use them.\n",
        "import_default_google_fonts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hzh4DG0fCRQO"
      },
      "outputs": [],
      "source": [
        "#@title Set up Plot Settings\n",
        "\n",
        "pd.set_option('display.max_rows', None)  # Show all rows\n",
        "pd.set_option('display.max_columns', None)  # Show all columns\n",
        "# Suppress specific warning\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
        "\n",
        "xm_client = xmanager_api.XManagerApi(xm_deployment_env='alphabet')\n",
        "MEDIUM_SIZE = 12\n",
        "mpl.rcParams.update({\n",
        "    'font.size': MEDIUM_SIZE,\n",
        "    'axes.labelsize': MEDIUM_SIZE,\n",
        "    'axes.titlesize': MEDIUM_SIZE,\n",
        "})\n",
        "plt.rcParams['font.size'] = MEDIUM_SIZE\n",
        "plt.rcParams['axes.linewidth'] = 1\n",
        "plt.rcParams['axes.edgecolor'] = '#777777'\n",
        "plt.rcParams['axes.facecolor'] = '#FFFFFF'\n",
        "plt.rc('font', size=MEDIUM_SIZE)  # controls default text sizes\n",
        "plt.rc('axes', titlesize=MEDIUM_SIZE)  # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=MEDIUM_SIZE)  # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=MEDIUM_SIZE)  # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=MEDIUM_SIZE-5)  # legend fontsize\n",
        "plt.rc('figure', titlesize=MEDIUM_SIZE)  # fontsize of the figure title\n",
        "\n",
        "elegant_palette = sns.color_palette('muted')\n",
        "mpl.rcParams['font.family'] = 'Google Sans'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBQR_rQhVhd4"
      },
      "outputs": [],
      "source": [
        "def read_xm_metrics(example_xid, metric_name, unit_id, lowest=True):\n",
        "  experiment = xm_client.get_experiment(example_xid)\n",
        "  work_unit = experiment.get_work_unit(unit_id)\n",
        "  all_series = work_unit.list_measurement_series()\n",
        "  # Read measurement series metadata.\n",
        "  for series in all_series:\n",
        "    if series.label == metric_name:\n",
        "      # Read measurement points data.\n",
        "      all_measurements = []\n",
        "      for measurement in series.measurements:\n",
        "        all_measurements.append(measurement.objective_value)\n",
        "      if lowest:\n",
        "        return min(all_measurements)\n",
        "      else:\n",
        "        return all_measurements\n",
        "\n",
        "\n",
        "def add_min_columns(df):\n",
        "  # Function to calculate the minimum value in each list\n",
        "  def min_of_list(lst):\n",
        "    return min(lst)\n",
        "\n",
        "  # Calculate minimum values and add as new columns\n",
        "  df['min_valid_mean_absolute_error_all'] = df[\n",
        "      'valid_mean_absolute_error_all'\n",
        "  ].apply(min_of_list)\n",
        "  df['min_valid_mean_absolute_error_masked'] = df[\n",
        "      'valid_mean_absolute_error_masked'\n",
        "  ].apply(min_of_list)\n",
        "  df['min_valid_mean_squared_error_all'] = df[\n",
        "      'valid_mean_squared_error_all'\n",
        "  ].apply(min_of_list)\n",
        "  df['min_valid_mean_squared_error_masked'] = df[\n",
        "      'valid_mean_squared_error_masked'\n",
        "  ].apply(min_of_list)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def process_string_metric(input_string):\n",
        "  # Define the mapping of long error names to their abbreviations\n",
        "  error_map = {'mean_absolute_error': 'mae', 'mean_squared_error': 'mse'}\n",
        "\n",
        "  # Replace the errors in the string using the map\n",
        "  for long_error, short_error in error_map.items():\n",
        "    input_string = re.sub(long_error, short_error, input_string)\n",
        "\n",
        "  # Remove 'valid_' and replace '/' with '_'\n",
        "  input_string = input_string.replace('valid_', '').replace('/', '_')\n",
        "\n",
        "  return input_string\n",
        "\n",
        "\n",
        "def generate_percentiled_numbers(max_value, percentiles):\n",
        "  \"\"\"Generate a list of integer numbers based on the given percentiles of the maximum value.\n",
        "\n",
        "  Parameters:\n",
        "  max_value (int): The maximum value to base the percentages on.\n",
        "  percentiles (list of float): A list of percentiles (0-100) to calculate.\n",
        "\n",
        "  Returns:\n",
        "  list of int: A list of integers corresponding to the given percentiles.\n",
        "  \"\"\"\n",
        "  return [round(max_value * (p / 100))-1 for p in percentiles]\n",
        "\n",
        "# Custom formatter function to display y-ticks as floats\n",
        "def log_float_formatter(y, pos):\n",
        "    return f'{y:.2f}'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6m9nh51kpZY"
      },
      "outputs": [],
      "source": [
        "# @title Data Scaling\n",
        "\n",
        "# Get unique learning rates\n",
        "\n",
        "\n",
        "xm_id_dict = {  # Model Size, ParamSize, PatchSize\n",
        "    124248449: ['Tiny', 2.21, '10x5'],\n",
        "    124248804: ['ExtraSmall', 7.3, '10x5'],\n",
        "    # 124142001: ['Small', 24.6, '10x5'],\n",
        "    124248847: ['Base', 110.74, '10x5'],\n",
        "}\n",
        "\n",
        "compute_metrics = [\n",
        "    'core_hours_TPU v5 lite',\n",
        "]\n",
        "\n",
        "\n",
        "metric_names = [\n",
        "    # 'valid_mean_absolute_error_masked',\n",
        "    'valid_mean_squared_error_masked',\n",
        "    # 'forecast_0.2_eval/valid_mean_absolute_error_masked',\n",
        "    'forecast_0.2_eval/valid_mean_squared_error_masked',\n",
        "    # 'imputation_0.2_eval/valid_mean_absolute_error_masked',\n",
        "    'imputation_0.2_eval/valid_mean_squared_error_masked',\n",
        "\n",
        "]\n",
        "\n",
        "xm_exp_dict = collections.defaultdict(list)\n",
        "for key, values in xm_id_dict.items():\n",
        "  xm_id = key\n",
        "  model_size = values[0]\n",
        "  param_size = values[1]\n",
        "  patch_size = values[2]\n",
        "  experiment = xm_client.get_experiment(xm_id)\n",
        "  num_of_units = experiment.get_num_work_units()\n",
        "  for id in range(num_of_units):\n",
        "    real_id = id + 1\n",
        "    work_unit = experiment.get_work_unit(real_id)\n",
        "    key_list = work_unit.parameters.keys()\n",
        "    xm_exp_dict['unit_id'].append(id)\n",
        "    xm_exp_dict['xm_id'].append(xm_id)\n",
        "    xm_exp_dict['Param Size'].append(param_size)\n",
        "    xm_exp_dict['Model Size'].append(model_size)\n",
        "    xm_exp_dict['Patch Size'].append(patch_size)\n",
        "    for param_name in key_list:\n",
        "      xm_exp_dict[param_name].append(work_unit.parameters[param_name])\n",
        "    for metric in metric_names + compute_metrics:\n",
        "      xm_exp_dict[metric].append(\n",
        "          read_xm_metrics(xm_id, metric, real_id, lowest=False)\n",
        "      )\n",
        "df = pd.DataFrame(xm_exp_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC79G0dDDtKJ"
      },
      "outputs": [],
      "source": [
        "#@title Prepare the dataset\n",
        "\n",
        "def extract_min_last(df):\n",
        "  def process_column(col):\n",
        "    # Check if the column contains lists of floats\n",
        "    if col.apply(\n",
        "        lambda x: isinstance(x, list)\n",
        "        and all(isinstance(i, (float, int)) for i in x)\n",
        "    ).all():\n",
        "      # Extract the minimum and last values from the list\n",
        "      min_values = col.apply(lambda x: min(x) if x else None)\n",
        "      last_values = col.apply(lambda x: x[-1] if x else None)\n",
        "      return min_values, last_values\n",
        "    else:\n",
        "      return col, col\n",
        "\n",
        "  # Create new DataFrame with the extracted values\n",
        "  new_df = pd.DataFrame()\n",
        "\n",
        "  for column in df.columns:\n",
        "    min_col, last_col = process_column(df[column])\n",
        "    new_df[f'{column}_min'] = min_col\n",
        "    new_df[f'{column}_last'] = last_col\n",
        "\n",
        "  return new_df\n",
        "\n",
        "new_df = extract_min_last(df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5_1yw-31uRn"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1kMOHxtjf8w"
      },
      "outputs": [],
      "source": [
        "#@title Group by Model Size and Data Size\n",
        "new_metrics_names = []\n",
        "for metric_name in metric_names:\n",
        "  last_metric_name = f'{metric_name}_last'\n",
        "  new_metrics_names.append(last_metric_name)\n",
        "grouped_df = new_df.groupby(['config.dataset_configs.train_num_samples_min', 'Model Size_min'])[new_metrics_names]\n",
        "metrics_summary = grouped_df.mean()\n",
        "metrics_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BckKTi6bQ4IT"
      },
      "outputs": [],
      "source": [
        "# @title Data Scaling across Generative Tasks\n",
        "\n",
        "# Define the task names for subtitles\n",
        "task_names = [\n",
        "    'Random Imputation 80% \\n (Test Loss)',\n",
        "    'Temporal Extrapolation 60 Minutes \\n (Zero Shot)',\n",
        "    'Temporal Interpolation 60 Minutes \\n (Zero Shot)',\n",
        "    # '8-Class Activity Recognition',\n",
        "]  # Modify as needed\n",
        "\n",
        "# Reset the index if needed\n",
        "metrics_summary_reset = metrics_summary.reset_index()\n",
        "\n",
        "# Number of subplots (one per metric)\n",
        "n_metrics = 3\n",
        "\n",
        "# Custom colors for different model sizes\n",
        "model_colors = {\n",
        "    'Base': '#3182BD',\n",
        "    'ExtraSmall': '#6BAED6',\n",
        "    'Tiny': '#9ECAE1',\n",
        "}\n",
        "\n",
        "# Custom markers (shapes) for different model sizes\n",
        "model_shapes = {\n",
        "    'Base': 'o',  # Circle\n",
        "    'ExtraSmall': 'o',  # Square\n",
        "    'Tiny': 'o',  # Triangle\n",
        "}\n",
        "\n",
        "# Custom legend labels for different model sizes\n",
        "custom_legend_labels = {\n",
        "    'Base': 'ViT - 110M',\n",
        "    'ExtraSmall': 'ViT - 7M',\n",
        "    'Tiny': 'ViT - 2M',\n",
        "}\n",
        "\n",
        "data_size_marker_map = {\n",
        "    1000*5: 25,\n",
        "    10000*5: 50,\n",
        "    100000*5: 80,\n",
        "    750000*5: 120,\n",
        "    1321235*5: 150\n",
        "}\n",
        "# Create the figure and axes for the subplots\n",
        "fig, axes = plt.subplots(1, n_metrics, figsize=(10, 3), sharex=True, dpi=100)\n",
        "\n",
        "# Loop through each metric to plot the data\n",
        "for i, ax in enumerate(axes):\n",
        "  ax.set(xscale=\"log\", yscale=\"log\")\n",
        "  metric_last = new_metrics_names[i]  # Get the corresponding '_last' metric name\n",
        "\n",
        "  # Multiply x values by 5 and convert to log10 scale\n",
        "  # metrics_summary_reset['log_scaled_data_size'] = np.log10(\n",
        "  #     metrics_summary_reset['config.dataset_configs.train_num_samples_min'] * 5\n",
        "  # )\n",
        "  metrics_summary_reset['data_size_hours'] = metrics_summary_reset['config.dataset_configs.train_num_samples_min'] * 5\n",
        "\n",
        "  for model_size in model_colors.keys():\n",
        "      # Filter data for the current model size\n",
        "      subset = metrics_summary_reset[metrics_summary_reset['Model Size_min'] == model_size]\n",
        "\n",
        "      # Plot the line without markers first\n",
        "      sns.lineplot(\n",
        "          data=subset,\n",
        "          x='data_size_hours',\n",
        "          y=metric_last,\n",
        "          ax=ax,\n",
        "          color=model_colors[model_size],\n",
        "          linestyle='--',  # Dotted line\n",
        "          label=custom_legend_labels[model_size],  # Custom legend label\n",
        "          linewidth=1.0\n",
        "      )\n",
        "\n",
        "      # Add scatter plot for the markers with different sizes\n",
        "      sns.scatterplot(\n",
        "          data=subset,\n",
        "          x='data_size_hours',\n",
        "          y=metric_last,\n",
        "          ax=ax,\n",
        "          color=model_colors[model_size],\n",
        "          marker=model_shapes[model_size],\n",
        "          s=subset['data_size_hours'].map(data_size_marker_map),  # Map data size to marker size\n",
        "          legend=False  # Avoid adding duplicate legend entries\n",
        "      )\n",
        "\n",
        "  # Set the subtitle (task name) with light bold font\n",
        "  ax.set_title(task_names[i])  # Set the title to the task name\n",
        "  ax.set_xlabel('Data Size (Hours)')  # Update x-axis label to reflect log scaling\n",
        "  if i == 0:\n",
        "      ax.set_ylabel('Mean Squared Error')  # Set the y-axis label\n",
        "  else:\n",
        "      ax.set_ylabel('')\n",
        "  ax.yaxis.set_major_locator(LogLocator(base=10.0, subs=np.arange(1, 10), numticks=10))\n",
        "  ax.yaxis.set_major_formatter(FuncFormatter(log_float_formatter))\n",
        "  ax.get_legend().remove()\n",
        "\n",
        "# Create a list of handles for the custom legend (same shape but different colors)\n",
        "legend_handles = [\n",
        "    Line2D([0], [0], marker='o', color=model_colors[model_size], label=custom_legend_labels[model_size],\n",
        "           markerfacecolor=model_colors[model_size], markersize=10, markeredgecolor=None)\n",
        "    for model_size in model_colors.keys()\n",
        "]\n",
        "\n",
        "# Move the legend to the bottom of the figure and only show the custom colored circles\n",
        "fig.legend(handles=legend_handles[::-1], loc='lower center', bbox_to_anchor=(0.5, -0.2), ncol=3, fontsize=MEDIUM_SIZE-2, frameon=False)\n",
        "\n",
        "# Adjust layout for better display\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(bottom=0.1)  # Make space for the legend at the bottom\n",
        "plt.savefig(\"/tmp/lsm_gen_task_data_scaling.pdf\", bbox_inches='tight', format=\"pdf\")\n",
        "plt.show()\n",
        "%download_file /tmp/lsm_gen_task_data_scaling.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGxmwWwXHwsm"
      },
      "outputs": [],
      "source": [
        "# @title Model Scaling across Generative Tasks\n",
        "\n",
        "# Define the task names for subtitles\n",
        "task_names = [\n",
        "    'Random Imputation 80% \\n (Test Loss)',\n",
        "    'Temporal Extrapolation 60 Minutes \\n (Zero Shot)',\n",
        "    'Temporal Interpolation 60 Minutes \\n (Zero Shot)',\n",
        "    # '8-Class Activity Recognition',\n",
        "]  # Modify as needed\n",
        "\n",
        "# Reset the index if needed\n",
        "metrics_summary_reset = metrics_summary.reset_index()\n",
        "\n",
        "# Number of subplots (one per metric)\n",
        "n_metrics = 3\n",
        "\n",
        "\n",
        "model_colors = {\n",
        "    'Base': '#3182BD',\n",
        "    'ExtraSmall': '#6BAED6',\n",
        "    'Tiny': '#9ECAE1',\n",
        "    'Deb': 'purple',\n",
        "    'Small': 'orange',\n",
        "    'Large': 'cyan',\n",
        "}\n",
        "\n",
        "# Custom colors for different model sizes\n",
        "data_colors = {\n",
        "    1000: '#3182BD',\n",
        "    10000: '#3182BD',\n",
        "    100000: '#3182BD',\n",
        "    750000: '#3182BD',\n",
        "    1321235: '#3182BD',\n",
        "}\n",
        "\n",
        "# Custom markers (shapes) for different model sizes\n",
        "data_shapes = {\n",
        "    1000: 'o',  # Circle\n",
        "    10000: 'o',  # Square\n",
        "    100000: 'o',  # Triangle\n",
        "    750000: 'o',  # Plus\n",
        "    1321235: 'o',  # Diamond\n",
        "}\n",
        "\n",
        "# Custom legend labels for different model sizes\n",
        "custom_legend_labels = {\n",
        "    1000: '0.005 M',  # Circle\n",
        "    10000: '0.05 M',  # Square\n",
        "    100000: '0.5 M',  # Triangle\n",
        "    750000: '3.8 M',  # Plus\n",
        "    1321235: '6.6 M',  # Diamond\n",
        "}\n",
        "\n",
        "data_size_marker_map = {\n",
        "    1000: 25,\n",
        "    10000: 50,\n",
        "    100000: 80,\n",
        "    750000: 120,\n",
        "    1321235: 150\n",
        "}\n",
        "\n",
        "\n",
        "custom_model_params_map = {\n",
        "    'Base': 110000000,\n",
        "    'ExtraSmall': 7000000,\n",
        "    'Tiny': 2000000,\n",
        "}\n",
        "\n",
        "metrics_summary_reset['Model Params'] = metrics_summary_reset[\n",
        "    'Model Size_min'\n",
        "].map(custom_model_params_map)\n",
        "\n",
        "# Create the figure and axes for the subplots\n",
        "fig, axes = plt.subplots(1, n_metrics, figsize=(10, 3), sharex=True, dpi=100)\n",
        "\n",
        "# Loop through each metric to plot the data\n",
        "for i, ax in enumerate(axes):\n",
        "  ax.set(xscale=\"log\", yscale=\"log\")\n",
        "  metric_last = new_metrics_names[\n",
        "      i\n",
        "  ]  # Get the corresponding '_last' metric name\n",
        "  # Loop through model sizes to plot each one with its own color and shape\n",
        "  for data_size in data_colors.keys():\n",
        "    # Filter data for the current model size\n",
        "    subset = metrics_summary_reset[\n",
        "        metrics_summary_reset['config.dataset_configs.train_num_samples_min']\n",
        "        == data_size\n",
        "    ]\n",
        "\n",
        "    # Plot the data with custom color and shape\n",
        "    sns.lineplot(\n",
        "        data=subset,\n",
        "        x='Model Params',\n",
        "        y=metric_last,\n",
        "        ax=ax,\n",
        "        color=data_colors[data_size],\n",
        "        label=custom_legend_labels[data_size],  # Custom legend label\n",
        "        linestyle='--',  # Dotted line\n",
        "        linewidth=1.0\n",
        "    )\n",
        "\n",
        "    sns.scatterplot(\n",
        "          data=subset,\n",
        "          x='Model Params',\n",
        "          y=metric_last,\n",
        "          ax=ax,\n",
        "          color=subset['Model Size_min'].map(model_colors),\n",
        "          marker=data_shapes[data_size],\n",
        "          s=subset['config.dataset_configs.train_num_samples_min'].map(data_size_marker_map),  # Map data size to marker size\n",
        "          legend=False  # Avoid adding duplicate legend entries\n",
        "    )\n",
        "\n",
        "    # Set the subtitle (task name) with light bold font\n",
        "    ax.set_title(task_names[i], fontweight='medium')  # Set the title to the task name\n",
        "    ax.set_xlabel('Model Parameters')  # Update x-axis label to reflect log scaling\n",
        "    if i == 0:\n",
        "        ax.set_ylabel('Mean Squared Error')  # Set the y-axis label\n",
        "    else:\n",
        "        ax.set_ylabel('')\n",
        "\n",
        "  ax.yaxis.set_major_locator(LogLocator(base=10.0, subs=np.arange(1, 10), numticks=10))\n",
        "  ax.yaxis.set_major_formatter(FuncFormatter(log_float_formatter))\n",
        "  ax.get_legend().remove()\n",
        "\n",
        "# Create a list of handles for the custom legend (same color but different sizes)\n",
        "legend_handles = [\n",
        "    Line2D([0], [0], marker='o', color='black', label=custom_legend_labels[data_size],\n",
        "           markerfacecolor='black', markersize=np.sqrt(data_size_marker_map[data_size]*0.7),\n",
        "           markeredgecolor=None) for data_size in data_colors.keys()\n",
        "]\n",
        "\n",
        "# Move the legend to the bottom of the figure and only show the custom circle sizes legend\n",
        "fig.legend(handles=legend_handles, loc='lower center', bbox_to_anchor=(0.5, -0.2), ncol=5, fontsize=MEDIUM_SIZE-2, frameon=False)\n",
        "\n",
        "# Adjust layout for better display\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(bottom=0.1)  # Make space for the legend at the bottom\n",
        "plt.savefig(\"/tmp/lsm_gen_task_model_scaling.pdf\", bbox_inches='tight', format=\"pdf\")\n",
        "plt.show()\n",
        "%download_file /tmp/lsm_gen_task_model_scaling.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "213lt1P8KTnh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "lsm_scaling_gen_tasks_figures.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/iclr_figures/lsm_scaling_gen_tasks_figures.ipynb",
          "timestamp": 1727665579284
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/iclr_figures/lsm_scaling_gen_tasks_figures.ipynb",
          "timestamp": 1726865101784
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/iclr_figures/lsm_scaling_gen_tasks_figures.ipynb",
          "timestamp": 1726601384149
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/iclr_figures/lsm_scaling_gen_dis_tasks_figures.ipynb",
          "timestamp": 1726002208609
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/lsm_scaling_analys_final_iclr.ipynb?workspaceId=xliucs:scaling_analysis::citc",
          "timestamp": 1725479472885
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/lsm_scaling_analys_final_iclr.ipynb",
          "timestamp": 1723509130272
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/lsm_scaling_analys_data_size.ipynb?workspaceId=xliucs:scaling_analysis_new_pretrain::citc",
          "timestamp": 1723419354747
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/lsm_scaling_analys_data_size.ipynb",
          "timestamp": 1723071326500
        },
        {
          "file_id": "1Q3nbnc5dYAV6pyHWeKKXRVbxMwBPPQuB",
          "timestamp": 1719441633704
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/lsm_scaling_analys_data.ipynb",
          "timestamp": 1719435410567
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/lsm_scaling_analysis_parameter_sweep.ipynb?workspaceId=xliucs:lsm_notebooks_scaling_june22::citc",
          "timestamp": 1719177613159
        },
        {
          "file_id": "1avNG7EtMynPa2U-GHj2nd4nWWdGzcFq3",
          "timestamp": 1719177577595
        },
        {
          "file_id": "1WdhekZ_TCf-uOIWufr8RjKRHUcB0jzoZ",
          "timestamp": 1719170535908
        },
        {
          "file_id": "1rMS2cnwRlpP6NSfAA_mDuXFnbLEc6UnM",
          "timestamp": 1718735304309
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
