{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hzh4DG0fCRQO"
      },
      "outputs": [],
      "source": [
        "from google3.learning.deepmind.xmanager2.client import xmanager_api\n",
        "import matplotlib as mpl\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import collections\n",
        "\n",
        "xm_client = xmanager_api.XManagerApi(xm_deployment_env='alphabet')\n",
        "MEDIUM_SIZE = 14\n",
        "mpl.rcParams.update({\n",
        "    'font.size': MEDIUM_SIZE,\n",
        "    'axes.labelsize': MEDIUM_SIZE,\n",
        "    'axes.titlesize': MEDIUM_SIZE,\n",
        "})\n",
        "mpl.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['font.size'] = 20\n",
        "plt.rcParams['axes.linewidth'] = 2\n",
        "plt.rcParams['axes.edgecolor'] = '#777777'\n",
        "plt.rcParams['axes.facecolor'] = '#FFFFFF'\n",
        "\n",
        "SMALL_SIZE = 16\n",
        "MEDIUM_SIZE = 18\n",
        "BIGGER_SIZE = 20\n",
        "\n",
        "plt.rc('font', size=MEDIUM_SIZE)  # controls default text sizes\n",
        "plt.rc('axes', titlesize=MEDIUM_SIZE)  # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=MEDIUM_SIZE)  # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=MEDIUM_SIZE)  # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=MEDIUM_SIZE)  # legend fontsize\n",
        "plt.rc('figure', titlesize=MEDIUM_SIZE)  # fontsize of the figure title\n",
        "\n",
        "elegant_palette = sns.color_palette('muted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBQR_rQhVhd4"
      },
      "outputs": [],
      "source": [
        "def read_xm_metrics(example_xid, metric_name, unit_id, lowest=True):\n",
        "  experiment = xm_client.get_experiment(example_xid)\n",
        "  work_unit = experiment.get_work_unit(unit_id)\n",
        "  all_series = work_unit.list_measurement_series()\n",
        "  # Read measurement series metadata.\n",
        "  for series in all_series:\n",
        "    if series.label == metric_name:\n",
        "      # Read measurement points data.\n",
        "      all_measurements = []\n",
        "      for measurement in series.measurements:\n",
        "        all_measurements.append(measurement.objective_value)\n",
        "      if lowest:\n",
        "        return min(all_measurements)\n",
        "      else:\n",
        "        return all_measurements\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6m9nh51kpZY"
      },
      "outputs": [],
      "source": [
        "# @title Data Scaling\n",
        "\n",
        "# Get unique learning rates\n",
        "\n",
        "xm_id = 117136753\n",
        "xm_id_dict = {\n",
        "    'Large': [117136753, 328.13],\n",
        "    'Base': [117137176, 110.74],\n",
        "    'Small': [117137094, 24.59],\n",
        "    'Tiny': [117137514, 2.22],\n",
        "    }\n",
        "\n",
        "metric_names = [\n",
        "    'valid_mean_absolute_error_all',\n",
        "    'valid_mean_absolute_error_masked',\n",
        "    'valid_mean_squared_error_all',\n",
        "    'valid_mean_squared_error_masked',\n",
        "]\n",
        "xm_exp_dict = collections.defaultdict(list)\n",
        "for key, values in xm_id_dict.items():\n",
        "  xm_id = values[0]\n",
        "  param_size = values[1]\n",
        "  experiment = xm_client.get_experiment(xm_id)\n",
        "  num_of_units = experiment.get_num_work_units()\n",
        "  for id in range(num_of_units):\n",
        "    real_id = id + 1\n",
        "    work_unit = experiment.get_work_unit(real_id)\n",
        "    key_list = work_unit.parameters.keys()\n",
        "    xm_exp_dict['unit_id'].append(id)\n",
        "    xm_exp_dict['xm_id'].append(xm_id)\n",
        "    xm_exp_dict['Param Size'].append(param_size)\n",
        "    xm_exp_dict['Model Size'].append(key)\n",
        "    for param_name in key_list:\n",
        "      xm_exp_dict[param_name].append(work_unit.parameters[param_name])\n",
        "    for metric in metric_names:\n",
        "      xm_exp_dict[metric].append(\n",
        "          read_xm_metrics(xm_id, metric, real_id, lowest=False)\n",
        "      )\n",
        "df = pd.DataFrame(xm_exp_dict)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W87ASfU1bPF7"
      },
      "outputs": [],
      "source": [
        "metric_name = 'valid_mean_absolute_error_all'\n",
        "metric_name_short = 'MAE All Patches'\n",
        "sample_size = 5_000_000\n",
        "compute_hours = [0, 7000, 12500, 18750, 25000, 50000, 75000, 100000]\n",
        "\n",
        "# Create a figure with three subplots in a row\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6), dpi=600)\n",
        "\n",
        "# Figure 1: Compute Scaling\n",
        "subset = df[\n",
        "    (df['config.dataset_configs.train_num_samples'] == sample_size)\n",
        "    \u0026 (df['Model Size'] == 'Large')\n",
        "]\n",
        "\n",
        "num_of_logging = len(subset.iloc[0][metric_name])\n",
        "logging_in_steps = [\n",
        "    int((idx - 1) / 100000 * num_of_logging) for idx in compute_hours\n",
        "]\n",
        "x = [int(x / 250) for x in compute_hours]\n",
        "y = [subset.iloc[0][metric_name][idx] for idx in logging_in_steps]\n",
        "\n",
        "sns.barplot(x=x, y=y[: len(x)], palette='Blues', ax=axes[0])\n",
        "axes[0].set_title('Compute Scaling')\n",
        "axes[0].set_xlabel(r'$\\mathbf{Compute}$' + '\\n TPU v5 VLP core hours')\n",
        "axes[0].set_ylabel(metric_name_short)\n",
        "\n",
        "# Figure 2: Data Scaling\n",
        "subset = df[(df['Model Size'] == 'Large')]\n",
        "x = [\n",
        "    row['config.dataset_configs.train_num_samples'] / 1_000_000 * 5\n",
        "    for _, row in subset.iterrows()\n",
        "]\n",
        "y = [min(row[metric_name]) for _, row in subset.iterrows()]\n",
        "\n",
        "sns.barplot(x=x, y=y, palette='Blues', ax=axes[1])\n",
        "axes[1].set_title('Data Scaling')\n",
        "axes[1].set_xlabel(r'$\\mathbf{Data\\ Size}$' + '\\n(Million Hours)')\n",
        "\n",
        "# Figure 3: Model Scaling\n",
        "subset = df[(df['config.dataset_configs.train_num_samples'] == sample_size)]\n",
        "x = [row['Param Size'] for _, row in subset.iterrows()]\n",
        "y = [min(row[metric_name]) for _, row in subset.iterrows()]\n",
        "\n",
        "sns.barplot(x=x, y=y, palette='Blues', ax=axes[2])\n",
        "axes[2].set_title('Model Size Scaling')\n",
        "axes[2].set_xlabel(r'$\\mathbf{Model\\ Size}$' + '\\n(Million of Params)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTaPMBXNdWBr"
      },
      "outputs": [],
      "source": [
        "metric_name = 'valid_mean_absolute_error_masked'\n",
        "metric_name_short = 'MAE Masked Patches'\n",
        "sample_size = 5_000_000\n",
        "compute_hours = [0, 7000, 12500, 18750, 25000, 50000, 75000, 100000]\n",
        "\n",
        "# Create a figure with three subplots in a row\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6), dpi=600)\n",
        "\n",
        "# Figure 1: Compute Scaling\n",
        "subset = df[\n",
        "    (df['config.dataset_configs.train_num_samples'] == sample_size)\n",
        "    \u0026 (df['Model Size'] == 'Large')\n",
        "]\n",
        "\n",
        "num_of_logging = len(subset.iloc[0][metric_name])\n",
        "logging_in_steps = [\n",
        "    int((idx - 1) / 100000 * num_of_logging) for idx in compute_hours\n",
        "]\n",
        "x = [int(x / 250) for x in compute_hours]\n",
        "y = [subset.iloc[0][metric_name][idx] for idx in logging_in_steps]\n",
        "\n",
        "sns.barplot(x=x, y=y[: len(x)], palette='Blues', ax=axes[0])\n",
        "axes[0].set_title('Compute Scaling')\n",
        "axes[0].set_xlabel(r'$\\mathbf{Compute}$' + '\\n TPU v5 VLP core hours')\n",
        "axes[0].set_ylabel(metric_name_short)\n",
        "\n",
        "# Figure 2: Data Scaling\n",
        "subset = df[(df['Model Size'] == 'Large')]\n",
        "x = [\n",
        "    row['config.dataset_configs.train_num_samples'] / 1_000_000 * 5\n",
        "    for _, row in subset.iterrows()\n",
        "]\n",
        "y = [min(row[metric_name]) for _, row in subset.iterrows()]\n",
        "\n",
        "sns.barplot(x=x, y=y, palette='Blues', ax=axes[1])\n",
        "axes[1].set_title('Data Scaling')\n",
        "axes[1].set_xlabel(r'$\\mathbf{Data\\ Size}$' + '\\n(Million Hours)')\n",
        "\n",
        "# Figure 3: Model Scaling\n",
        "subset = df[(df['config.dataset_configs.train_num_samples'] == sample_size)]\n",
        "x = [row['Param Size'] for _, row in subset.iterrows()]\n",
        "y = [min(row[metric_name]) for _, row in subset.iterrows()]\n",
        "\n",
        "sns.barplot(x=x, y=y, palette='Blues', ax=axes[2])\n",
        "axes[2].set_title('Model Size Scaling')\n",
        "axes[2].set_xlabel(r'$\\mathbf{Model\\ Size}$' + '\\n(Million of Params)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cD7Cnz0feJ4o"
      },
      "outputs": [],
      "source": [
        "#@title Compute Scaling across data sizes and model sizes\n",
        "\n",
        "# Sample data and parameters\n",
        "metric_name = 'valid_mean_absolute_error_all'\n",
        "metric_name_short = 'MAE All Patches'\n",
        "sample_size = 5_000_000\n",
        "compute_hours = [0, 7000, 12500, 18750, 25000, 50000, 75000, 100000]\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "unique_num_samples = df['config.dataset_configs.train_num_samples'].unique()\n",
        "unique_model_sizes = df['Model Size'].unique()\n",
        "\n",
        "# Create a figure with NxM subplots\n",
        "fig, axes = plt.subplots(\n",
        "    len(unique_num_samples),\n",
        "    len(unique_model_sizes),\n",
        "    figsize=(24, 6 * len(unique_num_samples)),\n",
        "    dpi=600,\n",
        ")\n",
        "axes = (\n",
        "    axes.flatten()\n",
        "    if len(unique_num_samples) \u003e 1 and len(unique_model_sizes) \u003e 1\n",
        "    else [axes]\n",
        ")\n",
        "\n",
        "for i, num_samples in enumerate(unique_num_samples):\n",
        "  for j, model_size in enumerate(unique_model_sizes):\n",
        "    ax = axes[i * len(unique_model_sizes) + j]\n",
        "\n",
        "    subset = df[\n",
        "        (df['config.dataset_configs.train_num_samples'] == num_samples)\n",
        "        \u0026 (df['Model Size'] == model_size)\n",
        "    ]\n",
        "    num_of_logging = len(subset.iloc[0][metric_name])\n",
        "    logging_in_steps = [\n",
        "        int((idx - 1) / 100000 * num_of_logging) for idx in compute_hours\n",
        "    ]\n",
        "    x = [int(x / 250) for x in compute_hours]\n",
        "    y = [subset.iloc[0][metric_name][idx] for idx in logging_in_steps]\n",
        "\n",
        "    sns.barplot(x=x, y=y[: len(x)], palette='Blues', ax=ax)\n",
        "    ax.set_title(f'Data: {num_samples}\\nModel Size: {model_size}')\n",
        "\n",
        "    if j == 0:\n",
        "      ax.set_ylabel(metric_name_short)\n",
        "    else:\n",
        "      ax.set_ylabel('')\n",
        "\n",
        "    if i == len(unique_num_samples) - 1:\n",
        "      ax.set_xlabel(r'$\\mathbf{Compute}$' + '\\n TPU v5 VLP core hours')\n",
        "    else:\n",
        "      ax.set_xlabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivier5SvmcAP"
      },
      "outputs": [],
      "source": [
        "#@title Data Scaling across model sizes\n",
        "\n",
        "# Sample data and parameters\n",
        "metric_name = 'valid_mean_absolute_error_all'\n",
        "metric_name_short = 'MAE All Patches'\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "unique_model_sizes = df['Model Size'].unique()\n",
        "\n",
        "# Create a figure with subplots for each model size\n",
        "fig, axes = plt.subplots(\n",
        "    1,\n",
        "    len(unique_model_sizes),\n",
        "    figsize=(24, 6),\n",
        "    dpi=600\n",
        ")\n",
        "\n",
        "for j, model_size in enumerate(unique_model_sizes):\n",
        "    ax = axes[j]\n",
        "\n",
        "    subset = df[df['Model Size'] == model_size]\n",
        "    x = [\n",
        "        row['config.dataset_configs.train_num_samples'] / 1_000_000 * 5\n",
        "        for _, row in subset.iterrows()\n",
        "    ]\n",
        "    y = [min(row[metric_name]) for _, row in subset.iterrows()]\n",
        "\n",
        "    sns.barplot(x=x, y=y, palette='Blues', ax=ax)\n",
        "    ax.set_title(f'Data Scaling\\nModel Size: {model_size}')\n",
        "\n",
        "    if j == 0:\n",
        "        ax.set_ylabel(metric_name_short)\n",
        "    else:\n",
        "        ax.set_ylabel('')\n",
        "\n",
        "    ax.set_xlabel(r'$\\mathbf{Data\\ Size}$' + '\\n(Million Samples)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXlkxryyoNWJ"
      },
      "outputs": [],
      "source": [
        "#@title Model Scaling All across data sizes\n",
        "\n",
        "# Sample data and parameters\n",
        "metric_name = 'valid_mean_absolute_error_all'\n",
        "metric_name_short = 'MAE All Patches'\n",
        "sample_size = 5_000_000\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "unique_num_samples = df['config.dataset_configs.train_num_samples'].unique()\n",
        "\n",
        "# Create a figure with subplots for each data size\n",
        "fig, axes = plt.subplots(\n",
        "    1,\n",
        "    len(unique_num_samples),\n",
        "    figsize=(28, 6),\n",
        "    dpi=600\n",
        ")\n",
        "\n",
        "for j, num_samples in enumerate(unique_num_samples):\n",
        "    ax = axes[j]\n",
        "\n",
        "    subset = df[df['config.dataset_configs.train_num_samples'] == num_samples]\n",
        "    x = [row['Param Size'] for _, row in subset.iterrows()]\n",
        "    y = [min(row[metric_name]) for _, row in subset.iterrows()]\n",
        "\n",
        "    sns.barplot(x=x, y=y, palette='Blues', ax=ax)\n",
        "    ax.set_title(f'Model Size Scaling\\nSamples: {num_samples}')\n",
        "\n",
        "    if j == 0:\n",
        "        ax.set_ylabel(metric_name_short)\n",
        "    else:\n",
        "        ax.set_ylabel('')\n",
        "\n",
        "    ax.set_xlabel(r'$\\mathbf{Model\\ Size}$' + '\\n(Million Params)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "lsm_scaling_analys_data_size.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/lsm_scaling_analysis_parameter_sweep.ipynb?workspaceId=xliucs:lsm_notebooks_scaling_june22::citc",
          "timestamp": 1719177613159
        },
        {
          "file_id": "1avNG7EtMynPa2U-GHj2nd4nWWdGzcFq3",
          "timestamp": 1719177577595
        },
        {
          "file_id": "1WdhekZ_TCf-uOIWufr8RjKRHUcB0jzoZ",
          "timestamp": 1719170535908
        },
        {
          "file_id": "1rMS2cnwRlpP6NSfAA_mDuXFnbLEc6UnM",
          "timestamp": 1718735304309
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
