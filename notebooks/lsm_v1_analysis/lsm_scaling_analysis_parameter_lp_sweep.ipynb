{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hzh4DG0fCRQO"
      },
      "outputs": [],
      "source": [
        "from google3.learning.deepmind.xmanager2.client import xmanager_api\n",
        "import matplotlib as mpl\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import collections\n",
        "\n",
        "xm_client = xmanager_api.XManagerApi(xm_deployment_env='alphabet')\n",
        "MEDIUM_SIZE = 14\n",
        "mpl.rcParams.update({\n",
        "    'font.size': MEDIUM_SIZE,\n",
        "    'axes.labelsize': MEDIUM_SIZE,\n",
        "    'axes.titlesize': MEDIUM_SIZE,\n",
        "})\n",
        "mpl.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['font.size'] = 20\n",
        "plt.rcParams['axes.linewidth'] = 2\n",
        "plt.rcParams['axes.edgecolor'] = '#777777'\n",
        "plt.rcParams['axes.facecolor'] = '#FFFFFF'\n",
        "\n",
        "SMALL_SIZE = 16\n",
        "MEDIUM_SIZE = 15\n",
        "BIGGER_SIZE = 20\n",
        "\n",
        "plt.rc('font', size=MEDIUM_SIZE)  # controls default text sizes\n",
        "plt.rc('axes', titlesize=MEDIUM_SIZE)  # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=MEDIUM_SIZE)  # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=MEDIUM_SIZE)  # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=MEDIUM_SIZE)  # legend fontsize\n",
        "plt.rc('figure', titlesize=MEDIUM_SIZE)  # fontsize of the figure title\n",
        "\n",
        "elegant_palette = sns.color_palette('muted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wcscg9dz_FTO"
      },
      "outputs": [],
      "source": [
        "def read_xm_metrics(example_xid, metric_name, unit_id, lowest=True):\n",
        "  experiment = xm_client.get_experiment(example_xid)\n",
        "  work_unit = experiment.get_work_unit(unit_id)\n",
        "  all_series = work_unit.list_measurement_series()\n",
        "  # Read measurement series metadata.\n",
        "  for series in all_series:\n",
        "    if series.label == metric_name:\n",
        "      # Read measurement points data.\n",
        "      all_measurements = []\n",
        "      for measurement in series.measurements:\n",
        "        all_measurements.append(measurement.objective_value)\n",
        "      if lowest:\n",
        "        return min(all_measurements)\n",
        "      else:\n",
        "        return all_measurements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq1ScXCx3RN1"
      },
      "source": [
        "## lsm_300min_mood_vs_activity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBQR_rQhVhd4"
      },
      "outputs": [],
      "source": [
        "task_name = 'lsm_300min_mood_vs_activity'\n",
        "\n",
        "metrics = [\n",
        "    f'linear_eval/{task_name}_accuracy',\n",
        "    f'linear_eval/{task_name}_loss',\n",
        "]\n",
        "\n",
        "xm_id = 121775751\n",
        "experiment = xm_client.get_experiment(xm_id)\n",
        "num_of_units = experiment.get_num_work_units()\n",
        "xm_exp_dict = collections.defaultdict(list)\n",
        "\n",
        "for id in range(num_of_units):\n",
        "  real_id = id + 1\n",
        "  work_unit = experiment.get_work_unit(real_id)\n",
        "  key_list = work_unit.parameters.keys()\n",
        "  xm_exp_dict['unit_id'].append(id)\n",
        "  xm_exp_dict['xm_id'].append(xm_id)\n",
        "  for param_name in key_list:\n",
        "    xm_exp_dict[param_name].append(work_unit.parameters[param_name])\n",
        "  for metric in metrics:\n",
        "    xm_exp_dict[metric].append(\n",
        "        read_xm_metrics(xm_id, metric, real_id, lowest=False)\n",
        "    )\n",
        "\n",
        "df = pd.DataFrame(xm_exp_dict)\n",
        "df.info(verbose=True)\n",
        "\n",
        "# Calculating the highest accuracy and lowest loss\n",
        "df['max_accuracy'] = df[f'linear_eval/{task_name}_accuracy'].apply(max)\n",
        "df['min_loss'] = df[f'linear_eval/{task_name}_loss'].apply(min)\n",
        "\n",
        "# Ranking by highest accuracy and lowest loss\n",
        "df_sorted_by_accuracy = df.sort_values(by='max_accuracy', ascending=False)\n",
        "df_sorted_by_loss = df.sort_values(by='min_loss', ascending=True)\n",
        "\n",
        "# Display the sorted DataFrames\n",
        "print('DataFrame sorted by highest accuracy:')\n",
        "print(df_sorted_by_accuracy)\n",
        "\n",
        "print('\\nDataFrame sorted by lowest loss:')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6m9nh51kpZY"
      },
      "outputs": [],
      "source": [
        "# Get unique learning rates\n",
        "unique_metric = f'config.linear_probe.{task_name}.lr_configs.base_learning_rate'\n",
        "abalation_metric = (\n",
        "    f'config.linear_probe.{task_name}.lr_configs.total_steps'\n",
        ")\n",
        "weight_decay_metric = f'config.linear_probe.{task_name}.weight_decay'\n",
        "unique_lr = df[unique_metric].unique()\n",
        "print('unique_lr: ', unique_lr)\n",
        "# Set an elegant color palette\n",
        "palette = sns.color_palette('Set2')\n",
        "color_mapping = dict(zip(df[abalation_metric].unique(), palette))\n",
        "df = df[df[weight_decay_metric] == 1e-5]\n",
        "# Create subplots\n",
        "plt.figure(figsize=(6 * len(unique_lr), 6 * len(metrics)), dpi=600)\n",
        "\n",
        "for row_idx, metric_name in enumerate(metrics):\n",
        "  for i, lr in enumerate(unique_lr):\n",
        "    plt.subplot(len(metrics), len(unique_lr), row_idx * len(unique_lr) + i + 1)\n",
        "    subset = df[df[unique_metric] == lr]\n",
        "    for _, row in subset.iterrows():\n",
        "      x = list(range(len(row[metric_name])))\n",
        "      y = row[metric_name]\n",
        "      weight_decay = row[abalation_metric]\n",
        "      plt.plot(x, y, label=f'{weight_decay}', color=color_mapping[weight_decay])\n",
        "    plt.title(f'LR: {lr}')\n",
        "    if row_idx == len(metrics) - 1:\n",
        "      plt.xlabel('Pretraining Steps (K)')\n",
        "    else:\n",
        "      plt.xlabel('')\n",
        "    if i == 0:\n",
        "      plt.ylabel(metric_name)\n",
        "    else:\n",
        "      plt.ylabel('')\n",
        "    # if row_idx == 0 and i == len(unique_lr) - 1:\n",
        "    plt.legend(title=abalation_metric.split('.')[-1], frameon=False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle(\n",
        "    f\"Ablation Study of {task_name} in  {unique_metric.split('.')[-1]} and\"\n",
        "    f\" {abalation_metric.split('.')[-1]} Across Metrics + 1M Dataset +\"\n",
        "    ' Small ViT',\n",
        "    y=1.02,\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjrfm4HY3S_u"
      },
      "source": [
        "## lsm_300min_600_activities_balanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeZP3hJEvwo2"
      },
      "outputs": [],
      "source": [
        "task_name = 'lsm_300min_600_activities_balanced'\n",
        "metrics = [\n",
        "    f'linear_eval/{task_name}_accuracy',\n",
        "    f'linear_eval/{task_name}_loss',\n",
        "]\n",
        "\n",
        "xm_id = 121792637\n",
        "experiment = xm_client.get_experiment(xm_id)\n",
        "num_of_units = experiment.get_num_work_units()\n",
        "xm_exp_dict = collections.defaultdict(list)\n",
        "\n",
        "for id in range(num_of_units):\n",
        "  real_id = id + 1\n",
        "  work_unit = experiment.get_work_unit(real_id)\n",
        "  key_list = work_unit.parameters.keys()\n",
        "  xm_exp_dict['unit_id'].append(id)\n",
        "  xm_exp_dict['xm_id'].append(xm_id)\n",
        "  for param_name in key_list:\n",
        "    xm_exp_dict[param_name].append(work_unit.parameters[param_name])\n",
        "  for metric in metrics:\n",
        "    xm_exp_dict[metric].append(read_xm_metrics(xm_id, metric, real_id, lowest=False))\n",
        "\n",
        "df = pd.DataFrame(xm_exp_dict)\n",
        "df.info(verbose=True)\n",
        "\n",
        "# Calculating the highest accuracy and lowest loss\n",
        "df['max_accuracy'] = df[f'linear_eval/{task_name}_accuracy'].apply(max)\n",
        "df['min_loss'] = df[f'linear_eval/{task_name}_loss'].apply(min)\n",
        "\n",
        "# Ranking by highest accuracy and lowest loss\n",
        "df_sorted_by_accuracy = df.sort_values(by='max_accuracy', ascending=False)\n",
        "df_sorted_by_loss = df.sort_values(by='min_loss', ascending=True)\n",
        "\n",
        "# Display the sorted DataFrames\n",
        "print(\"DataFrame sorted by highest accuracy:\")\n",
        "print(df_sorted_by_accuracy)\n",
        "\n",
        "print(\"\\nDataFrame sorted by lowest loss:\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUNPXoRx3JBc"
      },
      "outputs": [],
      "source": [
        "# Get unique learning rates\n",
        "unique_metric = f'config.linear_probe.{task_name}.lr_configs.base_learning_rate'\n",
        "abalation_metric = (\n",
        "    f'config.linear_probe.{task_name}.lr_configs.total_steps'\n",
        ")\n",
        "weight_decay_metric = f'config.linear_probe.{task_name}.weight_decay'\n",
        "unique_lr = df[unique_metric].unique()\n",
        "print('unique_lr: ', unique_lr)\n",
        "# Set an elegant color palette\n",
        "palette = sns.color_palette('Set2')\n",
        "color_mapping = dict(zip(df[abalation_metric].unique(), palette))\n",
        "df = df[df[weight_decay_metric] == 1e-5]\n",
        "# Create subplots\n",
        "plt.figure(figsize=(6 * len(unique_lr), 6 * len(metrics)), dpi=600)\n",
        "\n",
        "for row_idx, metric_name in enumerate(metrics):\n",
        "  for i, lr in enumerate(unique_lr):\n",
        "    plt.subplot(len(metrics), len(unique_lr), row_idx * len(unique_lr) + i + 1)\n",
        "    subset = df[df[unique_metric] == lr]\n",
        "    for _, row in subset.iterrows():\n",
        "      x = list(range(len(row[metric_name])))\n",
        "      y = row[metric_name]\n",
        "      weight_decay = row[abalation_metric]\n",
        "      plt.plot(x, y, label=f'{weight_decay}', color=color_mapping[weight_decay])\n",
        "    plt.title(f'LR: {lr}')\n",
        "    if row_idx == len(metrics) - 1:\n",
        "      plt.xlabel('Pretraining Steps (K)')\n",
        "    else:\n",
        "      plt.xlabel('')\n",
        "    if i == 0:\n",
        "      plt.ylabel(metric_name)\n",
        "    else:\n",
        "      plt.ylabel('')\n",
        "    # if row_idx == 0 and i == len(unique_lr) - 1:\n",
        "    plt.legend(title=abalation_metric.split('.')[-1], frameon=False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle(\n",
        "    f\"Ablation Study of {task_name} in  {unique_metric.split('.')[-1]} and\"\n",
        "    f\" {abalation_metric.split('.')[-1]} Across Metrics + 1M Dataset +\"\n",
        "    ' Small ViT',\n",
        "    y=1.02,\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUgdiarK3U_i"
      },
      "source": [
        "## lsm_300min_2000_mood_balanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BX7MG43xkqJ"
      },
      "outputs": [],
      "source": [
        "task_name = 'lsm_300min_2000_mood_balanced'\n",
        "metrics = [\n",
        "    f'linear_eval/{task_name}_accuracy',\n",
        "    f'linear_eval/{task_name}_loss',\n",
        "]\n",
        "\n",
        "xm_id = 121798270\n",
        "experiment = xm_client.get_experiment(xm_id)\n",
        "num_of_units = experiment.get_num_work_units()\n",
        "xm_exp_dict = collections.defaultdict(list)\n",
        "\n",
        "for id in range(num_of_units):\n",
        "  real_id = id + 1\n",
        "  work_unit = experiment.get_work_unit(real_id)\n",
        "  key_list = work_unit.parameters.keys()\n",
        "  xm_exp_dict['unit_id'].append(id)\n",
        "  xm_exp_dict['xm_id'].append(xm_id)\n",
        "  for param_name in key_list:\n",
        "    xm_exp_dict[param_name].append(work_unit.parameters[param_name])\n",
        "  for metric in metrics:\n",
        "    xm_exp_dict[metric].append(read_xm_metrics(xm_id, metric, real_id, lowest=False))\n",
        "\n",
        "df = pd.DataFrame(xm_exp_dict)\n",
        "df.info(verbose=True)\n",
        "\n",
        "# Calculating the highest accuracy and lowest loss\n",
        "df['max_accuracy'] = df[f'linear_eval/{task_name}_accuracy'].apply(max)\n",
        "df['min_loss'] = df[f'linear_eval/{task_name}_loss'].apply(min)\n",
        "\n",
        "# Ranking by highest accuracy and lowest loss\n",
        "df_sorted_by_accuracy = df.sort_values(by='max_accuracy', ascending=False)\n",
        "df_sorted_by_loss = df.sort_values(by='min_loss', ascending=True)\n",
        "\n",
        "# Display the sorted DataFrames\n",
        "print(\"DataFrame sorted by highest accuracy:\")\n",
        "print(df_sorted_by_accuracy)\n",
        "\n",
        "print(\"\\nDataFrame sorted by lowest loss:\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec7G_31B3MRu"
      },
      "outputs": [],
      "source": [
        "# Get unique learning rates\n",
        "unique_metric = f'config.linear_probe.{task_name}.lr_configs.base_learning_rate'\n",
        "abalation_metric = (\n",
        "    f'config.linear_probe.{task_name}.lr_configs.total_steps'\n",
        ")\n",
        "weight_decay_metric = f'config.linear_probe.{task_name}.weight_decay'\n",
        "unique_lr = df[unique_metric].unique()\n",
        "print('unique_lr: ', unique_lr)\n",
        "# Set an elegant color palette\n",
        "palette = sns.color_palette('Set2')\n",
        "color_mapping = dict(zip(df[abalation_metric].unique(), palette))\n",
        "df = df[df[weight_decay_metric] == 1e-5]\n",
        "# Create subplots\n",
        "plt.figure(figsize=(6 * len(unique_lr), 6 * len(metrics)), dpi=600)\n",
        "\n",
        "for row_idx, metric_name in enumerate(metrics):\n",
        "  for i, lr in enumerate(unique_lr):\n",
        "    plt.subplot(len(metrics), len(unique_lr), row_idx * len(unique_lr) + i + 1)\n",
        "    subset = df[df[unique_metric] == lr]\n",
        "    for _, row in subset.iterrows():\n",
        "      x = list(range(len(row[metric_name])))\n",
        "      y = row[metric_name]\n",
        "      weight_decay = row[abalation_metric]\n",
        "      plt.plot(x, y, label=f'{weight_decay}', color=color_mapping[weight_decay])\n",
        "    plt.title(f'LR: {lr}')\n",
        "    if row_idx == len(metrics) - 1:\n",
        "      plt.xlabel('Pretraining Steps (K)')\n",
        "    else:\n",
        "      plt.xlabel('')\n",
        "    if i == 0:\n",
        "      plt.ylabel(metric_name)\n",
        "    else:\n",
        "      plt.ylabel('')\n",
        "    # if row_idx == 0 and i == len(unique_lr) - 1:\n",
        "    plt.legend(title=abalation_metric.split('.')[-1], frameon=False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle(\n",
        "    f\"Ablation Study of {task_name} in  {unique_metric.split('.')[-1]} and\"\n",
        "    f\" {abalation_metric.split('.')[-1]} Across Metrics + 1M Dataset +\"\n",
        "    ' Small ViT',\n",
        "    y=1.02,\n",
        ")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/lsm_scaling_analysis_parameter_sweep.ipynb",
          "timestamp": 1722990108294
        },
        {
          "file_id": "1avNG7EtMynPa2U-GHj2nd4nWWdGzcFq3",
          "timestamp": 1719177577595
        },
        {
          "file_id": "1WdhekZ_TCf-uOIWufr8RjKRHUcB0jzoZ",
          "timestamp": 1719170535908
        },
        {
          "file_id": "1rMS2cnwRlpP6NSfAA_mDuXFnbLEc6UnM",
          "timestamp": 1718735304309
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
