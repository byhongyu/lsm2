{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZgHYGLdPCli"
      },
      "source": [
        "https://grants.corp.google.com/#/grants?request=20h%2Fchr-ards-fitbit-prod-research-deid-eng-team:r\u0026reason=%22b%2F285178698%22"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtEw_56jyQQX"
      },
      "source": [
        "Colab Kernel: Fitbit Prod Research Colab and please follow the steps:\n",
        "\n",
        "-   Use the Fitbit prod kernel;\n",
        "-   Restart the session;\n",
        "-   Add import tensorflow_datasets as tfds to the top; -\n",
        "-   Run ad_hoc import."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXEvAVc3OsWO"
      },
      "outputs": [],
      "source": [
        "# @title Imports and Utils\n",
        "import datetime\n",
        "from typing import List\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from google3.fitbit.research.sensing.common.colab import metadata_database_helpers\n",
        "from google3.fitbit.research.sensing.common.infra.transforms import data_loading\n",
        "from google3.fitbit.research.sensing.common.infra.utils import data_intermediates\n",
        "from google3.fitbit.research.sensing.common.proto import data_key_pb2\n",
        "from google3.fitbit.research.sensing.kereru.utils import data_loader\n",
        "# from google3.medical.waveforms.modelling.lsm.datasets.lsm import sensor_constants\n",
        "from google3.pyglib import gfile\n",
        "\n",
        "\n",
        "NORMALIZATION_PARAMETERS = {\n",
        "    'HR': [81.77487101546053, 14.071882463657419],\n",
        "    'eda_level_real': [4.610644695036417, 4.038640434725605],\n",
        "    'leads_contact_counts': [232.12929124219374, 47.452852397806325],\n",
        "    'steps': [8.040939885304397, 18.620454054986762],\n",
        "    'jerk_auto': [202.14300725778924, 35.148804019774296],\n",
        "    'step_count': [10.972891950490821, 16.380615326908973],\n",
        "    'log_energy': [57.67806502240965, 43.5115141491905],\n",
        "    'covariance': [44.4438237732618, 12.932594067855076],\n",
        "    'log_energy_ratio': [43.869075905081964, 22.12416232697863],\n",
        "    'zero_crossing_std': [159.64183077769096, 28.980474108337734],\n",
        "    'zero_crossing_avg': [50.14393512016058, 35.18255260603251],\n",
        "    'axis_mean': [117.5897743871761, 26.153304870045442],\n",
        "    'altim_std': [0.003743016795787982, 0.04998379208940402],\n",
        "    'kurtosis': [105.52495686705635, 61.87037974939132],\n",
        "    'sleep_coefficient': [8.607589305861987, 3.9187832962818043],\n",
        "    'wrist_temperatures': [30.80471706762349, 2.899243085851182],\n",
        "    'hrv_shannon_entropy_rr': [3.2953304951132885, 0.464777365409023],\n",
        "    'hrv_shannon_entropy_rrd': [2.9810634995051184, 0.48817021363471297],\n",
        "    'hrv_percentage_of_nn_30': [0.35201734277287905, 0.1902607735053669],\n",
        "    'ceda_magnitude_real_micro_siemens': [4.743574484899, 12.913499081063],\n",
        "    'ceda_slope_real_micro_siemens': [3.2444288158784063, 1.821951365148186],\n",
        "    'rmssd_percentile_0595': [34.1895276671302, 23.783359512525266],\n",
        "    'sdnn_percentile_0595': [45.335573726241854, 24.38601160405501],\n",
        "    'msa_probability': [48.1172590194961, 14.292898676874556],\n",
        "    'hrv_percent_good': [0.2714810920080538, 0.2762414786979745],\n",
        "    'hrv_rr_80th_percentile_mean': [828.8905850347666, 108.40428688789727],\n",
        "    'hrv_rr_20th_percentile_mean': [734.5942838543058, 88.41269789220864],\n",
        "    'hrv_rr_median': [780.925540250376, 94.86837708152842],\n",
        "    'hrv_rr_mean': [785.7749142736874, 90.44585649648346],\n",
        "    'hr_at_rest_mean': [82.86923994290905, 10.867752252500274],\n",
        "    'skin_temperature_magnitude': [31.469650973107296, 1.7002512792231534],\n",
        "    'skin_temperature_slope': [0.2655571148317653, 17.266512596820043],\n",
        "}\n",
        "\n",
        "\n",
        "FEATURES_TO_INCLUDE = [\n",
        "    'HR',\n",
        "    'eda_level_real',\n",
        "    'leads_contact_counts',\n",
        "    'steps',\n",
        "    'jerk_auto',\n",
        "    'log_energy',\n",
        "    'covariance',\n",
        "    'log_energy_ratio',\n",
        "    'zero_crossing_std',\n",
        "    'zero_crossing_avg',\n",
        "    'axis_mean',\n",
        "    'altim_std',\n",
        "    'kurtosis',\n",
        "    'sleep_coefficient',\n",
        "    'wrist_temperatures',\n",
        "    'hrv_shannon_entropy_rr',\n",
        "    'hrv_shannon_entropy_rrd',\n",
        "    'ceda_slope_real_micro_siemens',\n",
        "    'rmssd_percentile_0595',\n",
        "    'sdnn_percentile_0595',\n",
        "    'hrv_percent_good',\n",
        "    'hrv_rr_80th_percentile_mean',\n",
        "    'hrv_rr_20th_percentile_mean',\n",
        "    'hrv_rr_median',\n",
        "    'hr_at_rest_mean',\n",
        "    'skin_temperature_slope',\n",
        "]\n",
        "\n",
        "\n",
        "DATABASE_PATH = '/span/nonprod/consumer-health-research:fitbit-prod-research'\n",
        "DATA_KEY_TYPE = 'TIER2_SITE_DATA'\n",
        "DATA_STORAGE_KEYS_TO_LOAD = [\n",
        "    'steps_compact',\n",
        "    'steps',\n",
        "    'sleep_score',\n",
        "    'daily_typical_sleep_periods',\n",
        "    'daily_resting_heart_rate',\n",
        "    'daily_time_in_sleep_stages',\n",
        "    'd_user',\n",
        "    'daily_sleep',\n",
        "    'f_user_activity_daily',\n",
        "]\n",
        "\n",
        "\n",
        "## PLOTTING IMPORTS AND UTILS\n",
        "def visualize_features(array_feature, title: str):\n",
        "\n",
        "  fig = plt.figure(figsize=(20, 7))\n",
        "  ax1 = plt.subplot2grid((1, 12), (0, 0), colspan=12)\n",
        "  group = array_feature\n",
        "\n",
        "  ax1 = sns.heatmap(\n",
        "      group.T,\n",
        "      cmap='Reds',\n",
        "      cbar=True,\n",
        "      linewidths=0.0,\n",
        "      linecolor='black',\n",
        "      alpha=0.8,\n",
        "      ax=ax1,\n",
        "      yticklabels=True,\n",
        "  )\n",
        "\n",
        "  ax1.set_xticks(np.arange(0, group.shape[0], 60))\n",
        "  ax1.set_xticklabels(np.arange(0, group.shape[0], 60))\n",
        "  for tick in ax1.get_xticklabels():\n",
        "    tick.set_fontname('Ubuntu')\n",
        "    tick.set_style('italic')\n",
        "  ax1.tick_params(axis='x', labelsize=10.5)\n",
        "  ax1.set_xlabel('Minutes')\n",
        "\n",
        "  ax1.set_yticklabels(FEATURES_TO_INCLUDE)\n",
        "  for tick in ax1.get_yticklabels():\n",
        "    tick.set_fontname('Ubuntu')\n",
        "  ax1.tick_params(axis='y', labelsize=10.5)\n",
        "\n",
        "  plt.xticks(rotation=45)  # Rotate labels for better readability\n",
        "  plt.yticks(rotation=0)  # Rotate labels for better readability\n",
        "  plt.title(title, fontname='Ubuntu', fontsize=16)\n",
        "  plt.tight_layout()\n",
        "\n",
        "  ax1.set_ylabel('Feature', fontname='Ubuntu', fontsize=14)\n",
        "\n",
        "  ax1.axhline(y=0, color='k', linewidth=1, alpha=1)\n",
        "  ax1.axhline(y=group.shape[1], color='k', alpha=1, linewidth=1)\n",
        "  ax1.axvline(x=0, color='k', linewidth=1, alpha=1)\n",
        "  ax1.axvline(x=group.shape[0], color='k', alpha=1, linewidth=1)\n",
        "\n",
        "  for i in np.arange(0, group.shape[0], 60):\n",
        "    ax1.axvline(x=i, color='k', alpha=0.4, linewidth=1)\n",
        "  for i in np.arange(0, group.shape[1], 1):\n",
        "    ax1.axhline(y=i, color='k', alpha=0.4, linewidth=1)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def load_user_data(\n",
        "    data_key_type: str,\n",
        "    user_id: str,\n",
        "    data_storage_keys_to_load: list[str],\n",
        ") -\u003e data_intermediates.DataKeyAndKeyValues:\n",
        "  \"\"\"Loads Tier-2 user data to a DataKeyAndKeyValuesWithData.\n",
        "\n",
        "  Args:\n",
        "    data_key_type: Type of the DataKey to load.\n",
        "    user_id: User ID to load.\n",
        "    data_storage_keys_to_load: List of DataStorage keys that should be loaded\n",
        "      for the user. All available loaded data will be in the returned\n",
        "      DataKeyAndKeyValuesWithData's data field.\n",
        "\n",
        "  Returns:\n",
        "    DataKeyAndKeyValuesWithData with the loaded data.\n",
        "  \"\"\"\n",
        "  # Each user is represented by one DataKey in the database. Each DataKey has\n",
        "  # Data Storage elements associated with it. These are what will point to the\n",
        "  # capacitor files with the imported raw data.\n",
        "  dkkv = metadata_database_helpers.get_database_data_for_data_key(\n",
        "      DATABASE_PATH,\n",
        "      data_key_pb2.DataKey(\n",
        "          type=data_key_type,\n",
        "          session_id=user_id,\n",
        "      ),\n",
        "  )\n",
        "\n",
        "  dkkvwd = list(\n",
        "      data_loading.LoadDataDoFn(\n",
        "          data_storage_keys_to_load, data_loader.get_data_loader()\n",
        "      ).process(dkkv)\n",
        "  )[0]\n",
        "  return dkkvwd\n",
        "\n",
        "\n",
        "def describe_data_key_key_value(dkkv):\n",
        "  print(f'DataKey: {dkkv.data_key}')\n",
        "  print(f'\\ttype: {dkkv.data_key.type}')\n",
        "  print(f'\\tsession_id (i.e., user): {dkkv.data_key.session_id}')\n",
        "  print(f'\\tdata_storage_dict: {dkkv.data_storage_dict}')\n",
        "  print(f'\\tprocess_data_dict: {dkkv.process_data_dict}')\n",
        "\n",
        "\n",
        "def describe_loaded_data(\n",
        "    dkkvwd: data_intermediates.DataKeyAndKeyValues,\n",
        ") -\u003e None:\n",
        "  \"\"\"Print description of loaded DataKeyAndKeyValuesWithData.\"\"\"\n",
        "  print('Loaded data for DataKey:')\n",
        "  print(\n",
        "      '\\ttype:                   '\n",
        "      f' {dkkvwd.data_key_and_key_values.data_key.type}'\n",
        "  )\n",
        "  print(\n",
        "      '\\tsession_id (i.e., user):'\n",
        "      f' {dkkvwd.data_key_and_key_values.data_key.session_id}\\t'\n",
        "  )\n",
        "\n",
        "  print('Loaded data:')\n",
        "  for k, v in dkkvwd.data.items():\n",
        "    print(f'\\t{k}: {len(v)} elements')\n",
        "\n",
        "  print('Metadata table data:')\n",
        "  print(\n",
        "      '\\tPipeline Metadata:'\n",
        "      f' {len(dkkvwd.data_key_and_key_values.pipeline_metadata_dict)} items'\n",
        "  )\n",
        "  print(\n",
        "      '\\tData Storage:     '\n",
        "      f' {len(dkkvwd.data_key_and_key_values.data_storage_dict)} items'\n",
        "  )\n",
        "  print(\n",
        "      '\\tProcess Data:     '\n",
        "      f' {len(dkkvwd.data_key_and_key_values.process_data_dict)} items'\n",
        "  )\n",
        "\n",
        "\n",
        "def convert_days_to_datetime(days, start_date=datetime.datetime(1970, 1, 1)):\n",
        "  \"\"\"Converts a number of days to a datetime object, relative to a start date.\n",
        "\n",
        "  Args:\n",
        "    days: The number of days to add to the start date.\n",
        "    start_date: The starting datetime object (defaults to epoch time,\n",
        "      1970-01-01).\n",
        "\n",
        "  Returns:\n",
        "    A datetime object representing the calculated date.\n",
        "  \"\"\"\n",
        "  return start_date + datetime.timedelta(days=days)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTZTd6kHPUdU"
      },
      "outputs": [],
      "source": [
        "# @title Plotting\n",
        "\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, selected_mask):\n",
        "  \"\"\"Calculates MSE, MAE, and MAPE.\n",
        "\n",
        "  Args:\n",
        "      y_true: Ground truth values.\n",
        "      y_pred: Predicted values.\n",
        "      selected_mask: A boolean mask indicating which values to consider.\n",
        "\n",
        "  Returns:\n",
        "      A tuple containing the MSE, MAE, and MAPE.\n",
        "  \"\"\"\n",
        "  y_true = np.array(y_true)\n",
        "  y_pred = np.array(y_pred)\n",
        "  y_true = y_true[selected_mask]\n",
        "  y_pred = y_pred[selected_mask]\n",
        "  assert y_true.shape == y_pred.shape\n",
        "  mse = np.mean(np.square(y_true - y_pred))\n",
        "  mae = np.mean(np.abs(y_true - y_pred))\n",
        "  number_of_sample = y_true.shape[0]\n",
        "\n",
        "  # Avoid division by zero and only calculate MAPE where y_true is not zero\n",
        "  # non_zero_mask = y_true != 0\n",
        "  non_zero_mask = np.abs(y_true) \u003e 1e-3\n",
        "  mape = (\n",
        "      np.mean(\n",
        "          np.abs(\n",
        "              (y_true[non_zero_mask] - y_pred[non_zero_mask])\n",
        "              / y_true[non_zero_mask]\n",
        "          )\n",
        "      )\n",
        "      * 100\n",
        "  )\n",
        "\n",
        "  return mse, mae, mape, number_of_sample\n",
        "\n",
        "\n",
        "def plot_all_features(\n",
        "    features_to_include,\n",
        "    reconstruction_merged,\n",
        "    original,\n",
        "    normalization_parameters,\n",
        "    subset_mask,\n",
        "    figure_title: str,\n",
        "):\n",
        "  \"\"\"Visualizes all features in a single 4x8 grid of subplots.\n",
        "\n",
        "  Includes MSE, MAE, and MAPE in the title of each subplot, and only shows the\n",
        "  legend on the first subplot.\n",
        "\n",
        "  Args:\n",
        "      features_to_include: List of feature names.\n",
        "      reconstruction_merged:  Numpy array of reconstructed data.\n",
        "      original: Numpy array of original data.\n",
        "      normalization_parameters: Dictionary of normalization parameters (min,\n",
        "        range) for each feature.\n",
        "      subset_mask: Numpy array of subset mask where data is NOT missing in\n",
        "        original data but masked in addtional masking.\n",
        "      figure_title: Title of the figure.\n",
        "  \"\"\"\n",
        "\n",
        "  fig, axes = plt.subplots(8, 4, figsize=(24, 24))  # Increase figure width\n",
        "  axes = axes.flatten()\n",
        "\n",
        "  for i, feature in enumerate(features_to_include):\n",
        "    ax = axes[i]\n",
        "\n",
        "    ax.set_facecolor('xkcd:white')\n",
        "    ax.spines['top'].set_color('black')\n",
        "    ax.spines['bottom'].set_color('black')\n",
        "    ax.spines['left'].set_color('black')\n",
        "    ax.spines['right'].set_color('black')\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.xaxis.set_tick_params(which='major', size=10, width=2, direction='in')\n",
        "    ax.xaxis.set_tick_params(which='minor', size=7, width=2, direction='in')\n",
        "    ax.yaxis.set_tick_params(which='major', size=10, width=2, direction='in')\n",
        "    ax.yaxis.set_tick_params(which='minor', size=7, width=2, direction='in')\n",
        "\n",
        "    feature_index = features_to_include.index(feature)\n",
        "\n",
        "    reconstructed_data = (\n",
        "        reconstruction_merged[:, feature_index]\n",
        "        * normalization_parameters[feature][1]\n",
        "        + normalization_parameters[feature][0]\n",
        "    )\n",
        "    original_data = (\n",
        "        original[:, feature_index] * normalization_parameters[feature][1]\n",
        "        + normalization_parameters[feature][0]\n",
        "    )\n",
        "\n",
        "    ax.plot(reconstructed_data, label='Reconstruction from LSM', alpha=0.6)\n",
        "    ax.plot(original_data, label='Original Data (Ground-Truth)', alpha=0.6)\n",
        "\n",
        "    ax.set_xlabel('Time (minutes from midnight)', labelpad=10)\n",
        "    ax.set_ylabel(feature, labelpad=10)\n",
        "\n",
        "    # Calculate and display metrics in the title\n",
        "    selected_mask = subset_mask[:, feature_index]\n",
        "    mse, mae, mape, n_sample = calculate_metrics(\n",
        "        original_data, reconstructed_data, selected_mask\n",
        "    )\n",
        "    ax.set_title(\n",
        "        f'MSE: {mse:.2f}, MAE: {mae:.2f}, MAPE: {mape:.2f}%, N: {n_sample}',\n",
        "        fontsize=12,\n",
        "    )\n",
        "    for j in range(len(selected_mask)):\n",
        "      if selected_mask[j]:\n",
        "        ax.axvspan(j - 0.5, j + 0.5, facecolor='red', alpha=0.3)\n",
        "\n",
        "    # Only show legend on the first subplot\n",
        "    if i == 0:\n",
        "      ax.legend()\n",
        "    elif ax.get_legend() is not None:\n",
        "      ax.get_legend().remove()  # Remove the legend\n",
        "  plt.suptitle(\n",
        "      figure_title, fontsize=14, y=0.98\n",
        "  )  # Move title closer to subplots\n",
        "  plt.tight_layout(\n",
        "      rect=[0, 0, 1, 0.985]\n",
        "  )  # Minimize gap between title and subplots\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_one_sample(\n",
        "    file_path: str,\n",
        "    plotOn: bool,\n",
        "    model_name: str,\n",
        "    task_name: str,\n",
        "    patch_size=(10, 1),\n",
        "):\n",
        "  with gfile.Open(file_path, 'rb') as f:\n",
        "    loaded_data = np.load(f, allow_pickle=True)\n",
        "  input = np.reshape(\n",
        "      loaded_data['input_signal'][0, 0, :, :, 0],\n",
        "      (WINDOW_LENGTH, NUMBER_OF_FEATURE),\n",
        "  )\n",
        "  original = np.reshape(\n",
        "      loaded_data['input_signal'][0, 0, :, :, 0],\n",
        "      (WINDOW_LENGTH, NUMBER_OF_FEATURE),\n",
        "  ).copy()\n",
        "  imputation_mask = np.reshape(\n",
        "      loaded_data['imputation_mask'][0, 0, :, :, 0],\n",
        "      (WINDOW_LENGTH, NUMBER_OF_FEATURE),\n",
        "  )\n",
        "  input[imputation_mask] = np.nan\n",
        "\n",
        "  id = loaded_data['user_id'][:, 0][0].decode('utf-8')\n",
        "  key = loaded_data['key'][:, 0][0].decode('utf-8')\n",
        "  print('**************************************')\n",
        "  print('Getting data for: ', id, ' at: ', key)\n",
        "  print('**************************************')\n",
        "  dkkvwd = load_user_data(\n",
        "      data_key_type=DATA_KEY_TYPE,\n",
        "      user_id=id,\n",
        "      data_storage_keys_to_load=DATA_STORAGE_KEYS_TO_LOAD,\n",
        "  )\n",
        "\n",
        "  step_cnt = -1\n",
        "  sleep_minutes = -1\n",
        "  table = 'f_user_activity_daily'\n",
        "  if table not in dkkvwd.data.keys():\n",
        "    print('Table not in dkkvwd.data.keys()')\n",
        "    return\n",
        "  for i in dkkvwd.data[table]:\n",
        "    calculated_date = convert_days_to_datetime(i.activity_dt)\n",
        "    if calculated_date.strftime('%Y-%m-%d') == key[0:10]:\n",
        "      step_cnt = i.step_cnt\n",
        "      sleep_minutes = i.sleep_all_asleep_minute_cnt\n",
        "      break\n",
        "  if step_cnt == -1:\n",
        "    print('Step Count is -1')\n",
        "    return\n",
        "  additional_mask = np.zeros([WINDOW_LENGTH, NUMBER_OF_FEATURE])\n",
        "  num_patches = [\n",
        "      int(WINDOW_LENGTH / patch_size[0]),\n",
        "      int(NUMBER_OF_FEATURE / patch_size[1]),\n",
        "  ]\n",
        "  token_mask = loaded_data['token_mask'][:, 0, :].reshape(\n",
        "      num_patches[0], num_patches[1]\n",
        "  )\n",
        "  for i in range(num_patches[0]):\n",
        "    for j in range(num_patches[1]):\n",
        "      if token_mask[i, j] == 1:\n",
        "        additional_mask[\n",
        "            i * patch_size[0] : (i + 1) * patch_size[0],\n",
        "            j * patch_size[1] : (j + 1) * patch_size[1],\n",
        "        ] = 1\n",
        "        input[\n",
        "            i * patch_size[0] : (i + 1) * patch_size[0],\n",
        "            j * patch_size[1] : (j + 1) * patch_size[1],\n",
        "        ] = np.nan\n",
        "  additional_mask = additional_mask.astype(bool)\n",
        "\n",
        "  tmp = loaded_data['eval_plot_logits'][0, 0, :, :].reshape(\n",
        "      loaded_data['eval_plot_logits'].shape[2], patch_size[0], patch_size[1]\n",
        "  )[:, :, :]\n",
        "  reconstruction = np.zeros([WINDOW_LENGTH, NUMBER_OF_FEATURE])\n",
        "  for i in range(num_patches[0]):\n",
        "    for j in range(num_patches[1]):\n",
        "      reconstruction[\n",
        "          i * patch_size[0] : (i + 1) * patch_size[0],\n",
        "          j * patch_size[1] : (j + 1) * patch_size[1],\n",
        "      ] = tmp[i * num_patches[1] + j, :, :].reshape(\n",
        "          patch_size[0], patch_size[1]\n",
        "      )\n",
        "\n",
        "  reconstruction_merged = input.copy()\n",
        "  reconstruction_merged[imputation_mask] = reconstruction[imputation_mask]\n",
        "  reconstruction_merged[additional_mask] = reconstruction[additional_mask]\n",
        "\n",
        "  if plotOn:\n",
        "    print('********* USER ID *********')\n",
        "    print('ID: ', id)\n",
        "    print('Key: ', key)\n",
        "    visualize_features(imputation_mask, 'Missing Mask')\n",
        "    plt.show()\n",
        "    visualize_features(original, 'Original Signal')\n",
        "    plt.show()\n",
        "    visualize_features(additional_mask, 'Addtional Mask')\n",
        "    plt.show()\n",
        "    visualize_features(input, 'Input with Addtional Mask')\n",
        "    plt.show()\n",
        "    visualize_features(reconstruction, 'RECONSTRUCTION (ALL)')\n",
        "    plt.show()\n",
        "    visualize_features(reconstruction_merged, 'RECONSTRUCTION (MERGED)')\n",
        "    plt.show()\n",
        "    subset_mask = (~imputation_mask) \u0026 additional_mask\n",
        "    plot_all_features(\n",
        "        FEATURES_TO_INCLUDE,\n",
        "        reconstruction_merged,\n",
        "        original,\n",
        "        NORMALIZATION_PARAMETERS,\n",
        "        subset_mask,\n",
        "        f'Model: {model_name}, Task: {task_name}',\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0hoRWeQGmuM"
      },
      "outputs": [],
      "source": [
        "# @title Find the right file\n",
        "\n",
        "\n",
        "def get_config_file_paths(\n",
        "    root_path: str, dataset_name: str, config_type: str\n",
        ") -\u003e List[str]:\n",
        "  \"\"\"Returns a list of file paths for the given dataset and config type.\"\"\"\n",
        "  selected_path = gfile.Glob(f'{root_path}/{dataset_name}/{config_type}*')\n",
        "  return selected_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6yfBQgVS2SS"
      },
      "outputs": [],
      "source": [
        "# @title Experiment ID Dict\n",
        "EXPERIMENT_ID_DICT = {\n",
        "    'random_10_by_1': (\n",
        "        '25_3_20_nai_1rand8_200kstep_lsm_v2_missing_balanced_20250301_valid_dataset_0.3_only_xid_155551157_wid_1_20250325140044'\n",
        "    ),\n",
        "    'random_10_by_2': (\n",
        "        '25_3_20_nai_1rand8_10by2_200kstep_lsm_v2_missing_balanced_20250301_valid_dataset_0.3_only_xid_155551076_wid_1_20250325140733'\n",
        "    ),\n",
        "    'random_10_by_1_inherited': (\n",
        "        '25_3_20_inh_1rand8_200kstep_lsm_v2_missing_balanced_20250301_valid_dataset_0.3_only_xid_155551505_wid_1_20250325163411'\n",
        "    ),\n",
        "    'mix_10_by_1': (\n",
        "        '25_3_20_nai_1rand8_1fbar4_sharedemb384_200kstep_lsm_v2_missing_balanced_20250301_valid_dataset_0.3_only_xid_155553088_wid_1_20250325135212'\n",
        "    ),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNI3ks-B9JLH"
      },
      "outputs": [],
      "source": [
        "# @title Constants Set up\n",
        "ROOT_FILE_PATH = '/namespace/fitbit-medical-sandboxes/jg/partner/encrypted/chr-ards-fitbit-prod-research/deid/exp/dmcduff/ttl=52w/lsm_v2/exp_dumps'\n",
        "LIMIT_SAMPLE = 5\n",
        "WINDOW_LENGTH = 1440\n",
        "NUMBER_OF_FEATURE = 26\n",
        "PATCH_SIZE = [10, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrHMznsWrjwe"
      },
      "outputs": [],
      "source": [
        "SELECTED_CONFIG = 'imputation_0.04167' # @param ['forecast_0.00695', 'forecast_0.02084', 'forecast_0.04167', 'forecast_0.125', 'forecast_0.25', 'forecast_0.5', 'imputation_0.00695', 'imputation_0.02084', 'imputation_0.04167', 'imputation_0.125', 'imputation_0.25', 'imputation_0.5', 'random_imputation_0.2', 'random_imputation_0.5', 'random_imputation_0.8']\n",
        "MODEL_NAME = 'random_10_by_1_inherited' # @param ['mix_10_by_1', 'random_10_by_1', 'random_10_by_2', 'random_10_by_1_inherited']\n",
        "selected_paths = get_config_file_paths(\n",
        "    ROOT_FILE_PATH, EXPERIMENT_ID_DICT[MODEL_NAME], SELECTED_CONFIG\n",
        ")\n",
        "for file_path in selected_paths[:LIMIT_SAMPLE]:\n",
        "  if '10_by_2' in MODEL_NAME:\n",
        "    plot_one_sample(file_path, True, MODEL_NAME, SELECTED_CONFIG, (10, 2))\n",
        "  else:\n",
        "    plot_one_sample(file_path, True, MODEL_NAME, SELECTED_CONFIG, (10, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYPLGFUuFGeD"
      },
      "outputs": [],
      "source": [
        "selected_path = gfile.Glob(\n",
        "    f'{ROOT_FILE_PATH}/{EXPERIMENT_ID_DICT[MODEL_NAME]}/*'\n",
        ")\n",
        "sorted(\n",
        "    list(set([path.split('/')[-1].split('_eval')[0] for path in selected_path]))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLojsY6lFW-F"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//fitbit/research/sensing/fitbit_prod_research/colab_algo:rl_colab",
        "kind": "shared"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/lsm_v2_model_visualization.ipynb?workspaceId=xliucs:model_v2_vis_update::citc",
          "timestamp": 1743226491307
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/lsm_v2_model_visualization.ipynb",
          "timestamp": 1743028430394
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/lsm_v2_model_visualization.ipynb",
          "timestamp": 1741878753933
        },
        {
          "file_id": "1GheuDXJBd9jFLqQCTI90BA3R-eMIeH1P",
          "timestamp": 1741303624629
        },
        {
          "file_id": "1rqkLFnz9qEDXitPinNfmySiCuTgeO--h",
          "timestamp": 1741277525380
        },
        {
          "file_id": "1pveDEd1HaiIC4ciS2Qa3eI6vZ0vjRJYU",
          "timestamp": 1741276006405
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
