{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmwuaKU43Ppf"
      },
      "source": [
        "## LSM Naive Baselines\n",
        "##### Colab Kernel (Brainframe GPU)\n",
        "##### Dataset (Electrodes)\n",
        "\n",
        "Grants command for Access on Demand (AoD):\n",
        "\n",
        "https://grants.corp.google.com/#/grants?request=20h%2Fchr-ards-electrodes-deid-colab-jobs\u0026reason=b%2F314799341\n",
        "\n",
        "### About This Notebook:\n",
        "This notebook implements and evaluates naive baselines to compare against the LSM ViT MAE method. These baselines are evaluated on the validation set of the electrodes dataset. These baselines include:\n",
        "1. Mean fill\n",
        "2. Linear  interpolation\n",
        "3. Nearest neighbor\n",
        "4. MICE (as described here: TODO)\n",
        "\n",
        "To run and visualize examples of these baselines run all setup cells, and then run the `Plot Naive Baseline Examples` cell.\n",
        "\n",
        "To run naive baseline eval, across the `validation` set, set the `TO SET` values in the `Run Eval` and then run the cell. This takes ~1.5hrs to iterate over the ~650K examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYll81bDh32t"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BB_N45tQ3MNR"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "import functools\n",
        "from typing import Any, Callable, Dict, Iterator, Tuple, Optional, Type, Union\n",
        "import time\n",
        "\n",
        "import collections\n",
        "from collections import Counter\n",
        "\n",
        "from absl import logging\n",
        "from clu import metric_writers\n",
        "from clu import periodic_actions\n",
        "from clu import platform\n",
        "\n",
        "import flax\n",
        "from flax import jax_utils\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.profiler\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import ml_collections\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import tqdm\n",
        "\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "\n",
        "from colabtools import adhoc_import\n",
        "with adhoc_import.Google3():\n",
        "  from scenic.dataset_lib import dataset_utils\n",
        "  from scenic.google.xm import xm_utils\n",
        "  from scenic.model_lib.base_models import base_model\n",
        "  from scenic.model_lib.base_models import model_utils\n",
        "  from scenic.model_lib.layers import nn_ops\n",
        "  from scenic.train_lib import optax as scenic_optax\n",
        "  from scenic.train_lib import pretrain_utils\n",
        "  from scenic.train_lib import train_utils\n",
        "\n",
        "  from scenic.projects.multimask.models import model_utils as mm_model_utils\n",
        "\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import dataset_constants\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_tiny_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.models import lsm_vit as lsm_vit_mae\n",
        "  from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils import model_constants\n",
        "  from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils import model_utils as lsm_model_utils\n",
        "  from google3.experimental.largesensormodels.scenic.trainers import lsm_mae_trainer\n",
        "  from google3.experimental.largesensormodels.scenic.trainers import lsm_mae_utils\n",
        "\n",
        "  from google3.learning.deepmind.xmanager2.client import xmanager_api\n",
        "  from google3.pyglib import gfile\n",
        "\n",
        "\n",
        "Batch = Dict[str, jnp.ndarray]\n",
        "MetricFn = Callable[\n",
        "    [jnp.ndarray, jnp.ndarray, Dict[str, jnp.ndarray]],\n",
        "    Dict[str, Tuple[float, int]],\n",
        "]\n",
        "LossFn = Callable[\n",
        "    [jnp.ndarray, Batch, Optional[jnp.ndarray], jnp.ndarray], float\n",
        "]\n",
        "LrFns = Dict[str, Callable[[jnp.ndarray], jnp.ndarray]]\n",
        "Patch = Union[Tuple[int, int], Tuple[int, int, int]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wPDLOVQ64LI"
      },
      "outputs": [],
      "source": [
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JAhQVzOJ4MqM"
      },
      "outputs": [],
      "source": [
        "# @title Sample Config\n",
        "\n",
        "r\"\"\"A config to train a Tiny ViT MAE on LSM 5M dataset.\n",
        "\n",
        "Forked from google3/third_party/py/scenic/projects/multimask/configs/mae_cifar10_tiny.py\n",
        "\n",
        "To run on XManager:\n",
        "gxm third_party/py/scenic/google/xm/launch_xm.py -- \\\n",
        "--binary //experimental/largesensormodels/scenic:main \\\n",
        "--config=experimental/largesensormodels/scenic/configs/mae_lsm_tiny.py \\\n",
        "--platform=vlp_4x4 \\\n",
        "--exp_name=lsm_mae_tier2_TinyShallow_10_5_res \\\n",
        "--workdir=/cns/dz-d/home/xliucs/lsm/xm/\\{xid\\} \\\n",
        "--xm_resource_alloc=group:mobile-dynamic/h2o-ai-gqm-quota \\\n",
        "--priority=200\n",
        "\n",
        "To run locally:\n",
        "./third_party/py/scenic/google/runlocal.sh \\\n",
        "--uptc=\"\" \\\n",
        "--binary=//experimental/largesensormodels/scenic:main \\\n",
        "--config=$(pwd)/experimental/largesensormodels/scenic/configs/mae_lsm_tiny.py:runlocal\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# To set constants.\n",
        "# DATASET_NAME = 'lsm_300min_10M_impute'\n",
        "DATASET_NAME = 'lsm_300min_pretraining_165K_n10'\n",
        "\n",
        "CACHE_DATASET = False\n",
        "TRAIN_DATA_SIZE = 100000  # 100k train samples\n",
        "BATCH_SIZE = 8  # 256 * 200\n",
        "NUMBER_OF_EPOCH = 100\n",
        "\n",
        "# Model variant / patch H (time steps) / patch W (features)\n",
        "VARIANT = 'TiShallow/10/5'\n",
        "LRS = [1e-3]\n",
        "TOKEN_MASK_PROB = 'constant_0.8'\n",
        "LOSS_ONLY_MASKED_TOKENS = True\n",
        "USE_DATETIME_FEATURES = True\n",
        "USE_TRAIN_AUGMENTATIONS = True\n",
        "TRAIN_AUGMENTATIONS = ['stretch', 'flip', 'noise']\n",
        "\n",
        "# Derived constants.\n",
        "TRAIN_DATA_SIZE = min(\n",
        "    TRAIN_DATA_SIZE,\n",
        "    dataset_constants.lsm_dataset_constants[DATASET_NAME]['num_train_examples']\n",
        ")\n",
        "\n",
        "STEPS_PER_EPOCH = max(1, int(TRAIN_DATA_SIZE / BATCH_SIZE))\n",
        "NUM_TRAIN_STEPS = int(NUMBER_OF_EPOCH * STEPS_PER_EPOCH)\n",
        "\n",
        "LOG_EVAL_SUMMARY_STEPS = STEPS_PER_EPOCH\n",
        "LOG_CHECKPOINT_STEPS = LOG_EVAL_SUMMARY_STEPS * 5\n",
        "MAX_NUM_CHECKPOINTS = int(NUM_TRAIN_STEPS / LOG_CHECKPOINT_STEPS)\n",
        "\n",
        "\n",
        "def get_config_common_few_shot(\n",
        "    batch_size: Optional[int] = None,\n",
        "    target_resolution: int = 224,\n",
        "    resize_resolution: int = 256,\n",
        ") -\u003e ml_collections.ConfigDict:\n",
        "  \"\"\"Returns a standard-ish fewshot eval configuration.\n",
        "\n",
        "  Copied from\n",
        "  third_party/py/scenic/projects/baselines/configs/google/common/common_fewshot.py\n",
        "\n",
        "  Args:\n",
        "    batch_size: The batch size to use for fewshot evaluation.\n",
        "    target_resolution: The target resolution of the fewshot evaluation.\n",
        "    resize_resolution: The resize resolution of the fewshot evaluation.\n",
        "\n",
        "  Returns:\n",
        "    A ConfigDict with the fewshot evaluation configuration.\n",
        "  \"\"\"\n",
        "  config = ml_collections.ConfigDict()\n",
        "  config.batch_size = batch_size\n",
        "  config.representation_layer = 'pre_logits'\n",
        "  config.log_eval_steps = 25_000\n",
        "  config.datasets = {\n",
        "      'birds': ('caltech_birds2011', 'train', 'test'),\n",
        "      'caltech': ('caltech101', 'train', 'test'),\n",
        "      'cars': ('cars196:2.1.0', 'train', 'test'),\n",
        "      'cifar100': ('cifar100', 'train', 'test'),\n",
        "      'col_hist': ('colorectal_histology', 'train[:2000]', 'train[2000:]'),\n",
        "      'dtd': ('dtd', 'train', 'test'),\n",
        "      'imagenet': ('imagenet2012_subset/10pct', 'train', 'validation'),\n",
        "      'pets': ('oxford_iiit_pet', 'train', 'test'),\n",
        "      'uc_merced': ('uc_merced', 'train[:1000]', 'train[1000:]'),\n",
        "  }\n",
        "  config.pp_train = f'decode|resize({resize_resolution})|central_crop({target_resolution})|value_range(-1,1)'\n",
        "  config.pp_eval = f'decode|resize({resize_resolution})|central_crop({target_resolution})|value_range(-1,1)'\n",
        "  config.shots = [1, 5, 10, 25]\n",
        "  config.l2_regs = [2.0**i for i in range(-10, 20)]\n",
        "  config.walk_first = ('imagenet', 10)\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "def get_config(runlocal=''):\n",
        "  \"\"\"Returns the ViT experiment configuration.\"\"\"\n",
        "\n",
        "  runlocal = bool(runlocal)\n",
        "\n",
        "  # Experiment.\n",
        "  config = ml_collections.ConfigDict()\n",
        "  config.experiment_name = f'electrodes-mae-{DATASET_NAME}-{TRAIN_DATA_SIZE}'\n",
        "  config.dataset_name = f'lsm_prod/{DATASET_NAME}'\n",
        "\n",
        "  # Dataset.\n",
        "  config.data_dtype_str = 'float32'\n",
        "  config.dataset_configs = ml_collections.ConfigDict()\n",
        "  config.dataset_configs.dataset = f'lsm_prod/{DATASET_NAME}'\n",
        "  # config.dataset_configs.num_classes = NUM_CLASSES\n",
        "  config.dataset_configs.train_split = 'train'  # train data split\n",
        "  config.dataset_configs.train_num_samples = TRAIN_DATA_SIZE  # train sample\n",
        "  # eval data split - note: this split is used for validation and test.\n",
        "  config.dataset_configs.eval_split = 'test[:64]' if runlocal else 'test'\n",
        "  config.dataset_configs.cache_dataset = CACHE_DATASET\n",
        "  config.dataset_configs.prefetch_to_device = 2\n",
        "  # Shuffle_buffer_size is per host, so small-ish is ok.\n",
        "  config.dataset_configs.shuffle_buffer_size = 250_000\n",
        "  config.dataset_configs.repeat_data = False\n",
        "\n",
        "  # Model.\n",
        "  if len(VARIANT.split('/')) == 3:\n",
        "    version = VARIANT.split('/')[0]  # model variant\n",
        "    patch_h = VARIANT.split('/')[1]  # patch width\n",
        "    patch_w = VARIANT.split('/')[2]  # patch height\n",
        "  elif len(VARIANT.split('/')) == 2:\n",
        "    version = VARIANT.split('/')[0]  # model variant\n",
        "    patch_h = VARIANT.split('/')[1]  # patch width\n",
        "    patch_w = VARIANT.split('/')[1]  # patch height\n",
        "  else:\n",
        "    raise ValueError(f'Invalid model variant: {VARIANT}')\n",
        "\n",
        "  version = 'Deb' if runlocal else version\n",
        "  config.model_name = 'lsm_vit_mae'\n",
        "  config.model = ml_collections.ConfigDict()\n",
        "  # encoder\n",
        "  config.model.hidden_size = model_constants.HIDDEN_SIZES[version]\n",
        "  config.model.patches = ml_collections.ConfigDict()\n",
        "  config.model.patches.size = tuple([int(patch_h), int(patch_w)])\n",
        "  config.model.num_heads = model_constants.NUM_HEADS[version]\n",
        "  config.model.mlp_dim = model_constants.MLP_DIMS[version]\n",
        "  config.model.num_layers = model_constants.NUM_LAYERS[version]\n",
        "  config.model.dropout_rate = 0.0\n",
        "  config.model.classifier = 'none'  # Has to be \"none\" for the autoencoder\n",
        "  config.model.representation_size = None\n",
        "  config.model.positional_embedding = 'sinusoidal_2d'\n",
        "  config.model.positional_embedding_decoder = 'sinusoidal_2d'\n",
        "  # decoder\n",
        "  config.model.decoder_config = ml_collections.ConfigDict()\n",
        "  config.model.decoder_config.hidden_size = (\n",
        "      model_constants.DECODER_HIDDEN_SIZES[version]\n",
        "  )\n",
        "  config.model.decoder_config.mlp_dim = model_constants.DECODER_MLP_DIMS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.num_layers = model_constants.DECODER_NUM_LAYERS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.num_heads = model_constants.DECODER_NUM_HEADS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.dropout_rate = 0.0\n",
        "  config.model.decoder_config.attention_dropout_rate = 0.0\n",
        "\n",
        "  config.masked_feature_loss = ml_collections.ConfigDict()\n",
        "  config.masked_feature_loss.targets_type = 'rgb'\n",
        "  config.masked_feature_loss.token_mask_probability = TOKEN_MASK_PROB\n",
        "  config.masked_feature_loss.loss_only_masked_tokens = LOSS_ONLY_MASKED_TOKENS\n",
        "  config.masked_feature_loss.loss_type = 'squared'  # 'squared' or 'absolute'\n",
        "\n",
        "  # Datetime features.\n",
        "  config.use_datetime_features = USE_DATETIME_FEATURES\n",
        "\n",
        "  # Training.\n",
        "  config.trainer_name = 'lsm_mae_trainer'\n",
        "  config.batch_size = 8 if runlocal else BATCH_SIZE\n",
        "  config.num_training_steps = NUM_TRAIN_STEPS\n",
        "  config.log_eval_steps = LOG_EVAL_SUMMARY_STEPS\n",
        "  config.log_summary_steps = LOG_EVAL_SUMMARY_STEPS\n",
        "  config.rng_seed = 42\n",
        "  config.use_train_augmentations = USE_TRAIN_AUGMENTATIONS\n",
        "  config.train_augmentations = TRAIN_AUGMENTATIONS\n",
        "  sched = ml_collections.ConfigDict()\n",
        "  sched.re = '(.*)'\n",
        "  sched.lr_configs = ml_collections.ConfigDict()\n",
        "  sched.lr_configs.learning_rate_schedule = 'compound'\n",
        "  sched.lr_configs.factors = 'constant * cosine_decay * linear_warmup'\n",
        "  sched.lr_configs.total_steps = NUM_TRAIN_STEPS\n",
        "  sched.lr_configs.steps_per_cycle = sched.lr_configs.total_steps\n",
        "  sched.lr_configs.warmup_steps = STEPS_PER_EPOCH * NUMBER_OF_EPOCH * 0.05\n",
        "  sched.lr_configs.base_learning_rate = LRS[0]\n",
        "  config.schedule = ml_collections.ConfigDict({'all': sched})\n",
        "\n",
        "  # *Single* optimizer.\n",
        "  optim = ml_collections.ConfigDict()\n",
        "  optim.optax_name = 'scale_by_adam'\n",
        "  # optim.optax = dict(mu_dtype='bfloat16')\n",
        "  optim.optax_configs = ml_collections.ConfigDict({  # Optimizer settings.\n",
        "      'b1': 0.9,\n",
        "      'b2': 0.95,\n",
        "  })\n",
        "  config.optax = dict(mu_dtype='bfloat16')\n",
        "  optim.max_grad_norm = 1.0\n",
        "\n",
        "  optim.weight_decay = 1e-4\n",
        "  optim.weight_decay_decouple = True\n",
        "  config.optimizer = optim\n",
        "\n",
        "  # Fewshot.\n",
        "  # TODO(girishvn): This needs to be adapted to electrode dataset\n",
        "  config.fewshot = get_config_common_few_shot(batch_size=config.batch_size)\n",
        "  config.fewshot.datasets = {}\n",
        "  config.fewshot.walk_first = ()\n",
        "  config.fewshot.representation_layer = 'pre_logits'\n",
        "  config.fewshot.log_eval_steps = LOG_EVAL_SUMMARY_STEPS\n",
        "\n",
        "  # Logging.\n",
        "  config.write_summary = True\n",
        "  config.xprof = True  # Profile using xprof.\n",
        "  config.checkpoint = True  # Do checkpointing.\n",
        "  config.checkpoint_steps = LOG_CHECKPOINT_STEPS\n",
        "  config.debug_train = False  # Debug mode during training.\n",
        "  config.debug_eval = False  # Debug mode during eval.\n",
        "  config.max_checkpoints_to_keep = MAX_NUM_CHECKPOINTS\n",
        "  # BEGIN GOOGLE-INTERNAL\n",
        "  if runlocal:\n",
        "    # Current implementation fails with UPTC.\n",
        "    config.count_flops = False\n",
        "  # END GOOGLE-INTERNAL\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "# BEGIN GOOGLE-INTERNAL\n",
        "def get_hyper(hyper):\n",
        "  \"\"\"Defines the hyper-parameters sweeps for doing grid search.\"\"\"\n",
        "  return hyper.product([\n",
        "      hyper.sweep('config.schedule.all.lr_configs.base_learning_rate', LRS),\n",
        "  ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kUkp5g-YumOf"
      },
      "outputs": [],
      "source": [
        "# @title Patching Helper Functions\n",
        "\n",
        "def patch_img(img, ph, pw):\n",
        "  b, img_h, img_w, img_c = img.shape  # get the image shape\n",
        "  nh, nw = (img_h // ph, img_w // pw)  # number of patches\n",
        "\n",
        "  p1 = jnp.reshape(img, shape=(b, nh, ph, nw, pw, img_c))\n",
        "  p2 = jnp.transpose(p1, (0, 1, 3, 2, 4, 5))  # [b, nh, nw, ph, pw, c]\n",
        "  # p3 = jnp.reshape(p2, shape=(b, nh*nw, ph, pw, img_c))\n",
        "  patches = p2\n",
        "\n",
        "  return patches\n",
        "\n",
        "\n",
        "def get_pixel_mask(token_mask, ph, pw, img_shape):\n",
        "  b, h, w, c = img_shape\n",
        "  nh, nw = (h // ph, w // pw)  # number of patches\n",
        "  p1 = jnp.ones((b, nh, ph, nw, pw, c))  # [b, nh, ph, nw, pw, c]\n",
        "  p2 = jnp.transpose(p1, (0, 1, 3, 2, 4, 5))  # [b, nh, nw, ph, pw, c]\n",
        "  p3 = jnp.reshape(p2, shape=(b, nh*nw, ph*pw, c))  # [b, nh*nw, ph*pw, c]\n",
        "  # [b, n patches, patch size]\n",
        "  patched_ones = jnp.reshape(p3, shape=(b, nh*nw, ph*pw*c))\n",
        "\n",
        "  # Apply mask\n",
        "  weights_broadcast = jax.lax.broadcast_in_dim(\n",
        "    token_mask,\n",
        "    shape=patched_ones.shape,\n",
        "    broadcast_dimensions=tuple(range(token_mask.ndim)),\n",
        "  )\n",
        "  img_mask = model_utils.apply_weights(patched_ones, weights_broadcast)\n",
        "  img_mask = jnp.reshape(img_mask, shape=(b, nh, nw, ph, pw, c))\n",
        "  img_mask = jnp.transpose(img_mask, (0, 1, 3, 2, 4, 5))\n",
        "  img_mask = jnp.reshape(img_mask, shape=(b, nh*ph, nw*pw, c))\n",
        "\n",
        "  return img_mask\n",
        "\n",
        "\n",
        "def patch_and_mask_img(x, dropout_rng, config):\n",
        "  n_batch, _, _, _ = x.shape\n",
        "  ph, pw = config.model.patches.size\n",
        "\n",
        "  # Patch image\n",
        "  patches = patch_img(x, ph, pw)\n",
        "  height = patches.shape[1]\n",
        "  width = patches.shape[2]\n",
        "\n",
        "  # Generate mask indices.\n",
        "  n_tokens = height * width\n",
        "  token_mask_probability = config.masked_feature_loss.token_mask_probability\n",
        "  masking_configs = token_mask_probability.split('_')\n",
        "  mask_probability = float(masking_configs[1])\n",
        "\n",
        "  # Get masking strategy [random, forecast, imputation].\n",
        "  if len(masking_configs) \u003e= 3:\n",
        "    masking_strategy = masking_configs[2]\n",
        "  else:\n",
        "    masking_strategy = 'random'\n",
        "\n",
        "  # Get the mask dim (imputation and forecast).\n",
        "  if len(masking_configs) \u003e= 4:\n",
        "    mask_dim = masking_configs[3]\n",
        "    if mask_dim in ['h', 'time']:\n",
        "      mask_dim = 'h'\n",
        "      mask_dim_len = height\n",
        "      mask_offdim_len = width\n",
        "    elif mask_dim in ['w', 'feature', 'sensor']:\n",
        "      mask_dim = 'w'\n",
        "      mask_dim_len = width\n",
        "      mask_offdim_len = height\n",
        "    else:\n",
        "      raise ValueError(f'Unsupported mask_dim: {mask_dim}')\n",
        "  else:\n",
        "    mask_dim = 'h'\n",
        "    mask_dim_len = height\n",
        "    mask_offdim_len = width\n",
        "\n",
        "  if masking_strategy == 'random':  # Random Mask\n",
        "    n_masked = int(mask_probability * n_tokens)\n",
        "    mask_indices, unmasked_indices, token_mask = (\n",
        "        mm_model_utils.get_mask_indices(\n",
        "            n_batch, n_tokens, n_masked, dropout_rng  # TODO switch the rng each iteration TODO TODO TODO TODO(girishvn)\n",
        "        )\n",
        "    )\n",
        "  elif masking_strategy == 'forecast':  # Forecast\n",
        "    n_dim_masked = int(mask_probability * mask_dim_len)\n",
        "    mask_indices, unmasked_indices, token_mask = (\n",
        "        lsm_model_utils.get_forecast_mask_indices(\n",
        "            n_batch=n_batch, n_h=height, n_w=width,\n",
        "            n_dim_masked=n_dim_masked, mask_dim=mask_dim\n",
        "        )\n",
        "    )\n",
        "  elif masking_strategy == 'imputation':  # Imputation\n",
        "    n_dim_masked = int(mask_probability * mask_dim_len)\n",
        "    mask_indices, unmasked_indices, token_mask = (\n",
        "        lsm_model_utils.get_imputation_mask_indices(\n",
        "            n_batch=n_batch, n_h=height, n_w=width,\n",
        "            n_dim_masked=n_dim_masked, mask_dim=mask_dim,\n",
        "            rng=dropout_rng\n",
        "        )\n",
        "    )\n",
        "  elif masking_strategy == 'partialbar':  # Structured Bar\n",
        "    mask_dim_prob = float(masking_configs[4])\n",
        "    mask_offdim_prob = float(masking_configs[5])\n",
        "    n_dim_masked = int(mask_dim_prob * mask_dim_len)\n",
        "    n_offdim_masked = int(mask_offdim_prob * mask_offdim_len)\n",
        "    mask_indices, unmasked_indices, token_mask = (\n",
        "        lsm_model_utils.get_random_partial_bar_mask_indices(\n",
        "            n_batch=n_batch, n_h=height, n_w=width,\n",
        "            n_dim_masked=n_dim_masked, n_offdim_masked=n_offdim_masked,\n",
        "            mask_dim=mask_dim,\n",
        "            rng=dropout_rng\n",
        "        )\n",
        "    )\n",
        "  else:\n",
        "    raise ValueError(f'Unsupported masking strategy: {masking_strategy}')\n",
        "\n",
        "  # Convert Generate Pixel-Level Mask (From Patch-Level Mask)\n",
        "  pixel_mask = get_pixel_mask(token_mask, ph, pw, x.shape)\n",
        "\n",
        "  mask_info = {\n",
        "      'mask_indices': mask_indices,\n",
        "      'unmasked_indices': unmasked_indices,\n",
        "      'token_mask': token_mask,\n",
        "      'pixel_mask': pixel_mask\n",
        "  }\n",
        "\n",
        "  return mask_info\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UivMElSRQNlS"
      },
      "outputs": [],
      "source": [
        "# @title Naive Baseline Functions\n",
        "\n",
        "def fit_linear_interp(x, mask_info, config):\n",
        "\n",
        "  mask_indices = mask_info['mask_indices']\n",
        "  unmasked_indices = mask_info['unmasked_indices']\n",
        "  token_mask = mask_info['token_mask']\n",
        "  pixel_mask = mask_info['pixel_mask']\n",
        "\n",
        "  # Calculate prediction\n",
        "  x_pred = jnp.array(x.copy())\n",
        "  b, h, w, _ = x_pred.shape\n",
        "  for i in range(b):  # iterate through batch\n",
        "    for j in range(w):  # iterate through features\n",
        "      if not dataset.meta_data['input_valid_feats'][j]:\n",
        "        continue\n",
        "\n",
        "      feat_vals = x[i, :, j, :]\n",
        "      masked_feat_idx = jnp.where(pixel_mask[i, :, j, :] == 1)[0]\n",
        "      unmasked_feat_idx = jnp.where(pixel_mask[i, :, j, :] == 0)[0]\n",
        "      unmasked_feat_vals = jnp.ravel(feat_vals[unmasked_feat_idx])\n",
        "\n",
        "      # All features are masked - impossible to interpolate\n",
        "      if unmasked_feat_idx.size == 0:\n",
        "        x_pred = x_pred.at[i, jnp.arange(h), j, 0].set(0)\n",
        "      # No features are masked - no interpolation needed\n",
        "      elif masked_feat_idx.size == 0:\n",
        "        pass\n",
        "      else:\n",
        "        # Linear Interpolate\n",
        "        masked_feat_vals_interp = jnp.interp(\n",
        "            x=masked_feat_idx, xp=unmasked_feat_idx, fp=unmasked_feat_vals,\n",
        "        )\n",
        "        x_pred = x_pred.at[\n",
        "            i, masked_feat_idx, j, 0\n",
        "        ].set(masked_feat_vals_interp)\n",
        "\n",
        "  # repatch x_pred\n",
        "  ph, pw = config.model.patches.size\n",
        "  x_pred = patch_img(x_pred, ph, pw)\n",
        "  b, nh, nw, ph, pw, c = x_pred.shape\n",
        "  x_pred = jnp.reshape(x_pred, shape=(b, nh*nw, ph*pw*c))\n",
        "\n",
        "  return x_pred, {'token_mask': token_mask}\n",
        "\n",
        "\n",
        "def fit_mean_fill(x, mask_info, config):\n",
        "\n",
        "  mask_indices = mask_info['mask_indices']\n",
        "  unmasked_indices = mask_info['unmasked_indices']\n",
        "  token_mask = mask_info['token_mask']\n",
        "  pixel_mask = mask_info['pixel_mask']\n",
        "\n",
        "  # Calculate prediction\n",
        "  x_pred = jnp.array(x.copy())\n",
        "  b, h, w, _ = x_pred.shape\n",
        "  for i in range(b):  # iterate through batch\n",
        "    for j in range(w):  # iterate through features\n",
        "      if not dataset.meta_data['input_valid_feats'][j]:\n",
        "        continue\n",
        "\n",
        "      feat_vals = x[i, :, j, :]\n",
        "      masked_feat_idx = jnp.where(pixel_mask[i, :, j, :] == 1)[0]\n",
        "      unmasked_feat_idx = jnp.where(pixel_mask[i, :, j, :] == 0)[0]\n",
        "      unmasked_feat_vals = jnp.ravel(feat_vals[unmasked_feat_idx])\n",
        "\n",
        "      # All features are masked - impossible to interpolate\n",
        "      if unmasked_feat_idx.size == 0:\n",
        "        x_pred = x_pred.at[i, jnp.arange(h), j, 0].set(0)\n",
        "      # No features are masked - no interpolation needed\n",
        "      elif masked_feat_idx.size == 0:\n",
        "        pass\n",
        "      else:\n",
        "        mean_val = jnp.mean(x_pred[i, masked_feat_idx, j, 0])\n",
        "        x_pred = x_pred.at[\n",
        "            i, masked_feat_idx, j, 0\n",
        "        ].set(mean_val)\n",
        "\n",
        "  # repatch x_pred\n",
        "  ph, pw = config.model.patches.size\n",
        "  x_pred = patch_img(x_pred, ph, pw)\n",
        "  b, nh, nw, ph, pw, c = x_pred.shape\n",
        "  x_pred = jnp.reshape(x_pred, shape=(b, nh*nw, ph*pw*c))\n",
        "\n",
        "  return x_pred, {'token_mask': token_mask}\n",
        "\n",
        "\n",
        "def pandas_naive_baselines(x, mask_info, config):\n",
        "\n",
        "  token_mask = mask_info['token_mask']\n",
        "  pixel_mask = mask_info['pixel_mask']\n",
        "\n",
        "  # Get inputs.\n",
        "  x = jnp.asarray(x)\n",
        "  b, h, w, c = x.shape\n",
        "\n",
        "  # Convert mask to a mask of Nans.\n",
        "  x_nan_masked = x.at[jnp.where(pixel_mask == 1)].set(jnp.nan)\n",
        "  # b, h, w, c -\u003e h, b, w, c\n",
        "  x_batch_masked = jnp.transpose(x_nan_masked, (1, 0, 2, 3))\n",
        "  # h, b, w, c -\u003e h, b*w*c\n",
        "  x_batch_masked = jnp.reshape(x_batch_masked, (h, b*w))\n",
        "\n",
        "  # Convert to dataframe - enables single call interpolation on the whole batch\n",
        "  x_df = pd.DataFrame(x_batch_masked)\n",
        "\n",
        "  # Linear Interpolate\n",
        "  linear_interp_df = x_df.interpolate(method='linear', limit_direction='both', axis=0)\n",
        "  linear_interp_df = linear_interp_df.fillna(0)\n",
        "\n",
        "  # Nearest Neighbor Interpolate\n",
        "  nn_interp_df = x_df.interpolate(method='nearest', limit_direction='both', axis=0)\n",
        "  nn_interp_df = nn_interp_df.bfill().ffill()\n",
        "  nn_interp_df = nn_interp_df.fillna(0)\n",
        "\n",
        "  # Mean Fill Interpolate\n",
        "  mean_fill_interp_df = x_df.fillna(x_df.mean())\n",
        "  mean_fill_interp_df = mean_fill_interp_df.fillna(0)\n",
        "\n",
        "  # Convert from df to jnp array.\n",
        "  linear_interp = linear_interp_df.to_numpy()\n",
        "  nn_interp = nn_interp_df.to_numpy()\n",
        "  mean_fill_interp = mean_fill_interp_df.to_numpy()\n",
        "  linear_interp = jnp.asarray(linear_interp)\n",
        "  nn_interp = jnp.asarray(nn_interp)\n",
        "  mean_fill_interp = jnp.asarray(mean_fill_interp)\n",
        "\n",
        "  # Reshape to recover batch dim, feature dim, and channel dim\n",
        "  linear_interp = jnp.reshape(linear_interp, (h, b, w, c))\n",
        "  nn_interp = jnp.reshape(nn_interp, (h, b, w, c))\n",
        "  mean_fill_interp = jnp.reshape(mean_fill_interp, (h, b, w, c))\n",
        "\n",
        "  # Transpose to original shape\n",
        "  linear_interp = jnp.transpose(linear_interp, (1, 0, 2, 3))\n",
        "  nn_interp = jnp.transpose(nn_interp, (1, 0, 2, 3))\n",
        "  mean_fill_interp = jnp.transpose(mean_fill_interp, (1, 0, 2, 3))\n",
        "\n",
        "  # Repatch prediction\n",
        "  ph, pw = config.model.patches.size\n",
        "  linear_interp = patch_img(linear_interp, ph, pw)\n",
        "  nn_interp = patch_img(nn_interp, ph, pw)\n",
        "  mean_fill_interp = patch_img(mean_fill_interp, ph, pw)\n",
        "\n",
        "  b, nh, nw, ph, pw, c = linear_interp.shape\n",
        "  linear_interp = jnp.reshape(linear_interp, shape=(b, nh*nw, ph*pw*c))\n",
        "  nn_interp = jnp.reshape(nn_interp, shape=(b, nh*nw, ph*pw*c))\n",
        "  mean_fill_interp = jnp.reshape(mean_fill_interp, shape=(b, nh*nw, ph*pw*c))\n",
        "\n",
        "  # Return baseline logits, and mask\n",
        "  baseline_dict = {\n",
        "      'linear': linear_interp,\n",
        "      'nn': nn_interp,\n",
        "      'mean_fill': mean_fill_interp,\n",
        "  }\n",
        "  return baseline_dict, {'token_mask': token_mask}\n",
        "\n",
        "\n",
        "def fit_MICE_baselines(\n",
        "    model, x, mask_info, config, estimator_name=None, max_iter=10\n",
        "):\n",
        "\n",
        "  token_mask = mask_info['token_mask']\n",
        "  pixel_mask = mask_info['pixel_mask']\n",
        "\n",
        "  # Get inputs.\n",
        "  x = jnp.asarray(x)\n",
        "  b, h, w, c = x.shape\n",
        "\n",
        "  if b != 1:\n",
        "    raise ValueError('Batch size must be 1 for MICE')\n",
        "\n",
        "  # Convert mask to a mask of Nans.\n",
        "  x_nan_masked = x.at[jnp.where(pixel_mask == 1)].set(jnp.nan)\n",
        "  x_nan_masked = jnp.reshape(x_nan_masked, (h, w))\n",
        "\n",
        "  # Convert to dataframe - enables single call interpolation on the whole batch\n",
        "  x_df = pd.DataFrame(x_nan_masked)\n",
        "\n",
        "  # MICE Interpolation\n",
        "  # set estimator\n",
        "  if estimator_name == 'LinearRegression':\n",
        "    estimator = LinearRegression()\n",
        "  elif estimator_name == 'RandomForestRegressor':\n",
        "    estimator = RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        random_state=42,\n",
        "    )\n",
        "  else:\n",
        "    estimator = None\n",
        "\n",
        "  imp = IterativeImputer(\n",
        "      estimator=estimator,\n",
        "      max_iter=max_iter,\n",
        "      random_state=42,\n",
        "  )\n",
        "\n",
        "  interp_df = imp.fit_transform(x_df)\n",
        "  interp_df = pd.DataFrame(interp_df)\n",
        "\n",
        "  # Convert from df to jnp array.\n",
        "  interp_out = interp_df.to_numpy()\n",
        "  interp_out = jnp.asarray(interp_out)\n",
        "\n",
        "\n",
        "  # Reshape to recover batch dim, feature dim, and channel dim\n",
        "  interp_out = jnp.reshape(interp_out, (b, h, w, c))\n",
        "\n",
        "  # Repatch prediction\n",
        "  ph, pw = config.model.patches.size\n",
        "  interp_out = patch_img(interp_out, ph, pw)\n",
        "\n",
        "  b, nh, nw, ph, pw, c = interp_out.shape\n",
        "  interp_out = jnp.reshape(interp_out, shape=(b, nh*nw, ph*pw*c))\n",
        "\n",
        "  # Return baseline logits, and mask\n",
        "  baseline_dict = {\n",
        "      'MICE': interp_out,\n",
        "  }\n",
        "  return baseline_dict, {'token_mask': token_mask}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2dbsyvcvhLSI"
      },
      "outputs": [],
      "source": [
        "# @title Naive Evaluation Pipeline\n",
        "\n",
        "def naive_eval_step(\n",
        "    batch: Batch,\n",
        "    *,\n",
        "    metrics_fn: MetricFn,\n",
        "    config: ml_collections.ConfigDict,\n",
        "    debug: Optional[bool] = False,\n",
        "    rng: Optional[jax.random.PRNGKey] = None,\n",
        ") -\u003e Tuple[Dict[str, Tuple[float, int]], jnp.ndarray, Dict[str, Any]]:\n",
        "\n",
        "  # Flatten out process dimension\n",
        "  batch['input_signal'] = batch['input_signal'][0]\n",
        "  batch['batch_mask'] = batch['batch_mask'][0]\n",
        "\n",
        "  # Add prediction targets\n",
        "  batch['targets'] = lsm_mae_utils.get_targets(batch, config)\n",
        "\n",
        "  if rng is None:\n",
        "    # Always use the same seed, so that eval is as consistent as possible\n",
        "    rng = jax.random.PRNGKey(config.rng_seed)\n",
        "\n",
        "  # Patch and mask img.\n",
        "  mask_info = patch_and_mask_img(batch['input_signal'], rng, config)\n",
        "\n",
        "  # Calculate Baselines\n",
        "  logits_dict, aux = pandas_naive_baselines(batch['input_signal'], mask_info, config)\n",
        "\n",
        "  metrics_dict = dict()\n",
        "  metrics_dict['linear'] = metrics_fn(\n",
        "      logits_dict['linear'], aux['token_mask'], batch\n",
        "  )\n",
        "  metrics_dict['nn'] = metrics_fn(\n",
        "      logits_dict['nn'], aux['token_mask'], batch\n",
        "  )\n",
        "  metrics_dict['mean_fill'] = metrics_fn(\n",
        "      logits_dict['mean_fill'], aux['token_mask'], batch\n",
        "  )\n",
        "\n",
        "  return metrics_dict, logits_dict, aux\n",
        "\n",
        "\n",
        "def MICE_eval_step(\n",
        "    model,\n",
        "    batch: Batch,\n",
        "    *,\n",
        "    metrics_fn: MetricFn,\n",
        "    config: ml_collections.ConfigDict,\n",
        "    debug: Optional[bool] = False,\n",
        "    rng: Optional[jax.random.PRNGKey] = None,\n",
        ") -\u003e Tuple[Dict[str, Tuple[float, int]], jnp.ndarray, Dict[str, Any]]:\n",
        "\n",
        "  # Flatten out process dimension\n",
        "  batch['input_signal'] = batch['input_signal'][0]\n",
        "  batch['batch_mask'] = batch['batch_mask'][0]\n",
        "\n",
        "  # Add prediction targets\n",
        "  batch['targets'] = lsm_mae_utils.get_targets(batch, config)\n",
        "\n",
        "  if rng is None:\n",
        "    # Always use the same seed, so that eval is as consistent as possible\n",
        "    rng = jax.random.PRNGKey(config.rng_seed)\n",
        "\n",
        "  # Patch and mask img.\n",
        "  mask_info = patch_and_mask_img(batch['input_signal'], rng, config)\n",
        "\n",
        "  # Calculate Baselines\n",
        "  logits_dict, aux = fit_MICE_baselines(\n",
        "      model, batch['input_signal'], mask_info, config\n",
        "  )\n",
        "\n",
        "  metrics_dict = dict()\n",
        "  metrics_dict['MICE'] = metrics_fn(\n",
        "      logits_dict['MICE'], aux['token_mask'], batch\n",
        "  )\n",
        "\n",
        "  return metrics_dict, logits_dict, aux\n",
        "\n",
        "\n",
        "def regression_metrics_function(\n",
        "    predictions: jnp.ndarray,\n",
        "    prediction_masks: jnp.ndarray,\n",
        "    batch: base_model.Batch,\n",
        "    metrics: base_model.MetricNormalizerFnDict,\n",
        "    axis_name: Union[str, Tuple[str, ...]] = 'batch',\n",
        ") -\u003e Dict[str, Tuple[float, int]]:\n",
        "  \"\"\"Calculate metrics for the regression task.\n",
        "\n",
        "  Currently we assume each metric_fn has the API:\n",
        "    ```metric_fn(predictions, targets, weights)```\n",
        "  and returns an array of shape [batch,]. We also assume that to compute\n",
        "  the aggregate metric, one should sum across all batches, then divide by the\n",
        "  total samples seen. In this way we currently only support metrics of the 1/N\n",
        "  sum f(inputs, targets). Note, the caller is responsible for dividing by\n",
        "  the normalizer when computing the mean of each metric.\n",
        "\n",
        "  Args:\n",
        "   predictions: Output of model in shape [batch, length, channels].\n",
        "   prediction_masks: Masks used for masked modeling, shape [batch, length]\n",
        "   batch: Batch (dict) with keys 'targets' and optionally 'batch_mask'.\n",
        "   metrics: The regression metrics to evaluate. The key is the name of the\n",
        "     metric, and the value is the metrics function, normalizer, and a bool\n",
        "     indicating whether to apply prediction_masks.\n",
        "   axis_name: List of axes on which we run the pmsum.\n",
        "\n",
        "  Returns:\n",
        "    A dict of metrics, in which keys are metrics name and values are tuples of\n",
        "    (metric, normalizer).\n",
        "  \"\"\"\n",
        "  targets = batch['targets']\n",
        "  batch_weights = batch.get('batch_mask')\n",
        "  weights = jnp.expand_dims(batch_weights, axis=-1) * prediction_masks\n",
        "  evaluated_metrics = {}\n",
        "  for key, val in metrics.items():\n",
        "    curr_weights = weights if val[2] else batch_weights\n",
        "\n",
        "    val0 = val[0](\n",
        "        targets,\n",
        "        predictions,  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
        "        curr_weights,\n",
        "    )\n",
        "    val1 = val[1](\n",
        "        targets,\n",
        "        predictions,  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
        "        batch_weights,\n",
        "    )\n",
        "    evaluated_metrics[key] = (jnp.sum(val0), jnp.sum(val1))\n",
        "\n",
        "  return evaluated_metrics  # pytype: disable=bad-return-type  # jax-ndarray\n",
        "\n",
        "\n",
        "def naive_evaluate(\n",
        "    *,\n",
        "    rng: jnp.ndarray,\n",
        "    config: ml_collections.ConfigDict,\n",
        "    dataset: dataset_utils.Dataset,\n",
        ") -\u003e Tuple[Any, Optional[Dict[str, float]], Optional[Dict[str, Any]]]:\n",
        "\n",
        "  # Initialize model.\n",
        "  metrics_fn = functools.partial(\n",
        "        regression_metrics_function,\n",
        "        metrics=lsm_vit_mae._REGRESSION_METRICS\n",
        "  )\n",
        "\n",
        "  valid_iter = dataset.valid_iter\n",
        "  num_valid_ex = dataset.meta_data['num_val_examples']\n",
        "  if not isinstance(valid_iter, dict):  # Only on validation set.\n",
        "    valid_iter, num_valid_ex = {'valid': valid_iter}, {'valid': num_valid_ex}\n",
        "\n",
        "  for val_name, val_iter in valid_iter.items():\n",
        "    num_ex = num_valid_ex[val_name]\n",
        "    # Ceil rounding such that we include the last incomplete batch.\n",
        "    eval_batch_size = config.get('eval_batch_size', config.batch_size)\n",
        "    total_eval_steps = int(np.ceil(num_ex / eval_batch_size))\n",
        "    steps_per_eval = config.get('steps_per_eval') or total_eval_steps\n",
        "    eval_metrics = []\n",
        "    for idx in tqdm.tqdm(range(steps_per_eval)):\n",
        "      rng, mask_rng = jax.random.split(rng)  # pylint: disable=unused-variable\n",
        "      eval_batch = next(val_iter)\n",
        "\n",
        "      # Naive Baselines\n",
        "      e_metrics, _, _ = naive_eval_step(  # pylint: disable=unused-variable\n",
        "          eval_batch,\n",
        "          metrics_fn=metrics_fn,\n",
        "          config=config,\n",
        "          debug=config.debug_eval,\n",
        "          rng=mask_rng,\n",
        "      )\n",
        "\n",
        "      eval_metrics.append(e_metrics)\n",
        "\n",
        "  return eval_metrics\n",
        "\n",
        "\n",
        "def MICE_evaluate(\n",
        "    *,\n",
        "    rng: jnp.ndarray,\n",
        "    config: ml_collections.ConfigDict,\n",
        "    dataset: dataset_utils.Dataset,\n",
        ") -\u003e Tuple[Any, Optional[Dict[str, float]], Optional[Dict[str, Any]]]:\n",
        "\n",
        "  # Initialize model.\n",
        "  metrics_fn = functools.partial(\n",
        "        regression_metrics_function,\n",
        "        metrics=lsm_vit_mae._REGRESSION_METRICS\n",
        "  )\n",
        "\n",
        "  valid_iter = dataset.valid_iter\n",
        "  num_valid_ex = dataset.meta_data['num_val_examples']\n",
        "  if not isinstance(valid_iter, dict):  # Only on validation set.\n",
        "    valid_iter, num_valid_ex = {'valid': valid_iter}, {'valid': num_valid_ex}\n",
        "\n",
        "  for val_name, val_iter in valid_iter.items():\n",
        "    num_ex = num_valid_ex[val_name]\n",
        "    # Ceil rounding such that we include the last incomplete batch.\n",
        "    eval_batch_size = config.get('eval_batch_size', config.batch_size)\n",
        "    total_eval_steps = int(np.ceil(num_ex / eval_batch_size))\n",
        "    steps_per_eval = config.get('steps_per_eval') or total_eval_steps\n",
        "    eval_metrics = []\n",
        "    for idx in tqdm.tqdm(range(steps_per_eval)):\n",
        "      rng, mask_rng = jax.random.split(rng)  # pylint: disable=unused-variable\n",
        "      eval_batch = next(val_iter)\n",
        "\n",
        "      # MICE Baseline\n",
        "      e_metrics, _, _ = MICE_eval_step(  # pylint: disable=unused-variable\n",
        "          eval_batch,\n",
        "          metrics_fn=metrics_fn,\n",
        "          config=config,\n",
        "          debug=config.debug_eval,\n",
        "          rng=mask_rng,\n",
        "      )\n",
        "\n",
        "      eval_metrics.append(e_metrics)\n",
        "\n",
        "  return eval_metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KnQDfIXvf3PZ"
      },
      "outputs": [],
      "source": [
        "# @title Calculate Metrics Function\n",
        "\n",
        "def calc_metrics(\n",
        "    eval_metrics, method_list=('MICE', 'linear', 'nn', 'mean_fill')\n",
        "):\n",
        "\n",
        "  calculated_metrics = dict()\n",
        "  for k in method_list:\n",
        "    mae_all = 0\n",
        "    mae_masked = 0\n",
        "    mse_all = 0\n",
        "    mse_masked = 0\n",
        "    ex_count = 0\n",
        "    for v in eval_metrics:\n",
        "      mae_sample = v[k]['mean_absolute_error_masked'][0]\n",
        "      mse_sample = v[k]['mean_squared_error_masked'][0]\n",
        "\n",
        "      # TODO: this is hacky.\n",
        "      if mse_sample \u003e mae_sample + 100:\n",
        "        continue\n",
        "\n",
        "      mae_masked += mae_sample\n",
        "      mse_masked += mse_sample\n",
        "      ex_count += v[k]['mean_absolute_error_masked'][1]\n",
        "\n",
        "    print(f'{k}')\n",
        "    print('MAE Masked:', mae_masked / ex_count)\n",
        "    print('MSE Masked:', mse_masked / ex_count)\n",
        "    print('Ex Count:', ex_count)\n",
        "    print()\n",
        "\n",
        "    calculated_metrics[k] = (mae_masked / ex_count, mse_masked / ex_count, ex_count)\n",
        "\n",
        "  return calculated_metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_Mc0K7Jh8a6"
      },
      "source": [
        "# Run Examples and Naive Baseline Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nFCZLfJpGdN8"
      },
      "outputs": [],
      "source": [
        "# @title Plot Naive Baseline Examples\n",
        "\n",
        "# Things to set\n",
        "config = get_config(runlocal=False)  # get configs\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = lsm_tiny_dataset.get_dataset(config, data_rng)\n",
        "\n",
        "batch_x = next(dataset.valid_iter)\n",
        "x = jnp.asarray(batch_x['input_signal'][0])\n",
        "b, h, w, c = x.shape\n",
        "\n",
        "dropout_rng, rng = jax.random.split(rng)\n",
        "mask_info = patch_and_mask_img(x, dropout_rng, config)\n",
        "mask_indices = mask_info['mask_indices']\n",
        "unmasked_indices = mask_info['unmasked_indices']\n",
        "token_mask = mask_info['token_mask']\n",
        "pixel_mask = mask_info['pixel_mask']\n",
        "\n",
        "logits_dict, aux = pandas_naive_baselines(\n",
        "    x, mask_info, config\n",
        ")\n",
        "\n",
        "# NOTE: Simillar code is implemented in the function `pandas_naive_baselines`\n",
        "# Get input array.\n",
        "x_nan_masked = x.at[jnp.where(pixel_mask == 1)].set(jnp.nan)\n",
        "\n",
        "# set pixl mask\n",
        "x_batch_masked = jnp.transpose(x_nan_masked, (1, 0, 2, 3))  # b, h, w, c -\u003e h, b, w, c\n",
        "x_batch_masked = jnp.reshape(x_batch_masked, (h, b*w))  # b, w, h, c -\u003e b*w, h, c\n",
        "x_df = pd.DataFrame(x_batch_masked)\n",
        "\n",
        "# Linear Interpolate\n",
        "linear_interp_df = x_df.interpolate(method='linear', limit_direction='both', axis=0)\n",
        "linear_interp_df = linear_interp_df.fillna(0)\n",
        "\n",
        "# Nearest Neighbor Interpolate\n",
        "nn_interp_df = x_df.interpolate(method='nearest', limit_direction='both', axis=0)\n",
        "nn_interp_df = nn_interp_df.bfill().ffill()\n",
        "nn_interp_df = nn_interp_df.fillna(0)\n",
        "\n",
        "# Mean Fill Interpolate\n",
        "mean_fill_interp_df = x_df.fillna(x_df.mean())\n",
        "mean_fill_interp_df = mean_fill_interp_df.fillna(0)\n",
        "\n",
        "# Convert from df to jnp array.\n",
        "linear_interp = linear_interp_df.to_numpy()\n",
        "nn_interp = nn_interp_df.to_numpy()\n",
        "mean_fill_interp = mean_fill_interp_df.to_numpy()\n",
        "linear_interp = jnp.asarray(linear_interp)\n",
        "nn_interp = jnp.asarray(nn_interp)\n",
        "mean_fill_interp = jnp.asarray(mean_fill_interp)\n",
        "\n",
        "# Reshape to recover batch dim, feature dim, and channel dim\n",
        "linear_interp = jnp.reshape(linear_interp, (h, b, w, c))\n",
        "nn_interp = jnp.reshape(nn_interp, (h, b, w, c))\n",
        "mean_fill_interp = jnp.reshape(mean_fill_interp, (h, b, w, c))\n",
        "\n",
        "# Transpose to original shape\n",
        "linear_interp = jnp.transpose(linear_interp, (1, 0, 2, 3))\n",
        "nn_interp = jnp.transpose(nn_interp, (1, 0, 2, 3))\n",
        "mean_fill_interp = jnp.transpose(mean_fill_interp, (1, 0, 2, 3))\n",
        "\n",
        "\n",
        "# Plot example\n",
        "for idx in range(b):\n",
        "  vmin = jnp.min(x[idx])\n",
        "  vmax = jnp.max(x[idx])\n",
        "  plt.figure()\n",
        "  plt.imshow(jnp.transpose(x[idx], (1, 0, 2)), vmin=vmin, vmax=vmax)\n",
        "  plt.title('Input')\n",
        "\n",
        "  plt.figure()\n",
        "  plt.imshow(jnp.transpose(pixel_mask[idx], (1, 0, 2)))\n",
        "  plt.title('Mask')\n",
        "\n",
        "  plt.figure()\n",
        "  plt.imshow(jnp.transpose(x_nan_masked[idx], (1, 0, 2)), vmin=vmin, vmax=vmax)\n",
        "  plt.title('Masked Input')\n",
        "\n",
        "  plt.figure()\n",
        "  plt.imshow(jnp.transpose(linear_interp[idx], (1, 0, 2)), vmin=vmin, vmax=vmax)\n",
        "  plt.title('Linear Interpolation')\n",
        "\n",
        "  plt.figure()\n",
        "  plt.imshow(jnp.transpose(nn_interp[idx], (1, 0, 2)), vmin=vmin, vmax=vmax)\n",
        "  plt.title('Nearest Interpolation')\n",
        "\n",
        "  plt.figure()\n",
        "  plt.imshow(jnp.transpose(mean_fill_interp[idx], (1, 0, 2)), vmin=vmin, vmax=vmax)\n",
        "  plt.title('Mean Fill Interpolation')\n",
        "\n",
        "  plt.show()\n",
        "  print('\\n\\n\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LWAsiA9i9So"
      },
      "source": [
        "### Run Single Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5PilBlHG5Iy"
      },
      "outputs": [],
      "source": [
        "# TO SET:\n",
        "VARIANT = 'TiShallow/10/5'  # Used to set the imputation patch size\n",
        "TOKEN_MASK_PROB = 'constant_0.4_imputation'  # Used to set patching strategy\n",
        "# BATCH_SIZE = 16  # Batch size. 8 or 16 are reasonable to run in a Colab.\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "# Start pipeline.\n",
        "config = get_config(runlocal=False)  # get configs\n",
        "rng = jax.random.PRNGKey(config.rng_seed)  # set seeds\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = lsm_tiny_dataset.get_dataset(config, data_rng)  # get dataset\n",
        "\n",
        "# Run eval.\n",
        "eval_metrics = naive_evaluate(\n",
        "  rng=rng,\n",
        "  config=config,\n",
        "  dataset=dataset,\n",
        ")\n",
        "\n",
        "# Calculate and print metrics.\n",
        "calc_metrics(eval_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g67-JqAjBTd"
      },
      "source": [
        "### Run Eval Sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slviAp3AYqyC"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "BATCH_SIZE = 16  # Batch size 8 or 16 are reasonable to run in a Colab.\n",
        "\n",
        "# --------------------------------------\n",
        "# OPTION 1: Sweep across patch sizes.\n",
        "# Constants: 0.8 mask ratio, random mask strat.\n",
        "# --------------------------------------\n",
        "SWEPT_TIME_PATCHES = [(5, 5), (10, 5), (20, 5)]\n",
        "SWEPT_SENSOR_PATCHES = [(10, 1), (10, 2), (10, 10)]\n",
        "\n",
        "TOKEN_MASK_PROB_SWEEP = ['constant_0.8']  # Used to set patching strategy\n",
        "VARIANT_SWEEP = []\n",
        "for patch_size in SWEPT_TIME_PATCHES:\n",
        "  VARIANT_SWEEP.append(f'TiShallow/{patch_size[0]}/{patch_size[1]}')\n",
        "\n",
        "\n",
        "# --------------------------------------\n",
        "# OPTION 2: Sweep across time imputation / forecast / 0.8 random impute tasks\n",
        "# Constants: patch size 10x5\n",
        "# --------------------------------------\n",
        "\n",
        "# VARIANT_SWEEP = ['TiShallow/10/5']\n",
        "# TOKEN_MASK_PROB_SWEEP = [\n",
        "#     'constant_0.034_imputation', 'constant_0.067_imputation',\n",
        "#     'constant_0.1_imputation', 'constant_0.2_imputation',\n",
        "#     'constant_0.4_imputation',\n",
        "#     'constant_0.034_forecast', 'constant_0.067_forecast'\n",
        "#     'constant_0.1_forecast', 'constant_0.2_forecast',\n",
        "#     'constant_0.4_forecast',\n",
        "#     'constant_0.8',\n",
        "# ]\n",
        "\n",
        "\n",
        "# --------------------------------------\n",
        "# OPTION 3: Sweep across sensor imputation tasks\n",
        "# Constants: patch size 10x5\n",
        "# --------------------------------------\n",
        "\n",
        "# TOKEN_MASK_PROB_SWEEP_PARAMETERS = [\n",
        "#     (0.2, 0.034), (0.2, 0.067), (0.2, 0.1), (0.2, 0.2), (0.2, 0.4),\n",
        "#     (0.4, 0.034), (0.4, 0.067), (0.4, 0.1), (0.4, 0.2), (0.4, 0.4),\n",
        "#     (0.5, 0.034), (0.5, 0.067), (0.5, 0.1), (0.5, 0.2), (0.5, 0.4),\n",
        "#     (0.7, 0.034), (0.7, 0.067), (0.7, 0.1), (0.7, 0.2), (0.7, 0.4),\n",
        "#     (0.9, 0.034), (0.9, 0.067), (0.9, 0.1), (0.9, 0.2), (0.9, 0.4),\n",
        "#     (1.0, 0.034), (1.0, 0.067), (1.0, 0.1), (1.0, 0.2), (1.0, 0.4),\n",
        "# ]\n",
        "\n",
        "# TOKEN_MASK_PROB_SWEEP = []\n",
        "# for t in TOKEN_MASK_PROB_SWEEP_PARAMETERS:\n",
        "#   TOKEN_MASK_PROB_SWEEP.append(f'constant_1.0_partialbar_sensor_{t[0]}_{t[1]}')\n",
        "\n",
        "\n",
        "# --------------------------------------\n",
        "# RUN SWEEP PIPELINE.\n",
        "# NOTE. This may take hours / days depending on the sweep size.\n",
        "# Make sure to re-up AoD grant every 20 hours.\n",
        "# --------------------------------------\n",
        "count = 0\n",
        "metrics_list = []\n",
        "print('Running naive baseline sweep...')\n",
        "for v in VARIANT_SWEEP:\n",
        "  for t in TOKEN_MASK_PROB_SWEEP:\n",
        "\n",
        "    VARIANT = v  # Used to set  patch size\n",
        "    TOKEN_MASK_PROB = t\n",
        "\n",
        "    # Start pipeline.\n",
        "    config = get_config(runlocal=False)  # get configs\n",
        "    rng = jax.random.PRNGKey(config.rng_seed)  # set seeds\n",
        "    data_rng, rng = jax.random.split(rng)\n",
        "    dataset = lsm_tiny_dataset.get_dataset(config, data_rng)  # get dataset\n",
        "    ph, pw = config.model.patches.size\n",
        "\n",
        "    count += 1\n",
        "    print('\\nIteration:', count)\n",
        "    print('PATCH SIZE:', config.model.patches.size)\n",
        "    print('MASK STRATEGY:', config.masked_feature_loss.token_mask_probability)\n",
        "\n",
        "    # Run eval.\n",
        "    eval_metrics = naive_evaluate(\n",
        "      rng=rng,\n",
        "      config=config,\n",
        "      dataset=dataset,\n",
        "    )\n",
        "\n",
        "    # Calculate and print metrics.\n",
        "    metrics = calc_metrics(eval_metrics)\n",
        "\n",
        "    metrics['config.model.patches.size'] = [ph, pw]\n",
        "    metrics['patch_size'] = f'{ph}x{pw}'\n",
        "    metrics['masking_strategy'] = TOKEN_MASK_PROB\n",
        "    metrics_list.append(metrics)\n",
        "\n",
        "print('\\nDone.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUEQii0xFSDR"
      },
      "outputs": [],
      "source": [
        "naive_baselines = ['linear', 'mean_fill', 'nn']\n",
        "cols = list(metrics_list[0].keys())\n",
        "for nb in naive_baselines:\n",
        "  cols.remove(nb)\n",
        "\n",
        "df = pd.DataFrame(columns=cols)\n",
        "for m in metrics_list:\n",
        "\n",
        "  # Iterate through baselines\n",
        "  for nb in naive_baselines:\n",
        "    row = dict()\n",
        "    row['naive_baseline'] = nb\n",
        "    row['min_valid_mean_absolute_error_masked'] = float(m[nb][0])\n",
        "    row['min_valid_mean_squared_error_masked'] = float(m[nb][1])\n",
        "    row['example_count'] = int(m[nb][2])\n",
        "    for col in cols:\n",
        "      row[col] = m[col]\n",
        "\n",
        "    new_row = pd.DataFrame([row])\n",
        "    df = pd.concat([df, new_row], ignore_index=True)\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVP_jEk9VJFS"
      },
      "source": [
        "# MICE (Multivariate Imputation by Chained Equations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qbJLmCiKVKa5"
      },
      "outputs": [],
      "source": [
        "# @title Helpers\n",
        "\n",
        "# def fit_MICE_baselines(\n",
        "#     model, x, mask_info, config, estimator_name=None, max_iter=10\n",
        "# ):\n",
        "\n",
        "#   token_mask = mask_info['token_mask']\n",
        "#   pixel_mask = mask_info['pixel_mask']\n",
        "\n",
        "#   # Get inputs.\n",
        "#   x = jnp.asarray(x)\n",
        "#   b, h, w, c = x.shape\n",
        "\n",
        "#   # Convert mask to a mask of Nans.\n",
        "#   x_nan_masked = x.at[jnp.where(pixel_mask == 1)].set(jnp.nan)\n",
        "#   x_nan_masked = jnp.reshape(x_nan_masked, (b*h, w))\n",
        "\n",
        "#   sample_arrs = []\n",
        "#   for i in range(b):\n",
        "#     arr = jnp.full((h, 1), i, dtype=int)\n",
        "#     sample_arrs.append(arr)\n",
        "\n",
        "#   sample_arr = jnp.concatenate(sample_arrs)\n",
        "#   x_nan_masked = jnp.concatenate([sample_arr, x_nan_masked], axis=-1)\n",
        "\n",
        "#   # MICE Interpolation\n",
        "#   interp_out = model.fit_transform(x_nan_masked)  # impute\n",
        "#   interp_out = interp_out[:, 1:]  # remove sample count feature\n",
        "#   interp_out = jnp.reshape(interp_out, (b, h, w, c))\n",
        "\n",
        "#   # Repatch prediction\n",
        "#   ph, pw = config.model.patches.size\n",
        "#   interp_out = patch_img(interp_out, ph, pw)\n",
        "\n",
        "#   b, nh, nw, ph, pw, c = interp_out.shape\n",
        "#   interp_out = jnp.reshape(interp_out, shape=(b, nh*nw, ph*pw*c))\n",
        "\n",
        "#   # Return baseline logits, and mask\n",
        "#   baseline_dict = {\n",
        "#       'MICE': interp_out,\n",
        "#   }\n",
        "#   return baseline_dict, {'token_mask': token_mask}\n",
        "\n",
        "\n",
        "def fit_MICE_baselines(\n",
        "    model, x, mask_info, config, estimator_name=None, max_iter=10\n",
        "):\n",
        "\n",
        "  token_mask = mask_info['token_mask']\n",
        "  pixel_mask = mask_info['pixel_mask']\n",
        "\n",
        "  # Get inputs.\n",
        "  x = jnp.asarray(x)\n",
        "  b, h, w, c = x.shape\n",
        "\n",
        "  # Convert mask to a mask of Nans.\n",
        "  x_nan_masked = x.at[jnp.where(pixel_mask == 1)].set(jnp.nan)\n",
        "  x_nan_masked = jnp.reshape(x_nan_masked, (b, h, w))\n",
        "\n",
        "  interp_list = []\n",
        "  for i in range(b):\n",
        "    x_i = x_nan_masked[i]  # get input of shape h x w\n",
        "    x_i_df = pd.DataFrame(x_i)  # convert to dataframe\n",
        "\n",
        "    # MICE Interpolation\n",
        "    interp_xi = model.fit_transform(x_i_df)  # impute\n",
        "    interp_list.append(interp_xi)\n",
        "\n",
        "  # Combine batch of data\n",
        "  interp_out = jnp.stack(interp_list, axis=0)  # combine on new batch dimension\n",
        "  interp_out = jnp.expand_dims(interp_out, axis=-1)  # add channel dimension\n",
        "  interp_out = jnp.reshape(interp_out, (b, h, w, c))  # original shape\n",
        "\n",
        "  # Repatch prediction\n",
        "  ph, pw = config.model.patches.size\n",
        "  interp_out = patch_img(interp_out, ph, pw)\n",
        "\n",
        "  b, nh, nw, ph, pw, c = interp_out.shape\n",
        "  interp_out = jnp.reshape(interp_out, shape=(b, nh*nw, ph*pw*c))\n",
        "\n",
        "  # Return baseline logits, and mask\n",
        "  baseline_dict = {\n",
        "      'MICE': interp_out,\n",
        "  }\n",
        "  return baseline_dict, {'token_mask': token_mask}\n",
        "\n",
        "\n",
        "def MICE_eval_step(\n",
        "    model,\n",
        "    batch: Batch,\n",
        "    *,\n",
        "    metrics_fn: MetricFn,\n",
        "    config: ml_collections.ConfigDict,\n",
        "    debug: Optional[bool] = False,\n",
        "    rng: Optional[jax.random.PRNGKey] = None,\n",
        ") -\u003e Tuple[Dict[str, Tuple[float, int]], jnp.ndarray, Dict[str, Any]]:\n",
        "\n",
        "  # Flatten out process dimension\n",
        "  batch['input_signal'] = batch['input_signal'][0]\n",
        "  batch['batch_mask'] = batch['batch_mask'][0]\n",
        "\n",
        "  # Add prediction targets\n",
        "  batch['targets'] = lsm_mae_utils.get_targets(batch, config)\n",
        "\n",
        "  if rng is None:\n",
        "    # Always use the same seed, so that eval is as consistent as possible\n",
        "    rng = jax.random.PRNGKey(config.rng_seed)\n",
        "\n",
        "  # Patch and mask img.\n",
        "  mask_info = patch_and_mask_img(batch['input_signal'], rng, config)\n",
        "\n",
        "  # Calculate Baselines\n",
        "  logits_dict, aux = fit_MICE_baselines(\n",
        "      model, batch['input_signal'], mask_info, config\n",
        "  )\n",
        "\n",
        "  metrics_dict = dict()\n",
        "  metrics_dict['MICE'] = metrics_fn(\n",
        "      logits_dict['MICE'], aux['token_mask'], batch\n",
        "  )\n",
        "\n",
        "  return metrics_dict, logits_dict, aux\n",
        "\n",
        "\n",
        "def MICE_evaluate(\n",
        "    *,\n",
        "    model,\n",
        "    rng: jnp.ndarray,\n",
        "    config: ml_collections.ConfigDict,\n",
        "    dataset: dataset_utils.Dataset,\n",
        "    max_batches=None,\n",
        ") -\u003e Tuple[Any, Optional[Dict[str, float]], Optional[Dict[str, Any]]]:\n",
        "\n",
        "  # Initialize model.\n",
        "  metrics_fn = functools.partial(\n",
        "        regression_metrics_function,\n",
        "        metrics=lsm_vit_mae._REGRESSION_METRICS\n",
        "  )\n",
        "\n",
        "  valid_iter = dataset.valid_iter\n",
        "  num_valid_ex = dataset.meta_data['num_val_examples']\n",
        "  if not isinstance(valid_iter, dict):  # Only on validation set.\n",
        "    valid_iter, num_valid_ex = {'valid': valid_iter}, {'valid': num_valid_ex}\n",
        "\n",
        "  for val_name, val_iter in valid_iter.items():\n",
        "    num_ex = num_valid_ex[val_name]\n",
        "    # Ceil rounding such that we include the last incomplete batch.\n",
        "    eval_batch_size = config.get('eval_batch_size', config.batch_size)\n",
        "    total_eval_steps = int(np.ceil(num_ex / eval_batch_size))\n",
        "    steps_per_eval = config.get('steps_per_eval') or total_eval_steps\n",
        "    eval_metrics = []\n",
        "    for idx in tqdm.tqdm(range(steps_per_eval)):\n",
        "\n",
        "      if max_batches is not None and idx \u003e= max_batches:\n",
        "        print('\\n\\nENDING EARLY!\\n')\n",
        "        break\n",
        "\n",
        "      rng, mask_rng = jax.random.split(rng)  # pylint: disable=unused-variable\n",
        "      eval_batch = next(val_iter)\n",
        "\n",
        "      # MICE Baseline\n",
        "      e_metrics, _, _ = MICE_eval_step(  # pylint: disable=unused-variable\n",
        "          model,\n",
        "          eval_batch,\n",
        "          metrics_fn=metrics_fn,\n",
        "          config=config,\n",
        "          debug=config.debug_eval,\n",
        "          rng=mask_rng,\n",
        "      )\n",
        "\n",
        "      eval_metrics.append(e_metrics)\n",
        "\n",
        "  return eval_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jn5UnP4CXWGO"
      },
      "outputs": [],
      "source": [
        "# @title Single MICE Eval\n",
        "\n",
        "# TO SET:\n",
        "VARIANT = 'TiShallow/10/5'  # Used to set the imputation patch size\n",
        "# TOKEN_MASK_PROB = 'constant_0.034_imputation'  # Used to set patching strategy\n",
        "# TOKEN_MASK_PROB = 'constant_0.034_forecast'  # Used to set patching strategy\n",
        "# TOKEN_MASK_PROB = 'constant_1.0_partialbar_sensor_0.7_0.034'\n",
        "TOKEN_MASK_PROB = 'constant_1.0_partialbar_sensor_0.7_0.067'\n",
        "\n",
        "BATCH_SIZE = 128  # Batch size. 8 or 16 are reasonable to run in a Colab.\n",
        "MAX_BATCHES = 100\n",
        "\n",
        "# Start pipeline.\n",
        "config = get_config(runlocal=False)  # get configs\n",
        "rng = jax.random.PRNGKey(config.rng_seed)  # set seeds\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = lsm_tiny_dataset.get_dataset(config, data_rng)  # get dataset\n",
        "\n",
        "# estimator = HistGradientBoostingRegressor(\n",
        "#     max_iter=5,\n",
        "#     learning_rate=0.1,\n",
        "#     max_depth=3,\n",
        "#     random_state=42,\n",
        "#     max_bins=255\n",
        "# )\n",
        "\n",
        "# Define imp model\n",
        "estimator = None\n",
        "max_iter = 5\n",
        "imp = IterativeImputer(\n",
        "    estimator=estimator,\n",
        "    max_iter=max_iter,\n",
        "    random_state=42,\n",
        "    n_nearest_features=100\n",
        ")\n",
        "\n",
        "eval_metrics = MICE_evaluate(\n",
        "    model=imp,\n",
        "    rng=rng,\n",
        "    config=config,\n",
        "    dataset=dataset,\n",
        "    max_batches=MAX_BATCHES,\n",
        ")\n",
        "\n",
        "# Calculate and print metrics.\n",
        "calc_metrics(eval_metrics, method_list=['MICE'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKhzW2gMWsum"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "l-zn8sZgekGo"
      },
      "outputs": [],
      "source": [
        "# @title MICE Eval Sweep\n",
        "\n",
        "# Constants\n",
        "VARIANT = 'TiShallow/10/5'  # Used to set the imputation patch size\n",
        "BATCH_SIZE = 128  # Batch size. 8 or 16 are reasonable to run in a Colab.\n",
        "MAX_BATCHES = 100\n",
        "\n",
        "TOKEN_MASK_PROB_SWEEP = [\n",
        "    # 'constant_0.034_imputation',\n",
        "    # 'constant_0.067_imputation',\n",
        "    # 'constant_0.1_imputation',\n",
        "    # 'constant_0.2_imputation',\n",
        "    # 'constant_0.4_imputation',\n",
        "\n",
        "    # 'constant_0.034_forecast',\n",
        "    # 'constant_0.067_forecast',\n",
        "    # 'constant_0.1_forecast',\n",
        "    # 'constant_0.2_forecast',\n",
        "    # 'constant_0.4_forecast',\n",
        "\n",
        "    'constant_1.0_partialbar_sensor_0.7_0.034',\n",
        "    'constant_1.0_partialbar_sensor_0.7_0.067',\n",
        "    'constant_1.0_partialbar_sensor_0.7_0.1',\n",
        "    'constant_1.0_partialbar_sensor_0.7_0.2',\n",
        "    'constant_1.0_partialbar_sensor_0.7_0.4'\n",
        "]\n",
        "\n",
        "count = 0\n",
        "metrics_list = []\n",
        "for t in TOKEN_MASK_PROB_SWEEP:\n",
        "\n",
        "  TOKEN_MASK_PROB = t\n",
        "\n",
        "  # Start pipeline.\n",
        "  config = get_config(runlocal=False)  # get configs\n",
        "  rng = jax.random.PRNGKey(config.rng_seed)  # set seeds\n",
        "  data_rng, rng = jax.random.split(rng)\n",
        "  dataset = lsm_tiny_dataset.get_dataset(config, data_rng)  # get dataset\n",
        "\n",
        "  # Define imp model\n",
        "  estimator = None\n",
        "  max_iter = 5\n",
        "  imp = IterativeImputer(\n",
        "      estimator=estimator,\n",
        "      max_iter=max_iter,\n",
        "      random_state=42,\n",
        "  )\n",
        "\n",
        "  eval_metrics = MICE_evaluate(\n",
        "      model=imp,\n",
        "      rng=rng,\n",
        "      config=config,\n",
        "      dataset=dataset,\n",
        "      max_batches=MAX_BATCHES,\n",
        "  )\n",
        "\n",
        "  count += 1\n",
        "  print('\\nIteration:', count)\n",
        "  print('PATCH SIZE:', config.model.patches.size)\n",
        "  print('MASK STRATEGY:', config.masked_feature_loss.token_mask_probability)\n",
        "\n",
        "  # Calculate and print metrics.\n",
        "  metrics = calc_metrics(eval_metrics, method_list=['MICE'])\n",
        "  metrics['masking_strategy'] = TOKEN_MASK_PROB\n",
        "  metrics_list.append(metrics)\n",
        "\n",
        "print('\\nDone.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3156N5_99fsw"
      },
      "outputs": [],
      "source": [
        "cp_list = copy.deepcopy(metrics_list)\n",
        "\n",
        "rows = []\n",
        "for m in cp_list:\n",
        "  itm = m['MICE']\n",
        "  mae = itm[0]\n",
        "  mse = itm[1]\n",
        "  mask_strat = m['masking_strategy']\n",
        "\n",
        "  row = [mask_strat, mae, mse]\n",
        "  rows.append(row)\n",
        "\n",
        "rows_df = pd.DataFrame(rows, columns=['masking_strategy', 'mae', 'mse'])\n",
        "rows_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LKy1J0aFzNQB"
      },
      "outputs": [],
      "source": [
        "# @title Calculate Metrics Function\n",
        "\n",
        "def calc_metrics2(\n",
        "    eval_metrics, method_list=('MICE', 'linear', 'nn', 'mean_fill')\n",
        "):\n",
        "\n",
        "  calculated_metrics = dict()\n",
        "  for k in method_list:\n",
        "    mae_all = 0\n",
        "    mae_masked = 0\n",
        "    mse_all = 0\n",
        "    mse_masked = 0\n",
        "    ex_count = 0\n",
        "    for v in eval_metrics:\n",
        "      mae_sample = v[k]['mean_absolute_error_masked'][0]\n",
        "      mse_sample = v[k]['mean_squared_error_masked'][0]\n",
        "\n",
        "      # TODO: this is hacky.\n",
        "      if mse_sample \u003e mae_sample + 100:\n",
        "        continue\n",
        "\n",
        "      mae_masked += mae_sample\n",
        "      mse_masked += mse_sample\n",
        "      ex_count += v[k]['mean_absolute_error_masked'][1]\n",
        "\n",
        "    print(f'{k}')\n",
        "    print('MAE Masked:', mae_masked / ex_count)\n",
        "    print('MSE Masked:', mse_masked / ex_count)\n",
        "    print('Ex Count:', ex_count)\n",
        "    print()\n",
        "\n",
        "    calculated_metrics[k] = (mae_masked / ex_count, mse_masked / ex_count, ex_count)\n",
        "\n",
        "  return calculated_metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h5ZGVYz0SQ-"
      },
      "outputs": [],
      "source": [
        "calc_metrics2(\n",
        "    eval_metrics, method_list=['MICE']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5woo2Ew0a43"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5LWAsiA9i9So"
      ],
      "last_runtime": {
        "build_target": "//fitbit/research/sensing/electrodes/colab:rl_colab",
        "kind": "shared"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
