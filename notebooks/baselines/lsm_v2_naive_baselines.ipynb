{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ_dczj78iUd"
      },
      "source": [
        "## LSM v2 Naive Baselines\n",
        "##### Colab Kernel (Brainframe CPU)\n",
        "##### Dataset (Anything)\n",
        "\n",
        "Grants command for Access on Demand (AoD):\n",
        "\n",
        "https://grants.corp.google.com/#/grants?request=20h%2Fchr-ards-electrodes-deid-colab-jobs\u0026reason=b%2F314799341\n",
        "\n",
        "### About This Notebook:\n",
        "This notebook implements and evaluates naive baselines to compare against the LSM ViT MAE method. These baselines are evaluated on the validation set of the electrodes dataset. These baselines include:\n",
        "1. Mean fill\n",
        "2. Linear  interpolation\n",
        "3. Nearest neighbor\n",
        "4. MICE (as described here: TODO)\n",
        "\n",
        "To run and visualize examples of these baselines run all setup cells, and then run the `Plot Naive Baseline Examples` cell.\n",
        "\n",
        "To run naive baseline eval, across the `validation` set, set the `TO SET` values in the `Run Eval` and then run the cell. This takes ~1.5hrs to iterate over the ~650K examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYll81bDh32t"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BB_N45tQ3MNR"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "import functools\n",
        "from typing import Any, Callable, Dict, Iterator, Tuple, Optional, Type, Union\n",
        "import time\n",
        "\n",
        "import collections\n",
        "from collections import Counter\n",
        "\n",
        "from absl import logging\n",
        "from clu import metric_writers\n",
        "from clu import periodic_actions\n",
        "from clu import platform\n",
        "\n",
        "import flax\n",
        "from flax import jax_utils\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.profiler\n",
        "\n",
        "import copy\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import ml_collections\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import tqdm\n",
        "\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "\n",
        "from colabtools import adhoc_import\n",
        "with adhoc_import.Google3():\n",
        "  from scenic.dataset_lib import dataset_utils\n",
        "  from scenic.google.xm import xm_utils\n",
        "  from scenic.model_lib.base_models import base_model\n",
        "  from scenic.model_lib.base_models import model_utils\n",
        "  from scenic.model_lib.layers import nn_ops\n",
        "  from scenic.train_lib import optax as scenic_optax\n",
        "  from scenic.train_lib import pretrain_utils\n",
        "  from scenic.train_lib import train_utils\n",
        "\n",
        "  from scenic.projects.multimask.models import model_utils as mm_model_utils\n",
        "\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import get_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import dataset_constants\n",
        "  from google3.experimental.largesensormodels.scenic.models import lsm_vit as lsm_vit_mae\n",
        "  from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils import model_constants\n",
        "  from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils import model_utils as lsm_model_utils\n",
        "  from google3.experimental.largesensormodels.scenic.trainers import lsm_mae_trainer\n",
        "  from google3.experimental.largesensormodels.scenic.trainers import lsm_mae_utils\n",
        "\n",
        "  from google3.learning.deepmind.xmanager2.client import xmanager_api\n",
        "  from google3.pyglib import gfile\n",
        "\n",
        "  import ml_collections\n",
        "  from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils import model_constants\n",
        "  from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils.patcher_config import Patcher_Config\n",
        "  from google3.experimental.largesensormodels.scenic.trainers.masking.masker_config import MaskStrategy_Config, Masker_Config\n",
        "  from google3.experimental.largesensormodels.scenic.utils import config_constants\n",
        "  from google3.experimental.largesensormodels.scenic.utils import predefined_configs\n",
        "\n",
        "\n",
        "Batch = Dict[str, jnp.ndarray]\n",
        "MetricFn = Callable[\n",
        "    [jnp.ndarray, jnp.ndarray, Dict[str, jnp.ndarray]],\n",
        "    Dict[str, Tuple[float, int]],\n",
        "]\n",
        "LossFn = Callable[\n",
        "    [jnp.ndarray, Batch, Optional[jnp.ndarray], jnp.ndarray], float\n",
        "]\n",
        "LrFns = Dict[str, Callable[[jnp.ndarray], jnp.ndarray]]\n",
        "Patch = Union[Tuple[int, int], Tuple[int, int, int]]"
      ]
    },
    {
      "metadata": {
        "id": "pV7JBLEEBxQf"
      },
      "cell_type": "code",
      "source": [
        "# @title Re-import from max's CL\n",
        "\n",
        "# NOTE: This is currently (14 May 2025) needed to run sensor imputation baselines\n",
        "\n",
        "import functools\n",
        "from typing import Any, Callable, Dict, Iterator, Tuple, Optional, Type, Union\n",
        "import time\n",
        "\n",
        "import collections\n",
        "from collections import Counter\n",
        "\n",
        "from absl import logging\n",
        "from clu import metric_writers\n",
        "from clu import periodic_actions\n",
        "from clu import platform\n",
        "\n",
        "import flax\n",
        "from flax import jax_utils\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.profiler\n",
        "\n",
        "import copy\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import ml_collections\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import tqdm\n",
        "\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.data import Dataset\n",
        "\n",
        "x = Dataset\n",
        "\n",
        "\n",
        "from colabtools import adhoc_import\n",
        "with adhoc_import.Google3():\n",
        "  from scenic.dataset_lib import dataset_utils\n",
        "  from scenic.google.xm import xm_utils\n",
        "  from scenic.model_lib.base_models import base_model\n",
        "  from scenic.model_lib.base_models import model_utils\n",
        "  from scenic.model_lib.layers import nn_ops\n",
        "  from scenic.train_lib import optax as scenic_optax\n",
        "  from scenic.train_lib import pretrain_utils\n",
        "  from scenic.train_lib import train_utils\n",
        "  from scenic.projects.multimask.models import model_utils as mm_model_utils\n",
        "\n",
        "\n",
        "with adhoc_import.Google3CitcClient(\n",
        "    'lsm_mixwmod_25_4_30', username='xumax', behavior='preferred'\n",
        "):\n",
        "\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import get_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import dataset_constants\n",
        "  from google3.experimental.largesensormodels.scenic.models import lsm_vit as lsm_vit_mae\n",
        "  from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils import model_constants\n",
        "  from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils import model_utils as lsm_model_utils\n",
        "  from google3.experimental.largesensormodels.scenic.trainers import lsm_mae_trainer\n",
        "  from google3.experimental.largesensormodels.scenic.trainers import lsm_mae_utils\n",
        "\n",
        "  # from google3.learning.deepmind.xmanager2.client import xmanager_api\n",
        "  from google3.pyglib import gfile\n",
        "\n",
        "  import ml_collections\n",
        "  from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils import model_constants\n",
        "  from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils.patcher_config import Patcher_Config\n",
        "  from google3.experimental.largesensormodels.scenic.trainers.masking.masker_config import MaskStrategy_Config, Masker_Config\n",
        "  # from google3.experimental.largesensormodels.scenic.utils import config_constants\n",
        "  from google3.experimental.largesensormodels.scenic.utils import predefined_configs\n",
        "\n",
        "\n",
        "Batch = Dict[str, jnp.ndarray]\n",
        "MetricFn = Callable[\n",
        "    [jnp.ndarray, jnp.ndarray, Dict[str, jnp.ndarray]],\n",
        "    Dict[str, Tuple[float, int]],\n",
        "]\n",
        "LossFn = Callable[\n",
        "    [jnp.ndarray, Batch, Optional[jnp.ndarray], jnp.ndarray], float\n",
        "]\n",
        "LrFns = Dict[str, Callable[[jnp.ndarray], jnp.ndarray]]\n",
        "Patch = Union[Tuple[int, int], Tuple[int, int, int]]\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kUkp5g-YumOf"
      },
      "outputs": [],
      "source": [
        "# @title Patching Helper Functions\n",
        "\n",
        "def patch_img(img, ph, pw):\n",
        "  b, img_h, img_w, img_c = img.shape  # get the image shape\n",
        "  nh, nw = (img_h // ph, img_w // pw)  # number of patches\n",
        "\n",
        "  p1 = jnp.reshape(img, shape=(b, nh, ph, nw, pw, img_c))\n",
        "  p2 = jnp.transpose(p1, (0, 1, 3, 2, 4, 5))  # [b, nh, nw, ph, pw, c]\n",
        "  # p3 = jnp.reshape(p2, shape=(b, nh*nw, ph, pw, img_c))\n",
        "  patches = p2\n",
        "\n",
        "  return patches\n",
        "\n",
        "\n",
        "def get_pixel_mask(token_mask, ph, pw, img_shape):\n",
        "  b, h, w, c = img_shape\n",
        "  nh, nw = (h // ph, w // pw)  # number of patches\n",
        "  p1 = jnp.ones((b, nh, ph, nw, pw, c))  # [b, nh, ph, nw, pw, c]\n",
        "  p2 = jnp.transpose(p1, (0, 1, 3, 2, 4, 5))  # [b, nh, nw, ph, pw, c]\n",
        "  p3 = jnp.reshape(p2, shape=(b, nh*nw, ph*pw, c))  # [b, nh*nw, ph*pw, c]\n",
        "  # [b, n patches, patch size]\n",
        "  patched_ones = jnp.reshape(p3, shape=(b, nh*nw, ph*pw*c))\n",
        "\n",
        "  # Apply mask\n",
        "  weights_broadcast = jax.lax.broadcast_in_dim(\n",
        "    token_mask,\n",
        "    shape=patched_ones.shape,\n",
        "    broadcast_dimensions=tuple(range(token_mask.ndim)),\n",
        "  )\n",
        "  img_mask = model_utils.apply_weights(patched_ones, weights_broadcast)\n",
        "  img_mask = jnp.reshape(img_mask, shape=(b, nh, nw, ph, pw, c))\n",
        "  img_mask = jnp.transpose(img_mask, (0, 1, 3, 2, 4, 5))\n",
        "  img_mask = jnp.reshape(img_mask, shape=(b, nh*ph, nw*pw, c))\n",
        "\n",
        "  return img_mask\n",
        "\n",
        "\n",
        "def patch_and_mask_img(x, dropout_rng, config):\n",
        "  n_batch, _, _, _ = x.shape\n",
        "  ph, pw = config.model.patches.size\n",
        "\n",
        "  # Patch image\n",
        "  patches = patch_img(x, ph, pw)\n",
        "  height = patches.shape[1]\n",
        "  width = patches.shape[2]\n",
        "\n",
        "  # Generate mask indices.\n",
        "  n_tokens = height * width\n",
        "  token_mask_probability = config.masked_feature_loss.token_mask_probability\n",
        "  masking_configs = token_mask_probability.split('_')\n",
        "  mask_probability = float(masking_configs[1])\n",
        "\n",
        "  # Get masking strategy [random, forecast, imputation].\n",
        "  if len(masking_configs) \u003e= 3:\n",
        "    masking_strategy = masking_configs[2]\n",
        "  else:\n",
        "    masking_strategy = 'random'\n",
        "\n",
        "  # Get the mask dim (imputation and forecast).\n",
        "  if len(masking_configs) \u003e= 4:\n",
        "    mask_dim = masking_configs[3]\n",
        "    if mask_dim in ['h', 'time']:\n",
        "      mask_dim = 'h'\n",
        "      mask_dim_len = height\n",
        "      mask_offdim_len = width\n",
        "    elif mask_dim in ['w', 'feature', 'sensor']:\n",
        "      mask_dim = 'w'\n",
        "      mask_dim_len = width\n",
        "      mask_offdim_len = height\n",
        "    else:\n",
        "      raise ValueError(f'Unsupported mask_dim: {mask_dim}')\n",
        "  else:\n",
        "    mask_dim = 'h'\n",
        "    mask_dim_len = height\n",
        "    mask_offdim_len = width\n",
        "\n",
        "  if masking_strategy == 'random':  # Random Mask\n",
        "    n_masked = int(mask_probability * n_tokens)\n",
        "    mask_indices, unmasked_indices, token_mask = (\n",
        "        mm_model_utils.get_mask_indices(\n",
        "            n_batch, n_tokens, n_masked, dropout_rng  # TODO switch the rng each iteration TODO TODO TODO TODO(girishvn)\n",
        "        )\n",
        "    )\n",
        "  elif masking_strategy == 'forecast':  # Forecast\n",
        "    n_dim_masked = int(mask_probability * mask_dim_len)\n",
        "    mask_indices, unmasked_indices, token_mask = (\n",
        "        lsm_model_utils.get_forecast_mask_indices(\n",
        "            n_batch=n_batch, n_h=height, n_w=width,\n",
        "            n_dim_masked=n_dim_masked, mask_dim=mask_dim\n",
        "        )\n",
        "    )\n",
        "  elif masking_strategy == 'imputation':  # Imputation\n",
        "    n_dim_masked = int(mask_probability * mask_dim_len)\n",
        "    mask_indices, unmasked_indices, token_mask = (\n",
        "        lsm_model_utils.get_imputation_mask_indices(\n",
        "            n_batch=n_batch, n_h=height, n_w=width,\n",
        "            n_dim_masked=n_dim_masked, mask_dim=mask_dim,\n",
        "            rng=dropout_rng\n",
        "        )\n",
        "    )\n",
        "  elif masking_strategy == 'partialbar':  # Structured Bar\n",
        "    mask_dim_prob = float(masking_configs[4])\n",
        "    mask_offdim_prob = float(masking_configs[5])\n",
        "    n_dim_masked = int(mask_dim_prob * mask_dim_len)\n",
        "    n_offdim_masked = int(mask_offdim_prob * mask_offdim_len)\n",
        "    mask_indices, unmasked_indices, token_mask = (\n",
        "        lsm_model_utils.get_random_partial_bar_mask_indices(\n",
        "            n_batch=n_batch, n_h=height, n_w=width,\n",
        "            n_dim_masked=n_dim_masked, n_offdim_masked=n_offdim_masked,\n",
        "            mask_dim=mask_dim,\n",
        "            rng=dropout_rng\n",
        "        )\n",
        "    )\n",
        "  else:\n",
        "    raise ValueError(f'Unsupported masking strategy: {masking_strategy}')\n",
        "\n",
        "  # Convert Generate Pixel-Level Mask (From Patch-Level Mask)\n",
        "  pixel_mask = get_pixel_mask(token_mask, ph, pw, x.shape)\n",
        "\n",
        "  mask_info = {\n",
        "      'mask_indices': mask_indices,\n",
        "      'unmasked_indices': unmasked_indices,\n",
        "      'token_mask': token_mask,\n",
        "      'pixel_mask': pixel_mask\n",
        "  }\n",
        "\n",
        "  return mask_info\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UivMElSRQNlS"
      },
      "outputs": [],
      "source": [
        "# @title Naive Baseline Functions\n",
        "\n",
        "def fit_linear_interp(x, mask_info, config):\n",
        "\n",
        "  mask_indices = mask_info['mask_indices']\n",
        "  unmasked_indices = mask_info['unmasked_indices']\n",
        "  token_mask = mask_info['token_mask']\n",
        "  pixel_mask = mask_info['pixel_mask']\n",
        "\n",
        "  # Calculate prediction\n",
        "  x_pred = jnp.array(x.copy())\n",
        "  b, h, w, _ = x_pred.shape\n",
        "  for i in range(b):  # iterate through batch\n",
        "    for j in range(w):  # iterate through features\n",
        "      if not dataset.meta_data['input_valid_feats'][j]:\n",
        "        continue\n",
        "\n",
        "      feat_vals = x[i, :, j, :]\n",
        "      masked_feat_idx = jnp.where(pixel_mask[i, :, j, :] == 1)[0]\n",
        "      unmasked_feat_idx = jnp.where(pixel_mask[i, :, j, :] == 0)[0]\n",
        "      unmasked_feat_vals = jnp.ravel(feat_vals[unmasked_feat_idx])\n",
        "\n",
        "      # All features are masked - impossible to interpolate\n",
        "      if unmasked_feat_idx.size == 0:\n",
        "        x_pred = x_pred.at[i, jnp.arange(h), j, 0].set(0)\n",
        "      # No features are masked - no interpolation needed\n",
        "      elif masked_feat_idx.size == 0:\n",
        "        pass\n",
        "      else:\n",
        "        # Linear Interpolate\n",
        "        masked_feat_vals_interp = jnp.interp(\n",
        "            x=masked_feat_idx, xp=unmasked_feat_idx, fp=unmasked_feat_vals,\n",
        "        )\n",
        "        x_pred = x_pred.at[\n",
        "            i, masked_feat_idx, j, 0\n",
        "        ].set(masked_feat_vals_interp)\n",
        "\n",
        "  # repatch x_pred\n",
        "  ph, pw = config.model.patches.size\n",
        "  x_pred = patch_img(x_pred, ph, pw)\n",
        "  b, nh, nw, ph, pw, c = x_pred.shape\n",
        "  x_pred = jnp.reshape(x_pred, shape=(b, nh*nw, ph*pw*c))\n",
        "\n",
        "  return x_pred, {'token_mask': token_mask}\n",
        "\n",
        "\n",
        "def fit_mean_fill(x, mask_info, config):\n",
        "\n",
        "  mask_indices = mask_info['mask_indices']\n",
        "  unmasked_indices = mask_info['unmasked_indices']\n",
        "  token_mask = mask_info['token_mask']\n",
        "  pixel_mask = mask_info['pixel_mask']\n",
        "\n",
        "  # Calculate prediction\n",
        "  x_pred = jnp.array(x.copy())\n",
        "  b, h, w, _ = x_pred.shape\n",
        "  for i in range(b):  # iterate through batch\n",
        "    for j in range(w):  # iterate through features\n",
        "      if not dataset.meta_data['input_valid_feats'][j]:\n",
        "        continue\n",
        "\n",
        "      feat_vals = x[i, :, j, :]\n",
        "      masked_feat_idx = jnp.where(pixel_mask[i, :, j, :] == 1)[0]\n",
        "      unmasked_feat_idx = jnp.where(pixel_mask[i, :, j, :] == 0)[0]\n",
        "      unmasked_feat_vals = jnp.ravel(feat_vals[unmasked_feat_idx])\n",
        "\n",
        "      # All features are masked - impossible to interpolate\n",
        "      if unmasked_feat_idx.size == 0:\n",
        "        x_pred = x_pred.at[i, jnp.arange(h), j, 0].set(0)\n",
        "      # No features are masked - no interpolation needed\n",
        "      elif masked_feat_idx.size == 0:\n",
        "        pass\n",
        "      else:\n",
        "        mean_val = jnp.mean(x_pred[i, masked_feat_idx, j, 0])\n",
        "        x_pred = x_pred.at[\n",
        "            i, masked_feat_idx, j, 0\n",
        "        ].set(mean_val)\n",
        "\n",
        "  # repatch x_pred\n",
        "  ph, pw = config.model.patches.size\n",
        "  x_pred = patch_img(x_pred, ph, pw)\n",
        "  b, nh, nw, ph, pw, c = x_pred.shape\n",
        "  x_pred = jnp.reshape(x_pred, shape=(b, nh*nw, ph*pw*c))\n",
        "\n",
        "  return x_pred, {'token_mask': token_mask}\n",
        "\n",
        "\n",
        "def pandas_naive_baselines(x, mask_info, config):\n",
        "\n",
        "  token_mask = mask_info['token_mask']\n",
        "  pixel_mask = mask_info['pixel_mask']\n",
        "\n",
        "  # Get inputs.\n",
        "  x = jnp.asarray(x)\n",
        "  b, h, w, c = x.shape\n",
        "\n",
        "  # Convert mask to a mask of Nans.\n",
        "  x_nan_masked = x.at[jnp.where(pixel_mask == 1)].set(jnp.nan)\n",
        "  # b, h, w, c -\u003e h, b, w, c\n",
        "  x_batch_masked = jnp.transpose(x_nan_masked, (1, 0, 2, 3))\n",
        "  # h, b, w, c -\u003e h, b*w*c\n",
        "  x_batch_masked = jnp.reshape(x_batch_masked, (h, b*w))\n",
        "\n",
        "  # Convert to dataframe - enables single call interpolation on the whole batch\n",
        "  x_df = pd.DataFrame(x_batch_masked)\n",
        "\n",
        "  # Linear Interpolate\n",
        "  linear_interp_df = x_df.interpolate(method='linear', limit_direction='both', axis=0)\n",
        "  linear_interp_df = linear_interp_df.bfill().ffill()\n",
        "  linear_interp_df = linear_interp_df.fillna(0)\n",
        "\n",
        "  # Nearest Neighbor Interpolate\n",
        "  nn_interp_df = x_df.interpolate(method='nearest', limit_direction='both', axis=0)\n",
        "  nn_interp_df = nn_interp_df.bfill().ffill()\n",
        "  nn_interp_df = nn_interp_df.fillna(0)\n",
        "\n",
        "  # Mean Fill Interpolate\n",
        "  mean_fill_interp_df = x_df.fillna(x_df.mean())\n",
        "  mean_fill_interp_df = mean_fill_interp_df.fillna(0)\n",
        "\n",
        "  # Spline\n",
        "  # spline_interp_df = x_df.interpolate(method='spline', order=3, axis=0)\n",
        "  # spline_interp_df = spline_interp_df.bfill().ffill()\n",
        "  # spline_interp_df = spline_interp_df.fillna(0)\n",
        "\n",
        "  # spline_interp_df = x_df.interpolate(method='cubic', axis=0)\n",
        "  # spline_interp_df = spline_interp_df.bfill().ffill()\n",
        "  # spline_interp_df = spline_interp_df.fillna(0)\n",
        "\n",
        "  # Convert from df to jnp array.\n",
        "  linear_interp = linear_interp_df.to_numpy()\n",
        "  nn_interp = nn_interp_df.to_numpy()\n",
        "  mean_fill_interp = mean_fill_interp_df.to_numpy()\n",
        "  # spline_interp = spline_interp_df.to_numpy()\n",
        "\n",
        "  linear_interp = jnp.asarray(linear_interp)\n",
        "  nn_interp = jnp.asarray(nn_interp)\n",
        "  mean_fill_interp = jnp.asarray(mean_fill_interp)\n",
        "  # spline_interp = jnp.asarray(spline_interp)\n",
        "\n",
        "  # Reshape to recover batch dim, feature dim, and channel dim\n",
        "  linear_interp = jnp.reshape(linear_interp, (h, b, w, c))\n",
        "  nn_interp = jnp.reshape(nn_interp, (h, b, w, c))\n",
        "  mean_fill_interp = jnp.reshape(mean_fill_interp, (h, b, w, c))\n",
        "  # spline_interp = jnp.reshape(spline_interp, (h, b, w, c))\n",
        "\n",
        "  # Transpose to original shape\n",
        "  linear_interp = jnp.transpose(linear_interp, (1, 0, 2, 3))\n",
        "  nn_interp = jnp.transpose(nn_interp, (1, 0, 2, 3))\n",
        "  mean_fill_interp = jnp.transpose(mean_fill_interp, (1, 0, 2, 3))\n",
        "  # spline_interp = jnp.transpose(spline_interp, (1, 0, 2, 3))\n",
        "\n",
        "  # Repatch prediction\n",
        "  ph, pw = config.model.patches.size\n",
        "  linear_interp = patch_img(linear_interp, ph, pw)\n",
        "  nn_interp = patch_img(nn_interp, ph, pw)\n",
        "  mean_fill_interp = patch_img(mean_fill_interp, ph, pw)\n",
        "  # spline_interp = patch_img(spline_interp, ph, pw)\n",
        "\n",
        "  b, nh, nw, ph, pw, c = linear_interp.shape\n",
        "  linear_interp = jnp.reshape(linear_interp, shape=(b, nh*nw, ph*pw*c))\n",
        "  nn_interp = jnp.reshape(nn_interp, shape=(b, nh*nw, ph*pw*c))\n",
        "  mean_fill_interp = jnp.reshape(mean_fill_interp, shape=(b, nh*nw, ph*pw*c))\n",
        "  # spline_interp = jnp.reshape(spline_interp, shape=(b, nh*nw, ph*pw*c))\n",
        "\n",
        "  # Return baseline logits, and mask\n",
        "  baseline_dict = {\n",
        "      'linear': linear_interp,\n",
        "      'nn': nn_interp,\n",
        "      'mean_fill': mean_fill_interp,\n",
        "      # 'spline': spline_interp,\n",
        "  }\n",
        "  return baseline_dict, {'token_mask': token_mask}\n",
        "\n",
        "\n",
        "def fit_MICE_baselines(\n",
        "    model, x, mask_info, config, estimator_name=None, max_iter=10\n",
        "):\n",
        "\n",
        "  token_mask = mask_info['token_mask']\n",
        "  pixel_mask = mask_info['pixel_mask']\n",
        "\n",
        "  # Get inputs.\n",
        "  x = jnp.asarray(x)\n",
        "  b, h, w, c = x.shape\n",
        "\n",
        "  if b != 1:\n",
        "    raise ValueError('Batch size must be 1 for MICE')\n",
        "\n",
        "  # Convert mask to a mask of Nans.\n",
        "  x_nan_masked = x.at[jnp.where(pixel_mask == 1)].set(jnp.nan)\n",
        "  x_nan_masked = jnp.reshape(x_nan_masked, (h, w))\n",
        "\n",
        "  # Convert to dataframe - enables single call interpolation on the whole batch\n",
        "  x_df = pd.DataFrame(x_nan_masked)\n",
        "\n",
        "  # MICE Interpolation\n",
        "  # set estimator\n",
        "  if estimator_name == 'LinearRegression':\n",
        "    estimator = LinearRegression()\n",
        "  elif estimator_name == 'RandomForestRegressor':\n",
        "    estimator = RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        random_state=42,\n",
        "    )\n",
        "  else:\n",
        "    estimator = None\n",
        "\n",
        "  imp = IterativeImputer(\n",
        "      estimator=estimator,\n",
        "      max_iter=max_iter,\n",
        "      random_state=42,\n",
        "  )\n",
        "\n",
        "  interp_df = imp.fit_transform(x_df)\n",
        "  interp_df = pd.DataFrame(interp_df)\n",
        "\n",
        "  # Convert from df to jnp array.\n",
        "  interp_out = interp_df.to_numpy()\n",
        "  interp_out = jnp.asarray(interp_out)\n",
        "\n",
        "\n",
        "  # Reshape to recover batch dim, feature dim, and channel dim\n",
        "  interp_out = jnp.reshape(interp_out, (b, h, w, c))\n",
        "\n",
        "  # Repatch prediction\n",
        "  ph, pw = config.model.patches.size\n",
        "  interp_out = patch_img(interp_out, ph, pw)\n",
        "\n",
        "  b, nh, nw, ph, pw, c = interp_out.shape\n",
        "  interp_out = jnp.reshape(interp_out, shape=(b, nh*nw, ph*pw*c))\n",
        "\n",
        "  # Return baseline logits, and mask\n",
        "  baseline_dict = {\n",
        "      'MICE': interp_out,\n",
        "  }\n",
        "  return baseline_dict, {'token_mask': token_mask}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2dbsyvcvhLSI"
      },
      "outputs": [],
      "source": [
        "# @title Naive Evaluation Pipeline\n",
        "\n",
        "def naive_eval_step(\n",
        "    batch: Batch,\n",
        "    *,\n",
        "    metrics_fn: MetricFn,\n",
        "    config: ml_collections.ConfigDict,\n",
        "    debug: Optional[bool] = False,\n",
        "    rng: Optional[jax.random.PRNGKey] = None,\n",
        ") -\u003e Tuple[Dict[str, Tuple[float, int]], jnp.ndarray, Dict[str, Any]]:\n",
        "\n",
        "  # Flatten out process dimension\n",
        "  batch['input_signal'] = batch['input_signal'][0]\n",
        "  batch['batch_mask'] = batch['batch_mask'][0]\n",
        "  batch['token_mask'] = batch['token_mask'][0]\n",
        "  batch['imputation_mask'] = batch['imputation_mask'][0]\n",
        "  # batch['patched_imputationmask'] = batch['patched_imputationmask'][0]\n",
        "\n",
        "  # Add prediction targets\n",
        "  batch['targets'] = lsm_mae_utils.get_targets(batch, config)\n",
        "  batch['patched_imputationmask'] = lsm_mae_utils.patchify_imputationmask(batch, config)\n",
        "\n",
        "  # Patch and mask img.\n",
        "  pixel_mask = get_pixel_mask(batch['token_mask'], PH, PW, batch['input_signal'].shape)\n",
        "  mask_info = {\n",
        "      'token_mask': batch['token_mask'],\n",
        "      'pixel_mask': pixel_mask\n",
        "  }\n",
        "\n",
        "  # Calculate Baselines\n",
        "  logits_dict, aux = pandas_naive_baselines(batch['input_signal'], mask_info, config)\n",
        "\n",
        "  metrics_dict = dict()\n",
        "  metrics_dict['linear'] = metrics_fn(\n",
        "      logits_dict['linear'], aux['token_mask'], batch\n",
        "  )\n",
        "  metrics_dict['nn'] = metrics_fn(\n",
        "      logits_dict['nn'], aux['token_mask'], batch\n",
        "  )\n",
        "  metrics_dict['mean_fill'] = metrics_fn(\n",
        "      logits_dict['mean_fill'], aux['token_mask'], batch\n",
        "  )\n",
        "  # metrics_dict['spline'] = metrics_fn(\n",
        "  #     logits_dict['spline'], aux['token_mask'], batch\n",
        "  # )\n",
        "\n",
        "  return metrics_dict, logits_dict, aux\n",
        "\n",
        "\n",
        "def MICE_eval_step(\n",
        "    model,\n",
        "    batch: Batch,\n",
        "    *,\n",
        "    metrics_fn: MetricFn,\n",
        "    config: ml_collections.ConfigDict,\n",
        "    debug: Optional[bool] = False,\n",
        "    rng: Optional[jax.random.PRNGKey] = None,\n",
        ") -\u003e Tuple[Dict[str, Tuple[float, int]], jnp.ndarray, Dict[str, Any]]:\n",
        "\n",
        "  raise ValueError('NEEDS TO BE UPDATED TO LSMv2')\n",
        "\n",
        "  # Flatten out process dimension\n",
        "  batch['input_signal'] = batch['input_signal'][0]\n",
        "  batch['batch_mask'] = batch['batch_mask'][0]\n",
        "\n",
        "  # Add prediction targets\n",
        "  batch['targets'] = lsm_mae_utils.get_targets(batch, config)\n",
        "\n",
        "  if rng is None:\n",
        "    # Always use the same seed, so that eval is as consistent as possible\n",
        "    rng = jax.random.PRNGKey(config.rng_seed)\n",
        "\n",
        "  # Patch and mask img.\n",
        "  mask_info = patch_and_mask_img(batch['input_signal'], rng, config)\n",
        "\n",
        "  # Calculate Baselines\n",
        "  logits_dict, aux = fit_MICE_baselines(\n",
        "      model, batch['input_signal'], mask_info, config\n",
        "  )\n",
        "\n",
        "  metrics_dict = dict()\n",
        "  metrics_dict['MICE'] = metrics_fn(\n",
        "      logits_dict['MICE'], aux['token_mask'], batch\n",
        "  )\n",
        "\n",
        "  return metrics_dict, logits_dict, aux\n",
        "\n",
        "# OLD\n",
        "# def regression_metrics_function(\n",
        "#     predictions: jnp.ndarray,\n",
        "#     prediction_masks: jnp.ndarray,\n",
        "#     batch: base_model.Batch,\n",
        "#     metrics: base_model.MetricNormalizerFnDict,\n",
        "#     axis_name: Union[str, Tuple[str, ...]] = 'batch',\n",
        "# ) -\u003e Dict[str, Tuple[float, int]]:\n",
        "#   \"\"\"Calculate metrics for the regression task.\n",
        "\n",
        "#   Currently we assume each metric_fn has the API:\n",
        "#     ```metric_fn(predictions, targets, weights)```\n",
        "#   and returns an array of shape [batch,]. We also assume that to compute\n",
        "#   the aggregate metric, one should sum across all batches, then divide by the\n",
        "#   total samples seen. In this way we currently only support metrics of the 1/N\n",
        "#   sum f(inputs, targets). Note, the caller is responsible for dividing by\n",
        "#   the normalizer when computing the mean of each metric.\n",
        "\n",
        "#   Args:\n",
        "#    predictions: Output of model in shape [batch, length, channels].\n",
        "#    prediction_masks: Masks used for masked modeling, shape [batch, length]\n",
        "#    batch: Batch (dict) with keys 'targets' and optionally 'batch_mask'.\n",
        "#    metrics: The regression metrics to evaluate. The key is the name of the\n",
        "#      metric, and the value is the metrics function, normalizer, and a bool\n",
        "#      indicating whether to apply prediction_masks.\n",
        "#    axis_name: List of axes on which we run the pmsum.\n",
        "\n",
        "#   Returns:\n",
        "#     A dict of metrics, in which keys are metrics name and values are tuples of\n",
        "#     (metric, normalizer).\n",
        "#   \"\"\"\n",
        "#   targets = batch['targets']\n",
        "#   batch_weights = batch.get('batch_mask')\n",
        "#   weights = jnp.expand_dims(batch_weights, axis=-1) * prediction_masks\n",
        "#   evaluated_metrics = {}\n",
        "#   for key, val in metrics.items():\n",
        "#     curr_weights = weights if val[2] else batch_weights\n",
        "\n",
        "#     val0 = val[0](\n",
        "#         targets,\n",
        "#         predictions,  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
        "#         curr_weights,\n",
        "#     )\n",
        "#     val1 = val[1](\n",
        "#         targets,\n",
        "#         predictions,  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
        "#         batch_weights,\n",
        "#     )\n",
        "#     evaluated_metrics[key] = (jnp.sum(val0), jnp.sum(val1))\n",
        "\n",
        "#   return evaluated_metrics  # pytype: disable=bad-return-type  # jax-ndarray\n",
        "\n",
        "\n",
        "# NEW\n",
        "def regression_metrics_function(\n",
        "    predictions: jnp.ndarray,\n",
        "    prediction_masks: jnp.ndarray,\n",
        "    batch: base_model.Batch,\n",
        "    metrics: base_model.MetricNormalizerFnDict,\n",
        "    axis_name: Union[str, Tuple[str, ...]] = 'batch',\n",
        ") -\u003e Dict[str, Tuple[float, int]]:\n",
        "  \"\"\"Calculate metrics for the regression task.\n",
        "\n",
        "  Currently we assume each metric_fn has the API:\n",
        "    ```metric_fn(predictions, targets, weights)```\n",
        "  and returns an array of shape [batch_size,]. We also assume that to compute\n",
        "  the aggregate metric, one should sum across all batches, then divide by the\n",
        "  total samples seen. In this way we currently only support metrics of the 1/N\n",
        "  sum f(inputs, targets). Note, the caller is responsible for dividing by\n",
        "  the normalizer when computing the mean of each metric.\n",
        "\n",
        "  Args:\n",
        "   predictions: Output of model in shape [batch_size, num_patches, patch_size].\n",
        "     specifically, shape [batch_size, gh * gw, ph * pw]. see\n",
        "     patchify_imputationmask func in trainers/lsm_mae_utils for more info ph and\n",
        "     pw are the time and modalities of the patches (i.e. patch size) gh and gw\n",
        "     are the total number of number of patches (i.e. num patches size)\n",
        "   prediction_masks: Masks used for masked modeling, shape [batch_size,\n",
        "     num_patches]\n",
        "   batch: Batch (dict) with keys 'targets' and optionally 'batch_mask'.\n",
        "   metrics: The regression metrics to evaluate. The key is the name of the\n",
        "     metric. The value is the metrics function, normalizer, a bool indicating\n",
        "     whether to apply prediction_masks, and a bool indicating whether to apply\n",
        "     patched_imputationmask\n",
        "   axis_name: List of axes on which we run the pmsum.\n",
        "\n",
        "  Returns:\n",
        "    A dict of metrics, in which keys are metrics name and values are tuples of\n",
        "    (metric, normalizer).\n",
        "  \"\"\"\n",
        "  targets = batch['targets']\n",
        "  batch_weights = batch.get('batch_mask')\n",
        "  # create a mask with all data points, then chip at it based on input masks\n",
        "\n",
        "  evaluated_metrics = {}\n",
        "  for key, val in metrics.items():\n",
        "    curr_weights = jnp.ones(targets.shape)\n",
        "\n",
        "    # LOSS_ONLY_MASKED_TOKENS\n",
        "    if val[2]:\n",
        "      curr_weights = jnp.expand_dims(prediction_masks, axis=-1) * curr_weights\n",
        "    # LOSS_IGNORE_IMPUTATION\n",
        "    if val[3]:\n",
        "      # see loss_function for ViTMAESingleChannelModel for a similar computation\n",
        "      curr_weights = (\n",
        "          jnp.logical_not(batch['patched_imputationmask']) * curr_weights\n",
        "      )\n",
        "\n",
        "    val0 = val[0](\n",
        "        targets,\n",
        "        predictions,  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
        "        curr_weights,\n",
        "    )\n",
        "    val1 = val[1](\n",
        "        targets,\n",
        "        predictions,  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
        "        batch_weights,\n",
        "    )\n",
        "    evaluated_metrics[key] = (jnp.sum(val0), jnp.sum(val1))\n",
        "\n",
        "  return evaluated_metrics  # pytype: disable=bad-return-type  # jax-ndarray\n",
        "\n",
        "\n",
        "def naive_evaluate(\n",
        "    *,\n",
        "    rng: jnp.ndarray,\n",
        "    config: ml_collections.ConfigDict,\n",
        "    dataset: dataset_utils.Dataset,\n",
        ") -\u003e Tuple[Any, Optional[Dict[str, float]], Optional[Dict[str, Any]]]:\n",
        "\n",
        "  # Initialize model.\n",
        "  metrics_fn = functools.partial(\n",
        "        regression_metrics_function,\n",
        "        metrics=lsm_vit_mae._REGRESSION_METRICS\n",
        "  )\n",
        "\n",
        "  valid_iter = dataset.valid_iter\n",
        "  num_valid_ex = dataset.meta_data['num_val_examples']\n",
        "  if not isinstance(valid_iter, dict):  # Only on validation set.\n",
        "    valid_iter, num_valid_ex = {'valid': valid_iter}, {'valid': num_valid_ex}\n",
        "\n",
        "  for val_name, val_iter in valid_iter.items():\n",
        "    num_ex = num_valid_ex[val_name]\n",
        "    # Ceil rounding such that we include the last incomplete batch.\n",
        "    eval_batch_size = config.get('eval_batch_size', config.batch_size)\n",
        "    total_eval_steps = int(np.ceil(num_ex / eval_batch_size))\n",
        "    steps_per_eval = config.get('steps_per_eval') or total_eval_steps\n",
        "    eval_metrics = []\n",
        "    for idx in tqdm.tqdm(range(steps_per_eval)):\n",
        "      rng, mask_rng = jax.random.split(rng)  # pylint: disable=unused-variable\n",
        "      eval_batch = next(val_iter)\n",
        "\n",
        "      # Naive Baselines\n",
        "      e_metrics, _, _ = naive_eval_step(  # pylint: disable=unused-variable\n",
        "          eval_batch,\n",
        "          metrics_fn=metrics_fn,\n",
        "          config=config,\n",
        "          debug=config.debug_eval,\n",
        "          rng=mask_rng,\n",
        "      )\n",
        "\n",
        "      eval_metrics.append(e_metrics)\n",
        "\n",
        "  return eval_metrics\n",
        "\n",
        "\n",
        "def MICE_evaluate(\n",
        "    *,\n",
        "    rng: jnp.ndarray,\n",
        "    config: ml_collections.ConfigDict,\n",
        "    dataset: dataset_utils.Dataset,\n",
        ") -\u003e Tuple[Any, Optional[Dict[str, float]], Optional[Dict[str, Any]]]:\n",
        "\n",
        "  # Initialize model.\n",
        "  metrics_fn = functools.partial(\n",
        "        regression_metrics_function,\n",
        "        metrics=lsm_vit_mae._REGRESSION_METRICS\n",
        "  )\n",
        "\n",
        "  valid_iter = dataset.valid_iter\n",
        "  num_valid_ex = dataset.meta_data['num_val_examples']\n",
        "  if not isinstance(valid_iter, dict):  # Only on validation set.\n",
        "    valid_iter, num_valid_ex = {'valid': valid_iter}, {'valid': num_valid_ex}\n",
        "\n",
        "  for val_name, val_iter in valid_iter.items():\n",
        "    num_ex = num_valid_ex[val_name]\n",
        "    # Ceil rounding such that we include the last incomplete batch.\n",
        "    eval_batch_size = config.get('eval_batch_size', config.batch_size)\n",
        "    total_eval_steps = int(np.ceil(num_ex / eval_batch_size))\n",
        "    steps_per_eval = config.get('steps_per_eval') or total_eval_steps\n",
        "    eval_metrics = []\n",
        "    for idx in tqdm.tqdm(range(steps_per_eval)):\n",
        "      rng, mask_rng = jax.random.split(rng)  # pylint: disable=unused-variable\n",
        "      eval_batch = next(val_iter)\n",
        "\n",
        "      # MICE Baseline\n",
        "      e_metrics, _, _ = MICE_eval_step(  # pylint: disable=unused-variable\n",
        "          eval_batch,\n",
        "          metrics_fn=metrics_fn,\n",
        "          config=config,\n",
        "          debug=config.debug_eval,\n",
        "          rng=mask_rng,\n",
        "      )\n",
        "\n",
        "      eval_metrics.append(e_metrics)\n",
        "\n",
        "  return eval_metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KnQDfIXvf3PZ"
      },
      "outputs": [],
      "source": [
        "# @title Calculate Metrics Function\n",
        "\n",
        "def calc_metrics(\n",
        "    eval_metrics, method_list=('linear', 'nn', 'mean_fill')\n",
        "):\n",
        "\n",
        "  calculated_metrics = dict()\n",
        "  for k in method_list:\n",
        "    mae_all = 0\n",
        "    mae_masked = 0\n",
        "    mse_all = 0\n",
        "    mse_masked = 0\n",
        "    ex_count = 0\n",
        "    for v in eval_metrics:\n",
        "      mae_sample = v[k]['mean_absolute_error_masked_ignoreimp_mean'][0]\n",
        "      mse_sample = v[k]['mean_squared_error_masked_ignoreimp_mean'][0]\n",
        "\n",
        "      # TODO: this is hacky.\n",
        "      # added to remove huge outliers in mice...\n",
        "      # if mse_sample \u003e mae_sample + 100:\n",
        "      #   continue\n",
        "\n",
        "      mae_masked += mae_sample\n",
        "      mse_masked += mse_sample\n",
        "      ex_count += v[k]['mean_absolute_error_masked_ignoreimp_mean'][1]\n",
        "\n",
        "    print(f'{k}')\n",
        "    print('MAE Masked:', mae_masked / ex_count)\n",
        "    print('MSE Masked:', mse_masked / ex_count)\n",
        "    print('Ex Count:', ex_count)\n",
        "    print()\n",
        "\n",
        "    calculated_metrics[k] = (mae_masked / ex_count, mse_masked / ex_count, ex_count)\n",
        "\n",
        "  return calculated_metrics\n",
        "\n",
        "\n",
        "def dict_to_df(data, mask_strat):\n",
        "  rows = []\n",
        "  for method, metrics in data.items():\n",
        "    row = {}\n",
        "    row['mask_strategy'] = mask_strat\n",
        "    row['method'] = method\n",
        "    row['mean_absolute_error_masked_ignoreimp_mean'] = metrics[0]\n",
        "    row['mean_squared_error_masked_ignoreimp_mean'] = metrics[1]\n",
        "    rows.append(row)\n",
        "  return pd.DataFrame(rows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_Mc0K7Jh8a6"
      },
      "source": [
        "# Run Examples and Naive Baseline Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sC1kbILdBtnz"
      },
      "outputs": [],
      "source": [
        "# @title Sample Config\n",
        "\n",
        "# To set constants.\n",
        "# 1) Dataset variables.\n",
        "TRAIN_DATASET_NAME = [\n",
        "    # 'lsm_v2_pretraining_sessions_-1_windowsize_1440_sensorfeatures_26_validonly_False_missingratio_0.2_timestamp_202504110407_doublethreshold_False',\n",
        "    'lsm_v2_pretraining_sessions_-1_windowsize_1440_sensorfeatures_26_validonly_False_missingratio_0.5_timestamp_202504110538_doublethreshold_False'\n",
        "    # 'lsm_v2_pretraining_sessions_-1_windowsize_1440_sensorfeatures_26_validonly_False_missingratio_0.8_timestamp_202504110551_doublethreshold_False',\n",
        "]\n",
        "VALID_DATASET_NAME = 'lsm_v2_missing_balanced_20250301_valid_dataset'\n",
        "LSM_PREDEFINED_CONFIGS = predefined_configs.LSM_PREDEFINED_CONFIGS\n",
        "LOSS_IGNORE_IMPUTATION = [True]\n",
        "CACHE_DATASET = True\n",
        "TRAIN_DATA_SIZES = [1_000, 10_000, 100_000, 1_000_000, 1_601_088]\n",
        "USE_DATETIME_FEATURES = False\n",
        "USE_TRAIN_AUGMENTATIONS = [False]\n",
        "TRAIN_AUGMENTATIONS = ['stretch', 'flip', 'noise']\n",
        "SHUFFLE_SEED = 42\n",
        "SHUFFLE_BUFFER_SIZE = 250_000\n",
        "\n",
        "# 2) Training / eval variables.\n",
        "NUM_TRAIN_STEPS = 100_000\n",
        "LRS = [5e-3]\n",
        "WEIGHT_DECAYS = [1e-4]\n",
        "\n",
        "# 3) Logging variables.\n",
        "LOG_EVAL_SUMMARY_STEPS = NUM_TRAIN_STEPS / 10  # STEPS_PER_EPOCH\n",
        "LOG_CHECKPOINT_STEPS = NUM_TRAIN_STEPS / 10  # LOG_EVAL_SUMMARY_STEPS * 5\n",
        "LOG_TRAIN_SUMMARY_STEPS = NUM_TRAIN_STEPS / 100\n",
        "MAX_NUM_CHECKPOINTS = int(NUM_TRAIN_STEPS / LOG_CHECKPOINT_STEPS)\n",
        "ENABLE_DUMP_MODE = False\n",
        "\n",
        "# Model variant\n",
        "VARIANT = 'S'\n",
        "\n",
        "LOSS_ONLY_MASKED_TOKENS = True\n",
        "\n",
        "# Downstream Tasks.\n",
        "\n",
        "# Linear probe eval.\n",
        "LINEAR_PROBE_USE_TRAIN_AUGMENTATIONS = False\n",
        "LINEAR_PROBE_TRAIN_AUGMENTATIONS = ['noise']\n",
        "\n",
        "# THINGS THAT CHANGE\n",
        "PATCHER_CONFIG = Patcher_Config(\n",
        "    hidden_size=384,\n",
        "    kernel_size=(10, 1),\n",
        "    groups=1,\n",
        "    mode='2d',\n",
        ")\n",
        "\n",
        "MASKER_CONFIG = Masker_Config(\n",
        "    maskstrategy_list=[\n",
        "        MaskStrategy_Config(\n",
        "            strategy='random',\n",
        "            mask_probability=0.8,\n",
        "            weight=1,\n",
        "            mask_dim='time',\n",
        "            inherited_depend=True,\n",
        "        ),\n",
        "    ],\n",
        "    on_cpu=True,\n",
        "    inherited=True,\n",
        ")\n",
        "\n",
        "\n",
        "def get_config(runlocal=''):\n",
        "  \"\"\"Returns the ViT experiment configuration.\"\"\"\n",
        "\n",
        "  runlocal = bool(runlocal)\n",
        "\n",
        "  # Experiment.\n",
        "  config = ml_collections.ConfigDict()\n",
        "  if runlocal:\n",
        "    config.runlocal = True\n",
        "  else:\n",
        "    config.runlocal = False\n",
        "\n",
        "  config.experiment_name = f'LSM V2-{TRAIN_DATASET_NAME[0]}'\n",
        "  config.shuffle_seed = SHUFFLE_SEED\n",
        "  config.loss_ignore_imputation = LOSS_IGNORE_IMPUTATION[0]\n",
        "\n",
        "  # Dataset.\n",
        "  config.data_dtype_str = 'float32'\n",
        "  config.dataset_configs = ml_collections.ConfigDict()\n",
        "  config.dataset_configs.dataset = TRAIN_DATASET_NAME[0]\n",
        "  config.dataset_configs.valid_dataset = VALID_DATASET_NAME\n",
        "  config.dataset_configs.num_classes = None\n",
        "  config.dataset_configs.train_split = 'train'  # train data split\n",
        "  config.dataset_configs.train_num_samples = TRAIN_DATA_SIZES[0]  # train sample\n",
        "  config.dataset_configs.eval_split = 'valid'\n",
        "  config.dataset_configs.eval_num_samples = 64 if runlocal else None\n",
        "  config.dataset_configs.cache_dataset = CACHE_DATASET\n",
        "  config.dataset_configs.prefetch_to_device = 2\n",
        "  # Shuffle_buffer_size is per host, so small-ish is ok.\n",
        "  config.dataset_configs.shuffle_buffer_size = (\n",
        "      256 if runlocal else SHUFFLE_BUFFER_SIZE\n",
        "  )\n",
        "  config.enable_dump_mode = ENABLE_DUMP_MODE\n",
        "  # Model.\n",
        "  version = VARIANT\n",
        "\n",
        "  version = 'Deb' if runlocal else version\n",
        "######################## paste this for gen_eval !!!!! ########################\n",
        "  config.model_name = 'lsm_vit_mae'\n",
        "  config.model = ml_collections.ConfigDict()\n",
        "  config.model.patcher_config = PATCHER_CONFIG\n",
        "  config.model.num_heads = model_constants.NUM_HEADS[version]\n",
        "  config.model.mlp_dim = model_constants.MLP_DIMS[version]\n",
        "  config.model.num_layers = model_constants.NUM_LAYERS[version]\n",
        "  config.model.dropout_rate = 0.0\n",
        "  config.model.classifier = 'none'  # Has to be \"none\" for the autoencoder\n",
        "  config.model.representation_size = None\n",
        "  config.model.positional_embedding = 'sinusoidal_2d'\n",
        "  config.model.positional_embedding_decoder = 'sinusoidal_2d'\n",
        "\n",
        "  config.model.patches = ml_collections.ConfigDict()\n",
        "  config.model.patches.size = (PH, PW)\n",
        "\n",
        "  # decoder\n",
        "  config.model.decoder_config = ml_collections.ConfigDict()\n",
        "  config.model.decoder_config.hidden_size = (\n",
        "      model_constants.DECODER_HIDDEN_SIZES[version]\n",
        "  )\n",
        "  config.model.decoder_config.mlp_dim = model_constants.DECODER_MLP_DIMS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.num_layers = model_constants.DECODER_NUM_LAYERS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.num_heads = model_constants.DECODER_NUM_HEADS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.dropout_rate = 0.0\n",
        "  config.model.decoder_config.attention_dropout_rate = 0.0\n",
        "\n",
        "  config.masked_feature_loss = ml_collections.ConfigDict()\n",
        "  config.masked_feature_loss.targets_type = 'rgb'\n",
        "  config.masked_feature_loss.loss_only_masked_tokens = LOSS_ONLY_MASKED_TOKENS\n",
        "  config.masked_feature_loss.loss_type = 'squared'  # 'squared' or 'absolute'\n",
        "\n",
        "  config.masker_config = MASKER_CONFIG\n",
        "  # Datetime features.\n",
        "  config.use_datetime_features = USE_DATETIME_FEATURES\n",
        "######################## paste this for gen_eval !!!!! ########################\n",
        "\n",
        "  # Training.\n",
        "  config.trainer_name = 'lsm_mae_trainer'\n",
        "  config.batch_size = 8 if runlocal else BATCH_SIZE\n",
        "  config.num_training_steps = 100 if runlocal else NUM_TRAIN_STEPS\n",
        "  config.log_eval_steps = LOG_EVAL_SUMMARY_STEPS\n",
        "  config.log_summary_steps = LOG_TRAIN_SUMMARY_STEPS\n",
        "  config.rng_seed = 42\n",
        "  config.use_train_augmentations = USE_TRAIN_AUGMENTATIONS[0]\n",
        "  config.train_augmentations = TRAIN_AUGMENTATIONS\n",
        "  sched = ml_collections.ConfigDict()\n",
        "  sched.re = '(.*)'\n",
        "  sched.lr_configs = ml_collections.ConfigDict()\n",
        "  sched.lr_configs.learning_rate_schedule = 'compound'\n",
        "  sched.lr_configs.factors = 'constant * cosine_decay * linear_warmup'\n",
        "  sched.lr_configs.total_steps = NUM_TRAIN_STEPS\n",
        "  sched.lr_configs.steps_per_cycle = sched.lr_configs.total_steps\n",
        "  sched.lr_configs.warmup_steps = int(NUM_TRAIN_STEPS * 0.05)\n",
        "  sched.lr_configs.base_learning_rate = LRS[0]\n",
        "  config.schedule = ml_collections.ConfigDict({'all': sched})\n",
        "\n",
        "  # *Single* optimizer.\n",
        "  optim = ml_collections.ConfigDict()\n",
        "  optim.optax_name = 'scale_by_adam'\n",
        "  # optim.optax = dict(mu_dtype='bfloat16')\n",
        "  optim.optax_configs = ml_collections.ConfigDict({  # Optimizer settings.\n",
        "      'b1': 0.9,\n",
        "      'b2': 0.95,\n",
        "  })\n",
        "  config.optax = dict(mu_dtype='bfloat16')\n",
        "  optim.max_grad_norm = 1.0\n",
        "\n",
        "  optim.weight_decay = WEIGHT_DECAYS[0]\n",
        "  optim.weight_decay_decouple = True\n",
        "  config.optimizer = optim\n",
        "\n",
        "  # Downstream Tasks.\n",
        "  # TODO(girishvn): These (0, 1) need to be adapted to LSM datasets\n",
        "  # 0) Linear Probing.\n",
        "  # 1) Fewshot.\n",
        "\n",
        "  # 2) Reconstruction Eval Tasks (Forecast and Imputation).\n",
        "  config.forecast = LSM_PREDEFINED_CONFIGS['eval_fore_1day']\n",
        "  config.imputation = LSM_PREDEFINED_CONFIGS['eval_imp_1day']\n",
        "  config.random_imputation = LSM_PREDEFINED_CONFIGS['eval_randimp_1day']\n",
        "\n",
        "  # Logging.\n",
        "  config.write_summary = True\n",
        "  config.xprof = True  # Profile using xprof.\n",
        "  config.checkpoint = True  # Do checkpointing.\n",
        "  config.checkpoint_steps = LOG_CHECKPOINT_STEPS\n",
        "  config.debug_train = False  # Debug mode during training.\n",
        "  config.debug_eval = False  # Debug mode during eval.\n",
        "  config.max_checkpoints_to_keep = MAX_NUM_CHECKPOINTS\n",
        "  # BEGIN GOOGLE-INTERNAL\n",
        "  if runlocal:\n",
        "    # Current implementation fails with UPTC.\n",
        "    config.count_flops = False\n",
        "  # END GOOGLE-INTERNAL\n",
        "\n",
        "  return config\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nFCZLfJpGdN8"
      },
      "outputs": [],
      "source": [
        "# @title Plot Naive Baseline Examples\n",
        "\n",
        "\n",
        "MASKER_CONFIG = Masker_Config(\n",
        "    maskstrategy_list=[\n",
        "        MaskStrategy_Config(\n",
        "            strategy='bar',\n",
        "            mask_probability=2/26,\n",
        "            weight=1,\n",
        "            mask_dim='modality',\n",
        "            inherited_depend=False,\n",
        "        ),\n",
        "    ],\n",
        "    on_cpu=True,\n",
        "    inherited=True,\n",
        "    strictmaskperc=0.0,\n",
        ")\n",
        "\n",
        "\n",
        "# Things to set\n",
        "BATCH_SIZE = 8\n",
        "PH = 10\n",
        "PW = 1\n",
        "\n",
        "config = get_config(runlocal=False)  # get configs\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = get_dataset.get_dataset(config, data_rng)\n",
        "\n",
        "batch_x = next(dataset.valid_iter)\n",
        "x = jnp.asarray(batch_x['input_signal'][0])\n",
        "b, h, w, c = x.shape\n",
        "\n",
        "dropout_rng, rng = jax.random.split(rng)\n",
        "# mask_info = patch_and_mask_img(x, dropout_rng, config)\n",
        "# mask_indices = mask_info['mask_indices']\n",
        "# unmasked_indices = mask_info['unmasked_indices']\n",
        "\n",
        "token_mask = batch_x['token_mask'][0]\n",
        "pixel_mask = get_pixel_mask(token_mask, PH, PW, x.shape)\n",
        "mask_info = {\n",
        "    'token_mask': token_mask,\n",
        "    'pixel_mask': pixel_mask\n",
        "}\n",
        "\n",
        "logits_dict, aux = pandas_naive_baselines(\n",
        "    x, mask_info, config\n",
        ")\n",
        "\n",
        "# NOTE: Simillar code is implemented in the function `pandas_naive_baselines`\n",
        "# Get input array.\n",
        "x_nan_masked = x.at[jnp.where(pixel_mask == 1)].set(jnp.nan)\n",
        "\n",
        "# set pixl mask\n",
        "x_batch_masked = jnp.transpose(x_nan_masked, (1, 0, 2, 3))  # b, h, w, c -\u003e h, b, w, c\n",
        "x_batch_masked = jnp.reshape(x_batch_masked, (h, b*w))  # b, w, h, c -\u003e b*w, h, c\n",
        "x_df = pd.DataFrame(x_batch_masked)\n",
        "\n",
        "# Linear Interpolate\n",
        "linear_interp_df = x_df.interpolate(method='linear', limit_direction='both', axis=0)\n",
        "linear_interp_df = linear_interp_df.fillna(0)\n",
        "\n",
        "# Nearest Neighbor Interpolate\n",
        "nn_interp_df = x_df.interpolate(method='nearest', limit_direction='both', axis=0)\n",
        "nn_interp_df = nn_interp_df.bfill().ffill()\n",
        "nn_interp_df = nn_interp_df.fillna(0)\n",
        "\n",
        "# Mean Fill Interpolate\n",
        "mean_fill_interp_df = x_df.fillna(x_df.mean())\n",
        "mean_fill_interp_df = mean_fill_interp_df.fillna(0)\n",
        "\n",
        "# Convert from df to jnp array.\n",
        "linear_interp = linear_interp_df.to_numpy()\n",
        "nn_interp = nn_interp_df.to_numpy()\n",
        "mean_fill_interp = mean_fill_interp_df.to_numpy()\n",
        "linear_interp = jnp.asarray(linear_interp)\n",
        "nn_interp = jnp.asarray(nn_interp)\n",
        "mean_fill_interp = jnp.asarray(mean_fill_interp)\n",
        "\n",
        "# Reshape to recover batch dim, feature dim, and channel dim\n",
        "linear_interp = jnp.reshape(linear_interp, (h, b, w, c))\n",
        "nn_interp = jnp.reshape(nn_interp, (h, b, w, c))\n",
        "mean_fill_interp = jnp.reshape(mean_fill_interp, (h, b, w, c))\n",
        "\n",
        "# Transpose to original shape\n",
        "linear_interp = jnp.transpose(linear_interp, (1, 0, 2, 3))\n",
        "nn_interp = jnp.transpose(nn_interp, (1, 0, 2, 3))\n",
        "mean_fill_interp = jnp.transpose(mean_fill_interp, (1, 0, 2, 3))\n",
        "\n",
        "\n",
        "# Plot example\n",
        "for idx in range(b):\n",
        "  vmin = jnp.min(x[idx])\n",
        "  vmax = jnp.max(x[idx])\n",
        "  plt.figure(figsize=(20, 20))\n",
        "  plt.imshow(jnp.transpose(x[idx], (1, 0, 2)), vmin=vmin, vmax=vmax)\n",
        "  plt.title('Input')\n",
        "\n",
        "  plt.figure(figsize=(20, 20))\n",
        "  plt.imshow(jnp.transpose(pixel_mask[idx], (1, 0, 2)))\n",
        "  plt.title('Mask')\n",
        "\n",
        "  plt.figure(figsize=(20, 20))\n",
        "  plt.imshow(jnp.transpose(x_nan_masked[idx], (1, 0, 2)), vmin=vmin, vmax=vmax)\n",
        "  plt.title('Masked Input')\n",
        "\n",
        "  plt.figure(figsize=(20, 20))\n",
        "  plt.imshow(jnp.transpose(linear_interp[idx], (1, 0, 2)), vmin=vmin, vmax=vmax)\n",
        "  plt.title('Linear Interpolation')\n",
        "\n",
        "  plt.figure(figsize=(20, 20))\n",
        "  plt.imshow(jnp.transpose(nn_interp[idx], (1, 0, 2)), vmin=vmin, vmax=vmax)\n",
        "  plt.title('Nearest Interpolation')\n",
        "\n",
        "  plt.figure(figsize=(20, 20))\n",
        "  plt.imshow(jnp.transpose(mean_fill_interp[idx], (1, 0, 2)), vmin=vmin, vmax=vmax)\n",
        "  plt.title('Mean Fill Interpolation')\n",
        "\n",
        "  plt.show()\n",
        "  print('\\n\\n\\n')\n",
        "\n",
        "\n",
        "print('BATCH KEYS\\n', batch_x.keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LWAsiA9i9So"
      },
      "source": [
        "### Run Single Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5PilBlHG5Iy"
      },
      "outputs": [],
      "source": [
        "# TO SET:\n",
        "BATCH_SIZE = 8\n",
        "PH = 10\n",
        "PW = 1\n",
        "\n",
        "MASKER_CONFIG = Masker_Config(\n",
        "    maskstrategy_list=[\n",
        "        MaskStrategy_Config(\n",
        "            strategy='random',\n",
        "            mask_probability=0.8,\n",
        "            inherited_depend=True,\n",
        "        ),\n",
        "    ],\n",
        "    on_cpu=True,\n",
        "    inherited=True,\n",
        ")\n",
        "\n",
        "# Start pipeline.\n",
        "config = get_config(runlocal=False)  # get configs\n",
        "rng = jax.random.PRNGKey(config.rng_seed)  # set seeds\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = get_dataset.get_dataset(config, data_rng)  # get dataset\n",
        "\n",
        "# Run eval.\n",
        "eval_metrics = naive_evaluate(\n",
        "  rng=rng,\n",
        "  config=config,\n",
        "  dataset=dataset,\n",
        ")\n",
        "\n",
        "# Calculate and print metrics.\n",
        "calc_metrics(eval_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g67-JqAjBTd"
      },
      "source": [
        "### Run Eval Sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slviAp3AYqyC"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "BATCH_SIZE = 8\n",
        "PH = 10\n",
        "PW = 1\n",
        "\n",
        "# --------------------------------------\n",
        "# OPTION 2: Sweep across time imputation / forecast / 0.8 random impute tasks\n",
        "# Constants: patch size 10x5\n",
        "# --------------------------------------\n",
        "\n",
        "# RANDOM_SWEEP = ['random_0.3', 'random_0.5', 'random_0.8']\n",
        "# IMPUTATION_SWEEP = ['imp_0.00695', 'imp_0.02084', 'imp_0.04167', 'imp_0.125']\n",
        "# FORECAST_SWEEP = ['forecast_0.00695', 'forecast_0.02084', 'forecast_0.04167', 'forecast_0.125']\n",
        "# MASK_SWEEP = RANDOM_SWEEP + IMPUTATION_SWEEP + FORECAST_SWEEP\n",
        "\n",
        "MASK_SWEEP = ['modality_2', 'modality_6', 'modality_12']\n",
        "\n",
        "TOTAL_MODALITIES = 26\n",
        "\n",
        "# --------------------------------------\n",
        "# RUN SWEEP PIPELINE.\n",
        "# NOTE. This may take hours / days depending on the sweep size.\n",
        "# Make sure to re-up AoD grant every 20 hours.\n",
        "# --------------------------------------\n",
        "count = 0\n",
        "metrics_list = []\n",
        "df_list = []\n",
        "print('Running naive baseline sweep...')\n",
        "for t in MASK_SWEEP:\n",
        "\n",
        "  prob = float(t.split('_')[-1])\n",
        "\n",
        "  if t == 'random_0.3':\n",
        "    VALID_DATASET_NAME = 'lsm_v2_missing_balanced_20250502_valid_dataset_bounded_30p'\n",
        "  elif t == 'random_0.5':\n",
        "    VALID_DATASET_NAME = 'lsm_v2_missing_balanced_20250301_valid_dataset_bounded_50p'\n",
        "  else:\n",
        "    VALID_DATASET_NAME = 'lsm_v2_missing_balanced_20250301_valid_dataset'\n",
        "\n",
        "  if 'random' in t:\n",
        "    MASKER_CONFIG = Masker_Config(\n",
        "      maskstrategy_list=[\n",
        "          MaskStrategy_Config(\n",
        "              strategy='random',\n",
        "              mask_probability=prob,\n",
        "              weight=1,\n",
        "              mask_dim='time',\n",
        "              inherited_depend=True,\n",
        "          ),\n",
        "      ],\n",
        "      on_cpu=True,\n",
        "      inherited=True,\n",
        "    )\n",
        "\n",
        "  elif 'imp' in t:\n",
        "    MASKER_CONFIG = Masker_Config(\n",
        "      maskstrategy_list=[\n",
        "          MaskStrategy_Config(\n",
        "              strategy='bar',\n",
        "              mask_probability=prob,\n",
        "              mask_dim='time',\n",
        "              mask_dim_contiguous=True,\n",
        "              inherited_depend=False,\n",
        "          )\n",
        "      ],\n",
        "      on_cpu=True,\n",
        "      inherited=True,\n",
        "      strictmaskperc=0.0\n",
        "    )\n",
        "\n",
        "\n",
        "  elif 'forecast' in t:\n",
        "    MASKER_CONFIG = Masker_Config(\n",
        "      maskstrategy_list=[\n",
        "          MaskStrategy_Config(\n",
        "              strategy='bar',\n",
        "              mask_probability=prob,\n",
        "              mask_dim='time',\n",
        "              mask_dim_contiguous=True,\n",
        "              mask_dim_forecasting=True,\n",
        "              inherited_depend=False,\n",
        "          )\n",
        "      ],\n",
        "      on_cpu=True,\n",
        "      inherited=True,\n",
        "      strictmaskperc=0.0\n",
        "    )\n",
        "\n",
        "  elif 'modality' in t:\n",
        "\n",
        "    MASKER_CONFIG = Masker_Config(\n",
        "      maskstrategy_list=[\n",
        "          MaskStrategy_Config(\n",
        "              strategy='bar',\n",
        "              mask_probability=prob/TOTAL_MODALITIES,\n",
        "              weight=1,\n",
        "              mask_dim='modality',\n",
        "              inherited_depend=False,\n",
        "          ),\n",
        "      ],\n",
        "      on_cpu=True,\n",
        "      inherited=True,\n",
        "      strictmaskperc=0.0,\n",
        "    )\n",
        "\n",
        "  else:\n",
        "    raise ValueError(f'Unknown masking strategy: {t}')\n",
        "\n",
        "  # Start pipeline.\n",
        "  config = get_config(runlocal=False)  # get configs\n",
        "  rng = jax.random.PRNGKey(config.rng_seed)  # set seeds\n",
        "  data_rng, rng = jax.random.split(rng)\n",
        "  dataset = get_dataset.get_dataset(config, data_rng)  # get dataset\n",
        "\n",
        "  count += 1\n",
        "  print('\\nIteration:', count)\n",
        "  print('Mask Strat:', t)\n",
        "\n",
        "  # Run eval.\n",
        "  eval_metrics = naive_evaluate(\n",
        "    rng=rng,\n",
        "    config=config,\n",
        "    dataset=dataset,\n",
        "  )\n",
        "\n",
        "  # Calculate and print metrics.\n",
        "  metrics = calc_metrics(eval_metrics)\n",
        "\n",
        "  # Pandas DF\n",
        "  df_list.append(dict_to_df(metrics, t))\n",
        "\n",
        "  # Add to list\n",
        "  metrics['masking_strategy'] = t\n",
        "  metrics_list.append(metrics)\n",
        "\n",
        "  print('Metrics:\\n', metrics)\n",
        "  print('\\n')\n",
        "\n",
        "print('\\nDone.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUEQii0xFSDR"
      },
      "outputs": [],
      "source": [
        "naive_baselines = ['mean_fill', 'nn', 'linear']\n",
        "cols = list(metrics_list[0].keys())\n",
        "for nb in naive_baselines:\n",
        "  cols.remove(nb)\n",
        "\n",
        "df = pd.DataFrame(columns=cols)\n",
        "for m in metrics_list:\n",
        "\n",
        "  # Iterate through baselines\n",
        "  for nb in naive_baselines:\n",
        "    row = dict()\n",
        "    row['naive_baseline'] = nb\n",
        "    row['min_valid_mean_absolute_error_masked'] = float(m[nb][0])\n",
        "    row['min_valid_mean_squared_error_masked'] = float(m[nb][1])\n",
        "    row['example_count'] = int(m[nb][2])\n",
        "    for col in cols:\n",
        "      row[col] = m[col]\n",
        "\n",
        "    new_row = pd.DataFrame([row])\n",
        "    df = pd.concat([df, new_row], ignore_index=True)\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5LWAsiA9i9So",
        "3g67-JqAjBTd",
        "l67_6LWibak6",
        "OVP_jEk9VJFS"
      ],
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
