{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR7UwI8ImdvS"
      },
      "source": [
        "# Classification Baselines\n",
        "## Random Forest and Logistic Regression\n",
        "\n",
        "Statistical Model Baselines for LSM Classification Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OU9GZY5__6zd"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "import io\n",
        "import functools\n",
        "import random\n",
        "import time\n",
        "from typing import Any, Callable, Dict, Iterator, Tuple, Optional, Type, Union\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "from absl import logging\n",
        "from clu import metric_writers\n",
        "from clu import periodic_actions\n",
        "from clu import platform\n",
        "\n",
        "import flax\n",
        "from flax import jax_utils\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.profiler\n",
        "\n",
        "import pandas as pd\n",
        "import ml_collections\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from typing import Optional, Sequence, Union\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import average_precision_score\n",
        "import sklearn.metrics as skmetrics\n",
        "import sklearn.preprocessing as skpreprocessing\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from colabtools import adhoc_import\n",
        "with adhoc_import.Google3():\n",
        "  from scenic.dataset_lib import dataset_utils\n",
        "  from scenic.google.xm import xm_utils\n",
        "  from scenic.model_lib.base_models import base_model\n",
        "  from scenic.model_lib.base_models import model_utils\n",
        "  from scenic.model_lib.layers import nn_ops\n",
        "  from scenic.model_lib.layers import nn_layers\n",
        "  from scenic.projects.baselines import vit\n",
        "  from scenic.train_lib import optax as scenic_optax\n",
        "  from scenic.train_lib import pretrain_utils\n",
        "  from scenic.train_lib import train_utils\n",
        "\n",
        "  from scenic.projects.multimask.models import model_utils as mm_model_utils\n",
        "\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import dataset_constants\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_activity_subset_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_mood_vs_activity_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_mood_subj_dependent_preprocessed_40sps_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_tiny_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_combined_pretrain_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_fewshot_mood_vs_activity_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_fewshot_remapped_activity_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_remapped_activity_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_biological_sex_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_binned_age_dataset\n",
        "\n",
        "  from google3.experimental.largesensormodels.scenic.models import lsm_vit as lsm_vit_mae\n",
        "  from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils import model_constants\n",
        "  from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils import model_utils as lsm_model_utils\n",
        "  from google3.experimental.largesensormodels.scenic.trainers import lsm_mae_trainer\n",
        "\n",
        "  from google3.pyglib import gfile\n",
        "\n",
        "\n",
        "Batch = Dict[str, jnp.ndarray]\n",
        "MetricFn = Callable[\n",
        "    [jnp.ndarray, jnp.ndarray, Dict[str, jnp.ndarray]],\n",
        "    Dict[str, Tuple[float, int]],\n",
        "]\n",
        "LossFn = Callable[\n",
        "    [jnp.ndarray, Batch, Optional[jnp.ndarray], jnp.ndarray], float\n",
        "]\n",
        "LrFns = Dict[str, Callable[[jnp.ndarray], jnp.ndarray]]\n",
        "Patch = Union[Tuple[int, int], Tuple[int, int, int]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "G2sBPWikSBod"
      },
      "outputs": [],
      "source": [
        "# @title mAP Function\n",
        "\n",
        "def compute_mean_avg_precision(\n",
        "    targets: Sequence[int],\n",
        "    logits: Sequence[int],\n",
        "    n_classes: int,\n",
        "    return_per_class_ap=False\n",
        "):\n",
        "  \"\"\"Computes mean average precision for multi-label classification.\n",
        "\n",
        "  Forked from: google3/third_party/py/scenic/projects/av_mae/evaluation_lib.py\n",
        "\n",
        "  Args:\n",
        "    targets: List of length num_examples - classes indexed.\n",
        "    logits: List of length num_examples - classes indexed.\n",
        "    n_classes: Int number of classes\n",
        "    return_per_class_ap: If True, return results for each class in the summary.\n",
        "\n",
        "  Returns:\n",
        "    summary: Dictionary containing the mean average precision, and maybe the\n",
        "      average precision per class.\n",
        "  \"\"\"\n",
        "  targets = np.array(targets)\n",
        "  logits = np.array(logits)\n",
        "  if logits.shape[0] != targets.shape[0]:\n",
        "    raise ValueError(\n",
        "        'Predictions and targets have different leading shape\\n'\n",
        "        f'Preds: {logits.shape}\\nTargets: {targets.shape}'\n",
        "    )\n",
        "\n",
        "  # Convert preds / targets to OHE for sklearn metric compatibility.\n",
        "  if n_classes \u003e 2:\n",
        "    labels = skpreprocessing.label_binarize(\n",
        "        targets, classes=np.arange(n_classes)\n",
        "    )\n",
        "  else:\n",
        "    labels = tf.one_hot(targets, n_classes)\n",
        "\n",
        "  # Get average precision across all classes.\n",
        "  average_precisions = []\n",
        "  summary = {}\n",
        "  for i in range(n_classes):\n",
        "    ave_precision = skmetrics.average_precision_score(\n",
        "        labels[:, i], logits[:, i]\n",
        "    )\n",
        "    if return_per_class_ap:\n",
        "      summary_key = f'class_{i}_AP'\n",
        "      summary[summary_key] = ave_precision\n",
        "    average_precisions.append(ave_precision)\n",
        "\n",
        "  # Update and return metrics.\n",
        "  summary['nanmean_AP'] = np.nanmean(average_precisions)\n",
        "  summary['mAP'] = np.mean(average_precisions)\n",
        "  return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nzqyLf09Aahd"
      },
      "outputs": [],
      "source": [
        "# @title Sample Model Config\n",
        "\n",
        "r\"\"\"A config to train a Tiny ViT MAE on LSM 5M dataset.\n",
        "\n",
        "Forked from google3/third_party/py/scenic/projects/multimask/configs/mae_cifar10_tiny.py\n",
        "\n",
        "To run on XManager:\n",
        "gxm third_party/py/scenic/google/xm/launch_xm.py -- \\\n",
        "--binary //experimental/largesensormodels/scenic:main \\\n",
        "--config=experimental/largesensormodels/scenic/configs/mae_lsm_tiny.py \\\n",
        "--platform=vlp_4x4 \\\n",
        "--exp_name=lsm_mae_tier2_TinyShallow_10_5_res \\\n",
        "--workdir=/cns/dz-d/home/xliucs/lsm/xm/\\{xid\\} \\\n",
        "--xm_resource_alloc=group:mobile-dynamic/h2o-ai-gqm-quota \\\n",
        "--priority=200\n",
        "\n",
        "To run locally:\n",
        "./third_party/py/scenic/google/runlocal.sh \\\n",
        "--uptc=\"\" \\\n",
        "--binary=//experimental/largesensormodels/scenic:main \\\n",
        "--config=$(pwd)/experimental/largesensormodels/scenic/configs/mae_lsm_tiny.py:runlocal\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# To set constants.\n",
        "DATASET_NAME = 'lsm_300min_pretraining_165K_n10'\n",
        "CACHE_DATASET = False\n",
        "TRAIN_DATA_SIZE = 1000000  # 100k train samples\n",
        "BATCH_SIZE = 8\n",
        "NUMBER_OF_EPOCH = 100\n",
        "REPEAT_DATA = False\n",
        "\n",
        "# Model variant / patch H (time steps) / patch W (features)\n",
        "VARIANT = 'TiShallow/10/5'\n",
        "LRS = [1e-3]\n",
        "TOKEN_MASK_PROB = 'constant_0.8'\n",
        "LOSS_ONLY_MASKED_TOKENS = True\n",
        "USE_DATETIME_FEATURES = True\n",
        "USE_TRAIN_AUGMENTATIONS = True\n",
        "TRAIN_AUGMENTATIONS = ['stretch', 'flip', 'noise']\n",
        "OHE_LABELS = True\n",
        "\n",
        "# Derived constants.\n",
        "TRAIN_DATA_SIZE = min(\n",
        "    TRAIN_DATA_SIZE,\n",
        "    dataset_constants.lsm_dataset_constants[DATASET_NAME]['num_train_examples']\n",
        ")\n",
        "\n",
        "STEPS_PER_EPOCH = max(1, int(TRAIN_DATA_SIZE / BATCH_SIZE))\n",
        "NUM_TRAIN_STEPS = int(NUMBER_OF_EPOCH * STEPS_PER_EPOCH)\n",
        "\n",
        "LOG_EVAL_SUMMARY_STEPS = STEPS_PER_EPOCH\n",
        "LOG_CHECKPOINT_STEPS = LOG_EVAL_SUMMARY_STEPS * 5\n",
        "MAX_NUM_CHECKPOINTS = int(NUM_TRAIN_STEPS / LOG_CHECKPOINT_STEPS)\n",
        "\n",
        "\n",
        "def get_config(runlocal=''):\n",
        "  \"\"\"Returns the ViT experiment configuration.\"\"\"\n",
        "\n",
        "  runlocal = bool(runlocal)\n",
        "\n",
        "  # Experiment.\n",
        "  config = ml_collections.ConfigDict()\n",
        "  config.experiment_name = f'electrodes-mae-{DATASET_NAME}-{TRAIN_DATA_SIZE}'\n",
        "  config.dataset_name = f'lsm_prod/{DATASET_NAME}'\n",
        "\n",
        "  # Dataset.\n",
        "  config.data_dtype_str = 'float32'\n",
        "  config.dataset_configs = ml_collections.ConfigDict()\n",
        "  config.dataset_configs.dataset = f'lsm_prod/{DATASET_NAME}'\n",
        "  # config.dataset_configs.num_classes = NUM_CLASSES\n",
        "  config.dataset_configs.train_split = 'train'  # train data split\n",
        "  config.dataset_configs.train_num_samples = TRAIN_DATA_SIZE  # train sample\n",
        "  # eval data split - note: this split is used for validation and test.\n",
        "  config.dataset_configs.eval_split = 'test'\n",
        "  config.dataset_configs.cache_dataset = CACHE_DATASET\n",
        "  config.dataset_configs.prefetch_to_device = 2\n",
        "  # Shuffle_buffer_size is per host, so small-ish is ok.\n",
        "  config.dataset_configs.shuffle_buffer_size = 250_000\n",
        "  config.dataset_configs.repeat_data = REPEAT_DATA\n",
        "  config.dataset_configs.ohe_labels = OHE_LABELS\n",
        "\n",
        "  # Model.\n",
        "  if len(VARIANT.split('/')) == 3:\n",
        "    version = VARIANT.split('/')[0]  # model variant\n",
        "    patch_h = VARIANT.split('/')[1]  # patch width\n",
        "    patch_w = VARIANT.split('/')[2]  # patch height\n",
        "  elif len(VARIANT.split('/')) == 2:\n",
        "    version = VARIANT.split('/')[0]  # model variant\n",
        "    patch_h = VARIANT.split('/')[1]  # patch width\n",
        "    patch_w = VARIANT.split('/')[1]  # patch height\n",
        "  else:\n",
        "    raise ValueError(f'Invalid model variant: {VARIANT}')\n",
        "\n",
        "  version = 'Deb' if runlocal else version\n",
        "  config.model_name = 'lsm_vit_mae'\n",
        "  config.model = ml_collections.ConfigDict()\n",
        "  # encoder\n",
        "  config.model.hidden_size = model_constants.HIDDEN_SIZES[version]\n",
        "  config.model.patches = ml_collections.ConfigDict()\n",
        "  config.model.patches.size = tuple([int(patch_h), int(patch_w)])\n",
        "  config.model.num_heads = model_constants.NUM_HEADS[version]\n",
        "  config.model.mlp_dim = model_constants.MLP_DIMS[version]\n",
        "  config.model.num_layers = model_constants.NUM_LAYERS[version]\n",
        "  config.model.dropout_rate = 0.0\n",
        "  config.model.classifier = 'none'  # Has to be \"none\" for the autoencoder\n",
        "  config.model.representation_size = None\n",
        "  config.model.positional_embedding = 'sinusoidal_2d'\n",
        "  config.model.positional_embedding_decoder = 'sinusoidal_2d'\n",
        "  # decoder\n",
        "  config.model.decoder_config = ml_collections.ConfigDict()\n",
        "  config.model.decoder_config.hidden_size = (\n",
        "      model_constants.DECODER_HIDDEN_SIZES[version]\n",
        "  )\n",
        "  config.model.decoder_config.mlp_dim = model_constants.DECODER_MLP_DIMS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.num_layers = model_constants.DECODER_NUM_LAYERS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.num_heads = model_constants.DECODER_NUM_HEADS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.dropout_rate = 0.0\n",
        "  config.model.decoder_config.attention_dropout_rate = 0.0\n",
        "\n",
        "  config.masked_feature_loss = ml_collections.ConfigDict()\n",
        "  config.masked_feature_loss.targets_type = 'rgb'\n",
        "  config.masked_feature_loss.token_mask_probability = TOKEN_MASK_PROB\n",
        "  config.masked_feature_loss.loss_only_masked_tokens = LOSS_ONLY_MASKED_TOKENS\n",
        "  config.masked_feature_loss.loss_type = 'squared'  # 'squared' or 'absolute'\n",
        "\n",
        "  # Datetime features.\n",
        "  config.use_datetime_features = USE_DATETIME_FEATURES\n",
        "\n",
        "  # Training.\n",
        "  config.trainer_name = 'lsm_mae_trainer'\n",
        "  config.batch_size = BATCH_SIZE\n",
        "  config.num_training_steps = NUM_TRAIN_STEPS\n",
        "  config.log_eval_steps = LOG_EVAL_SUMMARY_STEPS\n",
        "  config.log_summary_steps = LOG_EVAL_SUMMARY_STEPS\n",
        "  config.rng_seed = 42\n",
        "  config.use_train_augmentations = USE_TRAIN_AUGMENTATIONS\n",
        "  config.train_augmentations = TRAIN_AUGMENTATIONS\n",
        "  sched = ml_collections.ConfigDict()\n",
        "  sched.re = '(.*)'\n",
        "  sched.lr_configs = ml_collections.ConfigDict()\n",
        "  sched.lr_configs.learning_rate_schedule = 'compound'\n",
        "  sched.lr_configs.factors = 'constant * cosine_decay * linear_warmup'\n",
        "  sched.lr_configs.total_steps = NUM_TRAIN_STEPS\n",
        "  sched.lr_configs.steps_per_cycle = sched.lr_configs.total_steps\n",
        "  sched.lr_configs.warmup_steps = STEPS_PER_EPOCH * NUMBER_OF_EPOCH * 0.05\n",
        "  sched.lr_configs.base_learning_rate = LRS[0]\n",
        "  config.schedule = ml_collections.ConfigDict({'all': sched})\n",
        "\n",
        "  # *Single* optimizer.\n",
        "  optim = ml_collections.ConfigDict()\n",
        "  optim.optax_name = 'scale_by_adam'\n",
        "  # optim.optax = dict(mu_dtype='bfloat16')\n",
        "  optim.optax_configs = ml_collections.ConfigDict({  # Optimizer settings.\n",
        "      'b1': 0.9,\n",
        "      'b2': 0.95,\n",
        "  })\n",
        "  config.optax = dict(mu_dtype='bfloat16')\n",
        "  optim.max_grad_norm = 1.0\n",
        "\n",
        "  optim.weight_decay = 1e-4\n",
        "  optim.weight_decay_decouple = True\n",
        "  config.optimizer = optim\n",
        "\n",
        "  # Logging.\n",
        "  config.write_summary = True\n",
        "  config.xprof = True  # Profile using xprof.\n",
        "  config.checkpoint = True  # Do checkpointing.\n",
        "  config.checkpoint_steps = LOG_CHECKPOINT_STEPS\n",
        "  config.debug_train = False  # Debug mode during training.\n",
        "  config.debug_eval = False  # Debug mode during eval.\n",
        "  config.max_checkpoints_to_keep = MAX_NUM_CHECKPOINTS\n",
        "  # BEGIN GOOGLE-INTERNAL\n",
        "  if runlocal:\n",
        "    # Current implementation fails with UPTC.\n",
        "    config.count_flops = False\n",
        "  # END GOOGLE-INTERNAL\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "# BEGIN GOOGLE-INTERNAL\n",
        "def get_hyper(hyper):\n",
        "  \"\"\"Defines the hyper-parameters sweeps for doing grid search.\"\"\"\n",
        "  return hyper.product([\n",
        "      hyper.sweep('config.schedule.all.lr_configs.base_learning_rate', LRS),\n",
        "  ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XHgDzHKnkfmL"
      },
      "outputs": [],
      "source": [
        "# @title Set random seeds\n",
        "\n",
        "random.seed(42)  # Replace 42 with any integer seed\n",
        "np.random.seed(42)  # Replace 42 with any integer seed\n",
        "tf.random.set_seed(42)  # Replace 42 with any integer seed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA9beDS7KRjF"
      },
      "source": [
        "# Activity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_nAQDyJZA4v4"
      },
      "outputs": [],
      "source": [
        "# @title Load Train and Test Datasets\n",
        "\n",
        "# Constants\n",
        "DATASET_NAME = 'lsm_300min_600_activities_remapped_8class'\n",
        "TRAIN_DATA_SIZE = None\n",
        "\n",
        "# Load Train Dataset\n",
        "BATCH_SIZE = dataset_constants.lsm_dataset_constants[\n",
        "    'lsm_300min_600_activities_remapped_8class'\n",
        "]['num_train_examples']\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = lsm_remapped_activity_dataset.get_dataset(config, data_rng)\n",
        "\n",
        "# Get Batch and Format to Desired Shape\n",
        "data_batch = next(dataset.train_iter)\n",
        "data_keys = data_batch.keys()\n",
        "p, b, t, f, c = data_batch['input_signal'].shape\n",
        "X_train = jnp.reshape(data_batch['input_signal'], [p*b, -1])\n",
        "y_train = jnp.reshape(data_batch['label'], [p*b, -1])\n",
        "\n",
        "# Load Test Dataset\n",
        "BATCH_SIZE = dataset_constants.lsm_dataset_constants[\n",
        "    'lsm_300min_600_activities_remapped_8class'\n",
        "]['num_test_examples']\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = lsm_remapped_activity_dataset.get_dataset(config, data_rng)\n",
        "\n",
        "# Get Batch and Format to Desired Shape\n",
        "data_batch = next(dataset.valid_iter)\n",
        "data_keys = data_batch.keys()\n",
        "p, b, t, f, c = data_batch['input_signal'].shape\n",
        "X_test = jnp.reshape(data_batch['input_signal'], [p*b, -1])\n",
        "y_test = jnp.reshape(data_batch['label'], [p*b, -1])\n",
        "\n",
        "# Print\n",
        "print('Data keys: ', data_keys)\n",
        "print('X_train shape: ', X_train.shape)\n",
        "print('y_train shape: ', y_train.shape)\n",
        "print('X_test shape: ', X_test.shape)\n",
        "print('y_test shape: ', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-EU8F5zuDWlm"
      },
      "outputs": [],
      "source": [
        "# @title Random Forest\n",
        "n_estimators = 500\n",
        "max_samples = 0.75\n",
        "max_depth = 10\n",
        "clf = RandomForestClassifier(\n",
        "    n_estimators=n_estimators,\n",
        "    max_depth=max_depth, random_state=42,\n",
        "    max_samples=max_samples, n_jobs=5,\n",
        ")\n",
        "\n",
        "y_train_rf = jnp.argmax(y_train, axis=1)\n",
        "y_test_rf = jnp.argmax(y_test, axis=1)\n",
        "\n",
        "# Fit Model\n",
        "clf.fit(X_train, y_train_rf)\n",
        "\n",
        "# Eval\n",
        "# Accuracy\n",
        "acc = clf.score(X_test, y_test_rf)\n",
        "\n",
        "# Mean Average Precision\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "mean_average_precision = average_precision_score(y_test, y_probs, average='macro')\n",
        "\n",
        "print('Acc:', acc)\n",
        "print('mAP:', mean_average_precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tkMp5ApYSUva"
      },
      "outputs": [],
      "source": [
        "# @title Logistic Regression\n",
        "\n",
        "clf = LogisticRegression(random_state=42, n_jobs=1, multi_class='ovr', max_iter=1000, solver='saga')\n",
        "\n",
        "y_train_lr = jnp.argmax(y_train, axis=1)\n",
        "y_test_lr = jnp.argmax(y_test, axis=1)\n",
        "\n",
        "# Fit Model\n",
        "start_t = time.time()\n",
        "clf.fit(X_train, y_train_lr)\n",
        "end_t = time.time()\n",
        "\n",
        "print('\\nFit time', end_t - start_t)\n",
        "\n",
        "# Eval\n",
        "# Accuracy\n",
        "acc = clf.score(X_test, y_test_lr)\n",
        "\n",
        "# Mean Average Precision\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "mean_average_precision = average_precision_score(y_test, y_probs, average='macro')\n",
        "\n",
        "print('\\nAcc:', acc)\n",
        "print('mAP:', mean_average_precision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNHh7_kyKX_S"
      },
      "source": [
        "# Exercise Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "H9HlRQN6GrB5"
      },
      "outputs": [],
      "source": [
        "# @title Load Train and Test Datasets\n",
        "\n",
        "# Constants\n",
        "DATASET_NAME = 'lsm_300min_mood_vs_activity'\n",
        "TRAIN_DATA_SIZE = None\n",
        "\n",
        "# Load Train Dataset\n",
        "act_samples = dataset_constants.lsm_dataset_constants[\n",
        "    'lsm_300min_600_activities_balanced']['num_train_examples']\n",
        "mood_samples = dataset_constants.lsm_dataset_constants[\n",
        "    'lsm_300min_2000_mood_balanced']['num_train_examples']\n",
        "BATCH_SIZE = act_samples + mood_samples\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = lsm_mood_vs_activity_dataset.get_dataset(config, data_rng)\n",
        "\n",
        "# Get Batch and Format to Desired Shape\n",
        "data_batch = next(dataset.train_iter)\n",
        "data_keys = data_batch.keys()\n",
        "p, b, t, f, c = data_batch['input_signal'].shape\n",
        "X_train = jnp.reshape(data_batch['input_signal'], [p*b, -1])\n",
        "y_train = jnp.reshape(data_batch['label'], [p*b, -1])\n",
        "\n",
        "# Load Test Dataset\n",
        "act_samples = dataset_constants.lsm_dataset_constants[\n",
        "    'lsm_300min_600_activities_balanced']['num_test_examples']\n",
        "mood_samples = dataset_constants.lsm_dataset_constants[\n",
        "    'lsm_300min_2000_mood_balanced']['num_test_examples']\n",
        "BATCH_SIZE = act_samples + mood_samples\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = lsm_mood_vs_activity_dataset.get_dataset(config, data_rng)\n",
        "\n",
        "# Get Batch and Format to Desired Shape\n",
        "data_batch = next(dataset.valid_iter)\n",
        "data_keys = data_batch.keys()\n",
        "p, b, t, f, c = data_batch['input_signal'].shape\n",
        "X_test = jnp.reshape(data_batch['input_signal'], [p*b, -1])\n",
        "y_test = jnp.reshape(data_batch['label'], [p*b, -1])\n",
        "\n",
        "# Print\n",
        "print('Data keys: ', data_keys)\n",
        "print('X_train shape: ', X_train.shape)\n",
        "print('y_train shape: ', y_train.shape)\n",
        "print('X_test shape: ', X_test.shape)\n",
        "print('y_test shape: ', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iKTWkB4iR2Mj"
      },
      "outputs": [],
      "source": [
        "# @title Random Forest\n",
        "n_estimators = 500\n",
        "max_samples = 0.75\n",
        "max_depth = 10\n",
        "clf = RandomForestClassifier(\n",
        "    n_estimators=n_estimators,\n",
        "    max_depth=max_depth, random_state=42,\n",
        "    max_samples=max_samples, n_jobs=5,\n",
        ")\n",
        "\n",
        "y_train_rf = jnp.argmax(y_train, axis=1)\n",
        "y_test_rf = jnp.argmax(y_test, axis=1)\n",
        "\n",
        "# Fit Model\n",
        "clf.fit(X_train, y_train_rf)\n",
        "\n",
        "# Eval\n",
        "# Accuracy\n",
        "acc = clf.score(X_test, y_test_rf)\n",
        "\n",
        "# Mean Average Precision\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "mean_average_precision = average_precision_score(y_test, y_probs, average='macro')\n",
        "\n",
        "print('Acc:', acc)\n",
        "print('mAP:', mean_average_precision)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TheZ9vxuuczA"
      },
      "outputs": [],
      "source": [
        "# @title Logistic Regression\n",
        "\n",
        "clf = LogisticRegression(random_state=42, n_jobs=1, multi_class='ovr', max_iter=1000, solver='saga')\n",
        "\n",
        "y_train_lr = jnp.argmax(y_train, axis=1)\n",
        "y_test_lr = jnp.argmax(y_test, axis=1)\n",
        "\n",
        "# Fit Model\n",
        "start_t = time.time()\n",
        "clf.fit(X_train, y_train_lr)\n",
        "end_t = time.time()\n",
        "\n",
        "print('\\nFit time', end_t - start_t)\n",
        "\n",
        "# Eval\n",
        "# Accuracy\n",
        "acc = clf.score(X_test, y_test_lr)\n",
        "\n",
        "# Mean Average Precision\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "mean_average_precision = average_precision_score(y_test, y_probs, average='macro')\n",
        "\n",
        "print('\\nAcc:', acc)\n",
        "print('mAP:', mean_average_precision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqz28pZwWaLB"
      },
      "source": [
        "# Subj. Dependent Mood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "myUOt8foWc4d"
      },
      "outputs": [],
      "source": [
        "# @title Load Train and Test Datasets\n",
        "\n",
        "# Constants\n",
        "DATASET_NAME = 'lsm_300min_2000_mood_subject_dependent_preprocessed_40sps'\n",
        "TRAIN_DATA_SIZE = None\n",
        "\n",
        "# Load Train Dataset\n",
        "BATCH_SIZE = 3553\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = (\n",
        "    lsm_mood_subj_dependent_preprocessed_40sps_dataset.get_preprocessed_dataset(\n",
        "        config, data_rng,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Get Batch and Format to Desired Shape\n",
        "data_batch = next(dataset.train_iter)\n",
        "data_keys = data_batch.keys()\n",
        "p, b, t, f, c = data_batch['input_signal'].shape\n",
        "X_train = jnp.reshape(data_batch['input_signal'], [p*b, -1])\n",
        "y_train = jnp.reshape(data_batch['label'], [p*b, -1])\n",
        "\n",
        "# Load Test Dataset\n",
        "BATCH_SIZE = 1154\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = (\n",
        "    lsm_mood_subj_dependent_preprocessed_40sps_dataset.get_preprocessed_dataset(\n",
        "        config, data_rng,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Get Batch and Format to Desired Shape\n",
        "data_batch = next(dataset.valid_iter)\n",
        "data_keys = data_batch.keys()\n",
        "p, b, t, f, c = data_batch['input_signal'].shape\n",
        "X_test = jnp.reshape(data_batch['input_signal'], [p*b, -1])\n",
        "y_test = jnp.reshape(data_batch['label'], [p*b, -1])\n",
        "\n",
        "# Print\n",
        "print('Data keys: ', data_keys)\n",
        "print('X_train shape: ', X_train.shape)\n",
        "print('y_train shape: ', y_train.shape)\n",
        "print('X_test shape: ', X_test.shape)\n",
        "print('y_test shape: ', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fFDGrDdFXMcN"
      },
      "outputs": [],
      "source": [
        "# @title Random Forest\n",
        "n_estimators = 500\n",
        "max_samples = 0.75\n",
        "max_depth = 10\n",
        "clf = RandomForestClassifier(\n",
        "    n_estimators=n_estimators,\n",
        "    max_depth=max_depth, random_state=42,\n",
        "    max_samples=max_samples, n_jobs=5,\n",
        ")\n",
        "\n",
        "y_train_rf = jnp.argmax(y_train, axis=1)\n",
        "y_test_rf = jnp.argmax(y_test, axis=1)\n",
        "\n",
        "# Fit Model\n",
        "clf.fit(X_train, y_train_rf)\n",
        "\n",
        "# Eval\n",
        "# Accuracy\n",
        "acc = clf.score(X_test, y_test_rf)\n",
        "\n",
        "# Mean Average Precision\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "mean_average_precision = average_precision_score(y_test, y_probs, average='macro')\n",
        "\n",
        "print('Acc:', acc)\n",
        "print('mAP:', mean_average_precision)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LGhdXb5jX0IL"
      },
      "outputs": [],
      "source": [
        "# @title Logistic Regression\n",
        "max_iter = 1000\n",
        "\n",
        "clf = LogisticRegression(random_state=42, n_jobs=1, multi_class='ovr', max_iter=max_iter, solver='saga')\n",
        "\n",
        "y_train_lr = jnp.argmax(y_train, axis=1)\n",
        "y_test_lr = jnp.argmax(y_test, axis=1)\n",
        "\n",
        "# Fit Model\n",
        "start_t = time.time()\n",
        "clf.fit(X_train, y_train_lr)\n",
        "end_t = time.time()\n",
        "\n",
        "print('\\nFit time', end_t - start_t)\n",
        "\n",
        "# Eval\n",
        "# Accuracy\n",
        "acc = clf.score(X_test, y_test_lr)\n",
        "\n",
        "# Mean Average Precision\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "mean_average_precision = average_precision_score(y_test, y_probs, average='macro')\n",
        "\n",
        "print('\\nAcc:', acc)\n",
        "print('mAP:', mean_average_precision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsBSG5qfK9Xv"
      },
      "source": [
        "# Biological Sex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BpqmOvk5LABS"
      },
      "outputs": [],
      "source": [
        "# @title Load Train and Test Datasets\n",
        "\n",
        "# Constants\n",
        "DATASET_NAME = 'lsm_300min_600_biological_sex'\n",
        "TRAIN_DATA_SIZE = None\n",
        "\n",
        "# Load Train Dataset\n",
        "BATCH_SIZE = 14203\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = (\n",
        "    lsm_biological_sex_dataset.get_dataset(\n",
        "        config, data_rng,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Get Batch and Format to Desired Shape\n",
        "data_batch = next(dataset.train_iter)\n",
        "data_keys = data_batch.keys()\n",
        "p, b, t, f, c = data_batch['input_signal'].shape\n",
        "X_train = jnp.reshape(data_batch['input_signal'], [p*b, -1])\n",
        "y_train = jnp.reshape(data_batch['label'], [p*b, -1])\n",
        "\n",
        "# Load Test Dataset\n",
        "BATCH_SIZE = 3250\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = (\n",
        "    lsm_biological_sex_dataset.get_dataset(\n",
        "        config, data_rng,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Get Batch and Format to Desired Shape\n",
        "data_batch = next(dataset.valid_iter)\n",
        "data_keys = data_batch.keys()\n",
        "p, b, t, f, c = data_batch['input_signal'].shape\n",
        "X_test = jnp.reshape(data_batch['input_signal'], [p*b, -1])\n",
        "y_test = jnp.reshape(data_batch['label'], [p*b, -1])\n",
        "\n",
        "# Print\n",
        "print('Data keys: ', data_keys)\n",
        "print('X_train shape: ', X_train.shape)\n",
        "print('y_train shape: ', y_train.shape)\n",
        "print('X_test shape: ', X_test.shape)\n",
        "print('y_test shape: ', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2CWEFNjkLcMd"
      },
      "outputs": [],
      "source": [
        "# @title Random Forest\n",
        "n_estimators = 500\n",
        "max_samples = 0.75\n",
        "max_depth = 10\n",
        "clf = RandomForestClassifier(\n",
        "    n_estimators=n_estimators,\n",
        "    max_depth=max_depth, random_state=42,\n",
        "    max_samples=max_samples, n_jobs=5,\n",
        ")\n",
        "\n",
        "y_train_rf = jnp.argmax(y_train, axis=1)\n",
        "y_test_rf = jnp.argmax(y_test, axis=1)\n",
        "\n",
        "# Fit Model\n",
        "clf.fit(X_train, y_train_rf)\n",
        "\n",
        "# Eval\n",
        "# Accuracy\n",
        "acc = clf.score(X_test, y_test_rf)\n",
        "\n",
        "# Mean Average Precision\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "mean_average_precision = average_precision_score(y_test, y_probs, average='macro')\n",
        "\n",
        "print('Acc:', acc)\n",
        "print('mAP:', mean_average_precision)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EZWwdQxqMWEl"
      },
      "outputs": [],
      "source": [
        "# @title Logistic Regression\n",
        "\n",
        "clf = LogisticRegression(random_state=42, n_jobs=1, multi_class='ovr', max_iter=1000, solver='saga')\n",
        "\n",
        "y_train_lr = jnp.argmax(y_train, axis=1)\n",
        "y_test_lr = jnp.argmax(y_test, axis=1)\n",
        "\n",
        "# Fit Model\n",
        "start_t = time.time()\n",
        "clf.fit(X_train, y_train_lr)\n",
        "end_t = time.time()\n",
        "\n",
        "print('\\nFit time', end_t - start_t)\n",
        "\n",
        "# Eval\n",
        "# Accuracy\n",
        "acc = clf.score(X_test, y_test_lr)\n",
        "\n",
        "# Mean Average Precision\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "mean_average_precision = average_precision_score(y_test, y_probs, average='macro')\n",
        "\n",
        "print('\\nAcc:', acc)\n",
        "print('mAP:', mean_average_precision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9GPvauLu4_V"
      },
      "source": [
        "# Binned Age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vAMGneQ5u59Y"
      },
      "outputs": [],
      "source": [
        "# @title Load Train and Test Datasets\n",
        "\n",
        "# Constants\n",
        "DATASET_NAME = 'lsm_300min_600_binnned_age'\n",
        "TRAIN_DATA_SIZE = None\n",
        "\n",
        "# Load Train Dataset\n",
        "BATCH_SIZE = 14372\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = (\n",
        "    lsm_binned_age_dataset.get_dataset(\n",
        "        config, data_rng,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Get Batch and Format to Desired Shape\n",
        "data_batch = next(dataset.train_iter)\n",
        "data_keys = data_batch.keys()\n",
        "p, b, t, f, c = data_batch['input_signal'].shape\n",
        "X_train = jnp.reshape(data_batch['input_signal'], [p*b, -1])\n",
        "y_train = jnp.reshape(data_batch['label'], [p*b, -1])\n",
        "\n",
        "# Load Test Dataset\n",
        "BATCH_SIZE = 3262\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = (\n",
        "    lsm_binned_age_dataset.get_dataset(\n",
        "        config, data_rng,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Get Batch and Format to Desired Shape\n",
        "data_batch = next(dataset.valid_iter)\n",
        "data_keys = data_batch.keys()\n",
        "p, b, t, f, c = data_batch['input_signal'].shape\n",
        "X_test = jnp.reshape(data_batch['input_signal'], [p*b, -1])\n",
        "y_test = jnp.reshape(data_batch['label'], [p*b, -1])\n",
        "\n",
        "# Print\n",
        "print('Data keys: ', data_keys)\n",
        "print('X_train shape: ', X_train.shape)\n",
        "print('y_train shape: ', y_train.shape)\n",
        "print('X_test shape: ', X_test.shape)\n",
        "print('y_test shape: ', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "O2kU4NOuwPtt"
      },
      "outputs": [],
      "source": [
        "# @title Random Forest\n",
        "n_estimators = 500\n",
        "max_samples = 0.75\n",
        "max_depth = 10\n",
        "clf = RandomForestClassifier(\n",
        "    n_estimators=n_estimators,\n",
        "    max_depth=max_depth, random_state=42,\n",
        "    max_samples=max_samples, n_jobs=5,\n",
        ")\n",
        "\n",
        "y_train_rf = jnp.argmax(y_train, axis=1)\n",
        "y_test_rf = jnp.argmax(y_test, axis=1)\n",
        "\n",
        "# Fit Model\n",
        "clf.fit(X_train, y_train_rf)\n",
        "\n",
        "# Eval\n",
        "# Accuracy\n",
        "acc = clf.score(X_test, y_test_rf)\n",
        "\n",
        "# Mean Average Precision\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "mean_average_precision = average_precision_score(y_test, y_probs, average='macro')\n",
        "\n",
        "print('Acc:', acc)\n",
        "print('mAP:', mean_average_precision)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sKwdScp3y5rE"
      },
      "outputs": [],
      "source": [
        "# @title Logistic Regression\n",
        "\n",
        "clf = LogisticRegression(random_state=42, n_jobs=1, multi_class='ovr', max_iter=1000, solver='saga')\n",
        "\n",
        "y_train_lr = jnp.argmax(y_train, axis=1)\n",
        "y_test_lr = jnp.argmax(y_test, axis=1)\n",
        "\n",
        "# Fit Model\n",
        "start_t = time.time()\n",
        "clf.fit(X_train, y_train_lr)\n",
        "end_t = time.time()\n",
        "\n",
        "print('\\nFit time', end_t - start_t)\n",
        "\n",
        "# Eval\n",
        "# Accuracy\n",
        "acc = clf.score(X_test, y_test_lr)\n",
        "\n",
        "# Mean Average Precision\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "mean_average_precision = average_precision_score(y_test, y_probs, average='macro')\n",
        "\n",
        "print('\\nAcc:', acc)\n",
        "print('mAP:', mean_average_precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qapCwUgzy8qB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//fitbit/research/sensing/electrodes/colab:rl_colab",
        "kind": "shared"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
