{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl8y8d9po67C"
      },
      "source": [
        "# LSM Pretrain Dataset Explorer\n",
        "##### Colab Kernel (Electrodes)\n",
        "##### Dataset (Electrodes)\n",
        "\n",
        "Grants command for Access on Demand (AoD):\n",
        "\n",
        "https://grants.corp.google.com/#/grants?request=20h%2Fchr-ards-electrodes-deid-colab-jobs\u0026reason=b%2F314799341\n",
        "\n",
        "### About This Notebook:\n",
        "This notebook explores pretrain (unlabeled) datasets for the LSM project. It loads and prints dataset meta data, and plots sample data for the following datasets:\n",
        "\n",
        "**Actively Used:**\n",
        "1. Balanced Pretrain Dataset (10 Samples Per 165k Subjects): `lsm_300min_pretraining_165K_n10`\n",
        "2. Combined 8M Pretrain Dataset: `lsm_300min_pretraining_165K_n10` + `lsm_300min_10M_impute`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GZBYUXeEUbg"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1MCewaurabYh"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "import io\n",
        "import functools\n",
        "from typing import Any, Callable, Dict, Iterator, Tuple, Optional, Type, Union\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "from absl import logging\n",
        "from clu import metric_writers\n",
        "from clu import periodic_actions\n",
        "from clu import platform\n",
        "\n",
        "import flax\n",
        "from flax import jax_utils\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.profiler\n",
        "\n",
        "import pandas as pd\n",
        "import ml_collections\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from colabtools import adhoc_import\n",
        "with adhoc_import.Google3():\n",
        "  from scenic.dataset_lib import dataset_utils\n",
        "  from scenic.google.xm import xm_utils\n",
        "  from scenic.model_lib.base_models import base_model\n",
        "  from scenic.model_lib.base_models import model_utils\n",
        "  from scenic.model_lib.layers import nn_ops\n",
        "  from scenic.model_lib.layers import nn_layers\n",
        "  from scenic.projects.baselines import vit\n",
        "  from scenic.train_lib import optax as scenic_optax\n",
        "  from scenic.train_lib import pretrain_utils\n",
        "  from scenic.train_lib import train_utils\n",
        "\n",
        "  from scenic.projects.multimask.models import model_utils as mm_model_utils\n",
        "\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import dataset_constants\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_activity_subset_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_mood_vs_activity_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_tiny_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_combined_pretrain_dataset\n",
        "\n",
        "  from google3.experimental.largesensormodels.scenic.models import lsm_vit as lsm_vit_mae\n",
        "  from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils import model_constants\n",
        "  from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils import model_utils as lsm_model_utils\n",
        "  from google3.experimental.largesensormodels.scenic.trainers import lsm_mae_trainer\n",
        "\n",
        "  from google3.pyglib import gfile\n",
        "\n",
        "\n",
        "Batch = Dict[str, jnp.ndarray]\n",
        "MetricFn = Callable[\n",
        "    [jnp.ndarray, jnp.ndarray, Dict[str, jnp.ndarray]],\n",
        "    Dict[str, Tuple[float, int]],\n",
        "]\n",
        "LossFn = Callable[\n",
        "    [jnp.ndarray, Batch, Optional[jnp.ndarray], jnp.ndarray], float\n",
        "]\n",
        "LrFns = Dict[str, Callable[[jnp.ndarray], jnp.ndarray]]\n",
        "Patch = Union[Tuple[int, int], Tuple[int, int, int]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "R4-nK9tgOOXv"
      },
      "outputs": [],
      "source": [
        "# @title Helper Functions\n",
        "\n",
        "\n",
        "def explore_sample_data(dataset, ohe_labels):\n",
        "  p_idx = 0\n",
        "  batch_idx = 0\n",
        "  x = next(dataset.valid_iter)\n",
        "\n",
        "  #Pparse data example\n",
        "  input = x['input_signal']  # Sensor signals\n",
        "  plt_input = jnp.transpose(input, (0, 1, 3, 2, 4))\n",
        "\n",
        "  datetime = x['datetime_signal']  # Datetime signals\n",
        "  plt_datetime = jnp.transpose(datetime, (0, 1, 3, 2, 4))\n",
        "\n",
        "  print(f'Example keys {list(x.keys())}\\n')\n",
        "  print('Exercise Log', x['exercise_log'])  # Exercise log\n",
        "  print('Log Value', x['log_value'] + 65536)  # Log value\n",
        "  print('\\nMood Log', x['mood_log'])  # Mood log\n",
        "\n",
        "  if ohe_labels:\n",
        "    print(f\"\\nLabel {x['label']}\\n\")  # Stress label\n",
        "\n",
        "  # Plot input signal\n",
        "  plt.figure(figsize= (15, 4))\n",
        "  plt.imshow(plt_input[p_idx][batch_idx])\n",
        "  plt.title('Sensor Inputs');\n",
        "\n",
        "  plt.figure(figsize= (15, 4))\n",
        "  plt.imshow(plt_datetime[p_idx][batch_idx])\n",
        "  plt.title('Datetime Inputs');\n",
        "\n",
        "\n",
        "def dataset_split_information(ds, event, offset=65536):\n",
        "  \"\"\"Given a dataset split retuns a dictionary of stats/metadata.\"\"\"\n",
        "  # event can be ['exercise', 'mood', 'stress']\n",
        "\n",
        "  if event == 'exercise':\n",
        "    log_key = 'exercise_log'\n",
        "  elif event == 'mood' or event == 'stress':\n",
        "    log_key = 'mood_log'\n",
        "  else:\n",
        "    raise ValueError(f'event must be exercise, mood, or stress')\n",
        "\n",
        "  subj_ids = []\n",
        "  ex_count = 0\n",
        "  log_values = []\n",
        "  for d in ds:\n",
        "\n",
        "    # Subject ID\n",
        "    # ids = d['labels']['ID']\n",
        "    # ids = [i.decode() for i in ids.flatten() if type(i) is bytes]\n",
        "    # subj_ids += ids\n",
        "\n",
        "    # Total example count.\n",
        "    ex_count += jnp.sum(d['batch_mask'])\n",
        "    if ex_count % 100000 == 0:\n",
        "      print(f'running example count: {ex_count}')\n",
        "\n",
        "    # Get label\n",
        "    if event == 'exercise' or event == 'mood':\n",
        "      valid_log = jnp.where(d[log_key])  # where exercise/mood log is True\n",
        "      valid_log_value = d['log_value'][valid_log]  # get valid log values\n",
        "      # add offset value\n",
        "      valid_log_value = [v + offset for v in valid_log_value.flatten() if v != 0]\n",
        "\n",
        "    elif event == 'stress':\n",
        "      log_value = jnp.argmax(d['label'], axis=-1)\n",
        "      valid_log = jnp.where(d['batch_mask'])  # where exercise/mood log is True\n",
        "      valid_log_value = log_value[valid_log]  # get valid log values\n",
        "      valid_log_value = valid_log_value.tolist()\n",
        "\n",
        "    # append list to log values list\n",
        "    log_values += valid_log_value\n",
        "\n",
        "  info = {'num_examples': ex_count,\n",
        "          'example_subj_ids': subj_ids,\n",
        "          'log_value': log_values}\n",
        "\n",
        "  return info\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NiW84UC2pNCQ"
      },
      "outputs": [],
      "source": [
        "# @title Sample Model Config\n",
        "\n",
        "r\"\"\"A config to train a Tiny ViT MAE on LSM 5M dataset.\n",
        "\n",
        "Forked from google3/third_party/py/scenic/projects/multimask/configs/mae_cifar10_tiny.py\n",
        "\n",
        "To run on XManager:\n",
        "gxm third_party/py/scenic/google/xm/launch_xm.py -- \\\n",
        "--binary //experimental/largesensormodels/scenic:main \\\n",
        "--config=experimental/largesensormodels/scenic/configs/mae_lsm_tiny.py \\\n",
        "--platform=vlp_4x4 \\\n",
        "--exp_name=lsm_mae_tier2_TinyShallow_10_5_res \\\n",
        "--workdir=/cns/dz-d/home/xliucs/lsm/xm/\\{xid\\} \\\n",
        "--xm_resource_alloc=group:mobile-dynamic/h2o-ai-gqm-quota \\\n",
        "--priority=200\n",
        "\n",
        "To run locally:\n",
        "./third_party/py/scenic/google/runlocal.sh \\\n",
        "--uptc=\"\" \\\n",
        "--binary=//experimental/largesensormodels/scenic:main \\\n",
        "--config=$(pwd)/experimental/largesensormodels/scenic/configs/mae_lsm_tiny.py:runlocal\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# To set constants.\n",
        "DATASET_NAME = 'lsm_300min_10M_impute'\n",
        "CACHE_DATASET = False\n",
        "TRAIN_DATA_SIZE = 1000000  # 100k train samples\n",
        "BATCH_SIZE = 8\n",
        "NUMBER_OF_EPOCH = 100\n",
        "REPEAT_DATA = False\n",
        "\n",
        "# Model variant / patch H (time steps) / patch W (features)\n",
        "VARIANT = 'TiShallow/1/1'\n",
        "LRS = [1e-3]\n",
        "TOKEN_MASK_PROB = 'constant_0.8'\n",
        "LOSS_ONLY_MASKED_TOKENS = True\n",
        "USE_DATETIME_FEATURES = True\n",
        "USE_TRAIN_AUGMENTATIONS = True\n",
        "TRAIN_AUGMENTATIONS = ['stretch', 'flip', 'noise']\n",
        "OHE_LABELS = True\n",
        "\n",
        "# Derived constants.\n",
        "TRAIN_DATA_SIZE = min(\n",
        "    TRAIN_DATA_SIZE,\n",
        "    dataset_constants.lsm_dataset_constants[DATASET_NAME]['num_train_examples']\n",
        ")\n",
        "\n",
        "STEPS_PER_EPOCH = max(1, int(TRAIN_DATA_SIZE / BATCH_SIZE))\n",
        "NUM_TRAIN_STEPS = int(NUMBER_OF_EPOCH * STEPS_PER_EPOCH)\n",
        "\n",
        "LOG_EVAL_SUMMARY_STEPS = STEPS_PER_EPOCH\n",
        "LOG_CHECKPOINT_STEPS = LOG_EVAL_SUMMARY_STEPS * 5\n",
        "MAX_NUM_CHECKPOINTS = int(NUM_TRAIN_STEPS / LOG_CHECKPOINT_STEPS)\n",
        "\n",
        "\n",
        "def get_config_common_few_shot(\n",
        "    batch_size: Optional[int] = None,\n",
        "    target_resolution: int = 224,\n",
        "    resize_resolution: int = 256,\n",
        ") -\u003e ml_collections.ConfigDict:\n",
        "  \"\"\"Returns a standard-ish fewshot eval configuration.\n",
        "\n",
        "  Copied from\n",
        "  third_party/py/scenic/projects/baselines/configs/google/common/common_fewshot.py\n",
        "\n",
        "  Args:\n",
        "    batch_size: The batch size to use for fewshot evaluation.\n",
        "    target_resolution: The target resolution of the fewshot evaluation.\n",
        "    resize_resolution: The resize resolution of the fewshot evaluation.\n",
        "\n",
        "  Returns:\n",
        "    A ConfigDict with the fewshot evaluation configuration.\n",
        "  \"\"\"\n",
        "  config = ml_collections.ConfigDict()\n",
        "  config.batch_size = batch_size\n",
        "  config.representation_layer = 'pre_logits'\n",
        "  config.log_eval_steps = 25_000\n",
        "  config.datasets = {\n",
        "      'birds': ('caltech_birds2011', 'train', 'test'),\n",
        "      'caltech': ('caltech101', 'train', 'test'),\n",
        "      'cars': ('cars196:2.1.0', 'train', 'test'),\n",
        "      'cifar100': ('cifar100', 'train', 'test'),\n",
        "      'col_hist': ('colorectal_histology', 'train[:2000]', 'train[2000:]'),\n",
        "      'dtd': ('dtd', 'train', 'test'),\n",
        "      'imagenet': ('imagenet2012_subset/10pct', 'train', 'validation'),\n",
        "      'pets': ('oxford_iiit_pet', 'train', 'test'),\n",
        "      'uc_merced': ('uc_merced', 'train[:1000]', 'train[1000:]'),\n",
        "  }\n",
        "  config.pp_train = f'decode|resize({resize_resolution})|central_crop({target_resolution})|value_range(-1,1)'\n",
        "  config.pp_eval = f'decode|resize({resize_resolution})|central_crop({target_resolution})|value_range(-1,1)'\n",
        "  config.shots = [1, 5, 10, 25]\n",
        "  config.l2_regs = [2.0**i for i in range(-10, 20)]\n",
        "  config.walk_first = ('imagenet', 10)\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "def get_config(runlocal=''):\n",
        "  \"\"\"Returns the ViT experiment configuration.\"\"\"\n",
        "\n",
        "  runlocal = bool(runlocal)\n",
        "\n",
        "  # Experiment.\n",
        "  config = ml_collections.ConfigDict()\n",
        "  config.experiment_name = f'electrodes-mae-{DATASET_NAME}-{TRAIN_DATA_SIZE}'\n",
        "  config.dataset_name = f'lsm_prod/{DATASET_NAME}'\n",
        "\n",
        "  # Dataset.\n",
        "  config.data_dtype_str = 'float32'\n",
        "  config.dataset_configs = ml_collections.ConfigDict()\n",
        "  config.dataset_configs.dataset = f'lsm_prod/{DATASET_NAME}'\n",
        "  # config.dataset_configs.num_classes = NUM_CLASSES\n",
        "  config.dataset_configs.train_split = 'train'  # train data split\n",
        "  config.dataset_configs.train_num_samples = TRAIN_DATA_SIZE  # train sample\n",
        "  # eval data split - note: this split is used for validation and test.\n",
        "  config.dataset_configs.eval_split = 'test[:64]' if runlocal else 'test'\n",
        "  config.dataset_configs.cache_dataset = CACHE_DATASET\n",
        "  config.dataset_configs.prefetch_to_device = 2\n",
        "  # Shuffle_buffer_size is per host, so small-ish is ok.\n",
        "  config.dataset_configs.shuffle_buffer_size = 250_000\n",
        "  config.dataset_configs.repeat_data = REPEAT_DATA\n",
        "  config.dataset_configs.ohe_labels = OHE_LABELS\n",
        "\n",
        "  # Model.\n",
        "  if len(VARIANT.split('/')) == 3:\n",
        "    version = VARIANT.split('/')[0]  # model variant\n",
        "    patch_h = VARIANT.split('/')[1]  # patch width\n",
        "    patch_w = VARIANT.split('/')[2]  # patch height\n",
        "  elif len(VARIANT.split('/')) == 2:\n",
        "    version = VARIANT.split('/')[0]  # model variant\n",
        "    patch_h = VARIANT.split('/')[1]  # patch width\n",
        "    patch_w = VARIANT.split('/')[1]  # patch height\n",
        "  else:\n",
        "    raise ValueError(f'Invalid model variant: {VARIANT}')\n",
        "\n",
        "  version = 'Deb' if runlocal else version\n",
        "  config.model_name = 'lsm_vit_mae'\n",
        "  config.model = ml_collections.ConfigDict()\n",
        "  # encoder\n",
        "  config.model.hidden_size = model_constants.HIDDEN_SIZES[version]\n",
        "  config.model.patches = ml_collections.ConfigDict()\n",
        "  config.model.patches.size = tuple([int(patch_h), int(patch_w)])\n",
        "  config.model.num_heads = model_constants.NUM_HEADS[version]\n",
        "  config.model.mlp_dim = model_constants.MLP_DIMS[version]\n",
        "  config.model.num_layers = model_constants.NUM_LAYERS[version]\n",
        "  config.model.dropout_rate = 0.0\n",
        "  config.model.classifier = 'none'  # Has to be \"none\" for the autoencoder\n",
        "  config.model.representation_size = None\n",
        "  config.model.positional_embedding = 'sinusoidal_2d'\n",
        "  config.model.positional_embedding_decoder = 'sinusoidal_2d'\n",
        "  # decoder\n",
        "  config.model.decoder_config = ml_collections.ConfigDict()\n",
        "  config.model.decoder_config.hidden_size = (\n",
        "      model_constants.DECODER_HIDDEN_SIZES[version]\n",
        "  )\n",
        "  config.model.decoder_config.mlp_dim = model_constants.DECODER_MLP_DIMS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.num_layers = model_constants.DECODER_NUM_LAYERS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.num_heads = model_constants.DECODER_NUM_HEADS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.dropout_rate = 0.0\n",
        "  config.model.decoder_config.attention_dropout_rate = 0.0\n",
        "\n",
        "  config.masked_feature_loss = ml_collections.ConfigDict()\n",
        "  config.masked_feature_loss.targets_type = 'rgb'\n",
        "  config.masked_feature_loss.token_mask_probability = TOKEN_MASK_PROB\n",
        "  config.masked_feature_loss.loss_only_masked_tokens = LOSS_ONLY_MASKED_TOKENS\n",
        "  config.masked_feature_loss.loss_type = 'squared'  # 'squared' or 'absolute'\n",
        "\n",
        "  # Datetime features.\n",
        "  config.use_datetime_features = USE_DATETIME_FEATURES\n",
        "\n",
        "  # Training.\n",
        "  config.trainer_name = 'lsm_mae_trainer'\n",
        "  config.batch_size = 8 if runlocal else BATCH_SIZE\n",
        "  config.num_training_steps = NUM_TRAIN_STEPS\n",
        "  config.log_eval_steps = LOG_EVAL_SUMMARY_STEPS\n",
        "  config.log_summary_steps = LOG_EVAL_SUMMARY_STEPS\n",
        "  config.rng_seed = 42\n",
        "  config.use_train_augmentations = USE_TRAIN_AUGMENTATIONS\n",
        "  config.train_augmentations = TRAIN_AUGMENTATIONS\n",
        "  sched = ml_collections.ConfigDict()\n",
        "  sched.re = '(.*)'\n",
        "  sched.lr_configs = ml_collections.ConfigDict()\n",
        "  sched.lr_configs.learning_rate_schedule = 'compound'\n",
        "  sched.lr_configs.factors = 'constant * cosine_decay * linear_warmup'\n",
        "  sched.lr_configs.total_steps = NUM_TRAIN_STEPS\n",
        "  sched.lr_configs.steps_per_cycle = sched.lr_configs.total_steps\n",
        "  sched.lr_configs.warmup_steps = STEPS_PER_EPOCH * NUMBER_OF_EPOCH * 0.05\n",
        "  sched.lr_configs.base_learning_rate = LRS[0]\n",
        "  config.schedule = ml_collections.ConfigDict({'all': sched})\n",
        "\n",
        "  # *Single* optimizer.\n",
        "  optim = ml_collections.ConfigDict()\n",
        "  optim.optax_name = 'scale_by_adam'\n",
        "  # optim.optax = dict(mu_dtype='bfloat16')\n",
        "  optim.optax_configs = ml_collections.ConfigDict({  # Optimizer settings.\n",
        "      'b1': 0.9,\n",
        "      'b2': 0.95,\n",
        "  })\n",
        "  config.optax = dict(mu_dtype='bfloat16')\n",
        "  optim.max_grad_norm = 1.0\n",
        "\n",
        "  optim.weight_decay = 1e-4\n",
        "  optim.weight_decay_decouple = True\n",
        "  config.optimizer = optim\n",
        "\n",
        "  # Fewshot.\n",
        "  # TODO(girishvn): This needs to be adapted to electrode dataset\n",
        "  config.fewshot = get_config_common_few_shot(batch_size=config.batch_size)\n",
        "  config.fewshot.datasets = {}\n",
        "  config.fewshot.walk_first = ()\n",
        "  config.fewshot.representation_layer = 'pre_logits'\n",
        "  config.fewshot.log_eval_steps = LOG_EVAL_SUMMARY_STEPS\n",
        "\n",
        "  # Logging.\n",
        "  config.write_summary = True\n",
        "  config.xprof = True  # Profile using xprof.\n",
        "  config.checkpoint = True  # Do checkpointing.\n",
        "  config.checkpoint_steps = LOG_CHECKPOINT_STEPS\n",
        "  config.debug_train = False  # Debug mode during training.\n",
        "  config.debug_eval = False  # Debug mode during eval.\n",
        "  config.max_checkpoints_to_keep = MAX_NUM_CHECKPOINTS\n",
        "  # BEGIN GOOGLE-INTERNAL\n",
        "  if runlocal:\n",
        "    # Current implementation fails with UPTC.\n",
        "    config.count_flops = False\n",
        "  # END GOOGLE-INTERNAL\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "# BEGIN GOOGLE-INTERNAL\n",
        "def get_hyper(hyper):\n",
        "  \"\"\"Defines the hyper-parameters sweeps for doing grid search.\"\"\"\n",
        "  return hyper.product([\n",
        "      hyper.sweep('config.schedule.all.lr_configs.base_learning_rate', LRS),\n",
        "  ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8Ho2Xzoyk5uP"
      },
      "outputs": [],
      "source": [
        "# @title Data Dir\n",
        "\n",
        "data_dir='/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-electrodes/deid/exp/dmcduff/ttl=6w/msa_1_5/lsm_tfds_datasets'\n",
        "print(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qvTb-evdGqgA"
      },
      "outputs": [],
      "source": [
        "# @title Dataset Feature Columns\n",
        "\n",
        "with gfile.Open('/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-electrodes/deid/raw/datasets/msa_1_5/lsm_tfds_datasets/lsm_prod/lsm_300min_100K_unimpute/1.0.0/Dataset_FeatureNames.csv', 'r') as f:\n",
        "  df = pd.read_csv(f)\n",
        "\n",
        "features = df.columns\n",
        "print(list(features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQPfNy8BDNND"
      },
      "source": [
        "# Balanced Pre-train Dataset\n",
        "### Dataset Name: lsm_300min_pretraining_165K_n10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0ap4K_V8EPk0"
      },
      "outputs": [],
      "source": [
        "# @title Load Dataset and Visualize Sample\n",
        "\n",
        "DATASET_NAME = 'lsm_300min_pretraining_165K_n10'\n",
        "TRAIN_DATA_SIZE = None\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = lsm_tiny_dataset.get_dataset(config, data_rng)\n",
        "\n",
        "print('Processed Dataset Meta Data:')\n",
        "for k in dataset.meta_data.keys():\n",
        "  print(k, dataset.meta_data[k])\n",
        "\n",
        "print('\\nData Sample Information:')\n",
        "explore_sample_data(dataset, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hqaSRPIHEctj"
      },
      "outputs": [],
      "source": [
        "# @title Metadata From Data Constants\n",
        "\n",
        "print('Data Constants Meta Data:')\n",
        "dataset_name = config.dataset_name.split('/')[-1]\n",
        "dataset_constants.lsm_dataset_constants[dataset_name]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DDMei-o8oO_"
      },
      "source": [
        "# Combined 8M Pre-train Dataset\n",
        "### Dataset name: lsm_300min_pretraining_8M_combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yOmFzR3o9XZP"
      },
      "outputs": [],
      "source": [
        "# @title Load Dataset and Visualize Sample\n",
        "\n",
        "DATASET_NAME = 'lsm_300min_pretraining_8M_combined'\n",
        "TRAIN_DATA_SIZE = None\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = lsm_combined_pretrain_dataset.get_dataset(config, data_rng)\n",
        "\n",
        "print('Processed Dataset Meta Data:')\n",
        "for k in dataset.meta_data.keys():\n",
        "  print(k, dataset.meta_data[k])\n",
        "\n",
        "print('\\nData Sample Information:')\n",
        "explore_sample_data(dataset, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cFZHd5F4DbFl"
      },
      "outputs": [],
      "source": [
        "# @title Metadata From Data Constants\n",
        "\n",
        "print('Data Constants Meta Data:')\n",
        "dataset_name = config.dataset_name.split('/')[-1]\n",
        "dataset_constants.lsm_dataset_constants[dataset_name]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-GZBYUXeEUbg",
        "gQPfNy8BDNND",
        "-DDMei-o8oO_"
      ],
      "last_runtime": {
        "build_target": "//fitbit/research/sensing/electrodes/colab:rl_colab",
        "kind": "shared"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
