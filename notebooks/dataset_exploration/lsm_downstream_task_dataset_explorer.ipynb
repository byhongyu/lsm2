{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl8y8d9po67C"
      },
      "source": [
        "# LSM Down-Stream-Task Dataset Explorer\n",
        "##### Colab Kernel (Electrodes)\n",
        "##### Dataset (Electrodes)\n",
        "\n",
        "Grants command for Access on Demand (AoD):\n",
        "\n",
        "https://grants.corp.google.com/#/grants?request=20h%2Fchr-ards-electrodes-deid-colab-jobs\u0026reason=b%2F314799341\n",
        "\n",
        "### About This Notebook:\n",
        "This notebook explores down-stream-task specific datasets for the LSM project. It loads and prints dataset meta data, plots sample data, and counts label occurences for the following datasets:\n",
        "\n",
        "**Actively Used:**\n",
        "1. Mood dataset: `lsm_300min_2000_mood_balanced`\n",
        "2. Stress dataset: `lsm_300min_2000_stress_balanced`\n",
        "3. Activity dataset: `lsm_300min_600_activities_balanced_v4`\n",
        "4. Exercise dataset: `lsm_300min_2000_mood_balanced` and `lsm_300min_600_activities_balanced`\n",
        "5. Biological sex dataset: derived from `lsm_300min_600_activities_balanced_v4`\n",
        "6. Age dataset: derived from `lsm_300min_600_activities_balanced_v4`\n",
        "7. Subject dependent mood dataset: derived from `lsm_300min_2000_mood_balanced`\n",
        "\n",
        "**Deprecated Datasets:**\n",
        "1. Exercise dataset 1: `lsm_300min_600_activities_balanced`\n",
        "2. Exercise dataset 2: `lsm_300min_300_activities_balanced`\n",
        "3. Excercise dataset 3 (subset of the above dataset): `lsm_300min_600_activities_9class_subset`\n",
        "\n",
        "The results from this notebook are (manually) updated in the following google sheets file:\n",
        "\n",
        "https://docs.google.com/spreadsheets/d/1-crJrg0XhedN5ayRpNd-7AUGr7PRxoQKxL__9gLlB5M/edit?usp=sharing\u0026resourcekey=0-fv0vnqQuas9QsAJVcUOoGA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GZBYUXeEUbg"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1MCewaurabYh"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "import io\n",
        "import functools\n",
        "from typing import Any, Callable, Dict, Iterator, Tuple, Optional, Type, Union\n",
        "import time\n",
        "from collections import Counter\n",
        "import time\n",
        "import random\n",
        "\n",
        "from absl import logging\n",
        "from clu import metric_writers\n",
        "from clu import periodic_actions\n",
        "from clu import platform\n",
        "\n",
        "import flax\n",
        "from flax import jax_utils\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.profiler\n",
        "\n",
        "import pandas as pd\n",
        "import ml_collections\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from colabtools import adhoc_import\n",
        "with adhoc_import.Google3():\n",
        "  from scenic.dataset_lib import dataset_utils\n",
        "  from scenic.google.xm import xm_utils\n",
        "  from scenic.model_lib.base_models import base_model\n",
        "  from scenic.model_lib.base_models import model_utils\n",
        "  from scenic.model_lib.layers import nn_ops\n",
        "  from scenic.model_lib.layers import nn_layers\n",
        "  from scenic.projects.baselines import vit\n",
        "  from scenic.train_lib import optax as scenic_optax\n",
        "  from scenic.train_lib import pretrain_utils\n",
        "  from scenic.train_lib import train_utils\n",
        "\n",
        "  from scenic.projects.multimask.models import model_utils as mm_model_utils\n",
        "\n",
        "  # LSM Dataset Imports\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import dataset_constants\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_activity_subset_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_mood_vs_activity_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_tiny_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_combined_pretrain_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_fewshot_mood_vs_activity_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_fewshot_remapped_activity_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_mood_subj_dependent_preprocessed_40sps_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_biological_sex_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_binned_age_dataset\n",
        "\n",
        "  from google3.experimental.largesensormodels.scenic.models import lsm_vit as lsm_vit_mae\n",
        "  from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils import model_constants\n",
        "  from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils import model_utils as lsm_model_utils\n",
        "  from google3.experimental.largesensormodels.scenic.trainers import lsm_mae_trainer\n",
        "\n",
        "  from google3.pyglib import gfile\n",
        "\n",
        "\n",
        "Batch = Dict[str, jnp.ndarray]\n",
        "MetricFn = Callable[\n",
        "    [jnp.ndarray, jnp.ndarray, Dict[str, jnp.ndarray]],\n",
        "    Dict[str, Tuple[float, int]],\n",
        "]\n",
        "LossFn = Callable[\n",
        "    [jnp.ndarray, Batch, Optional[jnp.ndarray], jnp.ndarray], float\n",
        "]\n",
        "LrFns = Dict[str, Callable[[jnp.ndarray], jnp.ndarray]]\n",
        "Patch = Union[Tuple[int, int], Tuple[int, int, int]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "R4-nK9tgOOXv"
      },
      "outputs": [],
      "source": [
        "# @title Helper Functions\n",
        "\n",
        "\n",
        "def explore_sample_data(dataset, ohe_labels):\n",
        "  p_idx = 0\n",
        "  batch_idx = 0\n",
        "  x = next(dataset.valid_iter)\n",
        "\n",
        "  #Pparse data example\n",
        "  input = x['input_signal']  # Sensor signals\n",
        "  plt_input = jnp.transpose(input, (0, 1, 3, 2, 4))\n",
        "\n",
        "  datetime = x['datetime_signal']  # Datetime signals\n",
        "  plt_datetime = jnp.transpose(datetime, (0, 1, 3, 2, 4))\n",
        "\n",
        "  print(f'Example keys {list(x.keys())}\\n')\n",
        "  print('Exercise Log', x['exercise_log'])  # Exercise log\n",
        "  print('Log Value', x['log_value'] + 65536)  # Log value\n",
        "  print('\\nMood Log', x['mood_log'])  # Mood log\n",
        "\n",
        "  if ohe_labels:\n",
        "    print(f\"\\nLabel {x['label']}\\n\")  # Stress label\n",
        "\n",
        "  # Plot input signal\n",
        "  plt.figure(figsize= (15, 4))\n",
        "  plt.imshow(plt_input[p_idx][batch_idx])\n",
        "  plt.title('Sensor Inputs');\n",
        "\n",
        "  plt.figure(figsize= (15, 4))\n",
        "  plt.imshow(plt_datetime[p_idx][batch_idx])\n",
        "  plt.title('Datetime Inputs');\n",
        "\n",
        "\n",
        "def dataset_split_information(ds, event, offset=65536):\n",
        "  \"\"\"Given a dataset split retuns a dictionary of stats/metadata.\"\"\"\n",
        "  # event can be ['exercise', 'mood', 'stress']\n",
        "\n",
        "  if event == 'exercise':\n",
        "    log_key = 'exercise_log'\n",
        "  elif event == 'mood' or event == 'stress':\n",
        "    log_key = 'mood_log'\n",
        "  elif event is None:\n",
        "    log_key = None\n",
        "  else:\n",
        "    raise ValueError(f'event must be exercise, mood, or stress')\n",
        "\n",
        "  subj_ids = []\n",
        "  ex_count = 0\n",
        "  log_values = []\n",
        "  for d in ds:\n",
        "\n",
        "    # Subject ID\n",
        "    # ids = d['labels']['ID']\n",
        "    # ids = [i.decode() for i in ids.flatten() if type(i) is bytes]\n",
        "    # subj_ids += ids\n",
        "\n",
        "    # Total example count.\n",
        "    ex_count += jnp.sum(d['batch_mask'])\n",
        "    if ex_count % 100000 == 0:\n",
        "      print(f'running example count: {ex_count}')\n",
        "\n",
        "    # Get label\n",
        "    if event == 'exercise' or event == 'mood':\n",
        "      valid_log = jnp.where(d[log_key])  # where exercise/mood log is True\n",
        "      valid_log_value = d['log_value'][valid_log]  # get valid log values\n",
        "      # add offset value\n",
        "      valid_log_value = [v + offset for v in valid_log_value.flatten() if v != 0]\n",
        "\n",
        "    elif event == 'stress':\n",
        "      log_value = jnp.argmax(d['label'], axis=-1)\n",
        "      valid_log = jnp.where(d['batch_mask'])  # where exercise/mood log is True\n",
        "      valid_log_value = log_value[valid_log]  # get valid log values\n",
        "      valid_log_value = valid_log_value.tolist()\n",
        "\n",
        "    elif event is None:\n",
        "      log_value = jnp.argmax(d['label'], axis=-1)\n",
        "      valid_log = jnp.where(d['batch_mask'])  # where exercise/mood log is True\n",
        "      valid_log_value = log_value[valid_log]  # get valid log values\n",
        "      valid_log_value = valid_log_value.tolist()\n",
        "\n",
        "    # append list to log values list\n",
        "    log_values += valid_log_value\n",
        "\n",
        "  info = {'num_examples': ex_count,\n",
        "          'example_subj_ids': subj_ids,\n",
        "          'log_value': log_values}\n",
        "\n",
        "  return info\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2PTd0Bk_T7Wt"
      },
      "outputs": [],
      "source": [
        "# @title Exercise Log Value Map\n",
        "\n",
        "log_value_map = pd.read_csv(io.StringIO('''\n",
        "adidas Train,59001\n",
        "Zumba,56001\n",
        "\"Yoga, Vinyasa\",52005\n",
        "\"Yoga, Hatha\",52003\n",
        "Yoga,52001\n",
        "Yoga,52000\n",
        "Wrestling,15730\n",
        "Workout,3000\n",
        "Weights,2131\n",
        "Weights,2050\n",
        "Weightlifting,91043\n",
        "\"Weight lifting (free, nautilus or universal-type), light or moderate effort, light workout, general\",2130\n",
        "Weeding,11360\n",
        "Water volleyball,18365\n",
        "Water skiing,18150\n",
        "Water polo,18360\n",
        "Water jogging,18366\n",
        "Water aerobics,2120\n",
        "Water aerobics,18355\n",
        "Warm It Up,3102\n",
        "Walking for pleasure,17160\n",
        "Walk,90013\n",
        "Volleyball,15711\n",
        "Ultimate frisbee,15250\n",
        "\"Treadmill, 15% Incline\",90022\n",
        "Treadmill,20049\n",
        "Treadmill,90019\n",
        "Trampoline,15700\n",
        "Tennis,15675\n",
        "Tai chi,15670\n",
        "Table tennis,15660\n",
        "Tabata Workout,20055\n",
        "TRX,20056\n",
        "\"Swimming, sidestroke, general\",18320\n",
        "\"Swimming, leisurely, not lap swimming, general\",18310\n",
        "\"Swimming, lake, ocean, river\",18300\n",
        "\"Swimming laps, freestyle, fast, vigorous effort\",18230\n",
        "Swim,90024\n",
        "\"Surfing, body or board\",18220\n",
        "Surfing,91056\n",
        "Stretching,2100\n",
        "Stressed,13\n",
        "Strength training,91042\n",
        "\"Standing; moderate/heavy (lifting more than 50 lbs, masonry, painting, paper hanging)\",11630\n",
        "Stairclimber,2065\n",
        "Squash,15650\n",
        "Sport,15000\n",
        "Spinning,55001\n",
        "Spinning,90002\n",
        "Softball,15640\n",
        "Soccer,15605\n",
        "Snowshoeing,19190\n",
        "Snowboarding,91051\n",
        "Snorkeling,18210\n",
        "Skiing,19160\n",
        "Skating,91052\n",
        "Skateboarding,15580\n",
        "Shooting,4130\n",
        "Scuba diving,18200\n",
        "\"Scrubbing floors, on hands and knees, scrubbing bathroom, bathtub\",5130\n",
        "Sailing,18120\n",
        "Run,90009\n",
        "Rugby,15560\n",
        "Rowing machine,91041\n",
        "Rowing Machine,90003\n",
        "Rowing,90014\n",
        "Rollerblading,91054\n",
        "Roller skating,15590\n",
        "Roller blading,15591\n",
        "Rock climbing,17120\n",
        "Rock climbing,15535\n",
        "Racquetball,15530\n",
        "Race walking,17110\n",
        "Powerlifting,91044\n",
        "Polo,15510\n",
        "Pilates,53001\n",
        "Pilates,53000\n",
        "Paddleboarding,91057\n",
        "Outdoor Workout,1072\n",
        "Outdoor Bike,1071\n",
        "Orienteering,15480\n",
        "\"Mowing lawn, walk, power mower\",8120\n",
        "\"Mowing lawn, walk, hand mower\",8110\n",
        "\"Mowing lawn, riding mower\",8100\n",
        "Mowing lawn,8095\n",
        "Mountain Bike,20048\n",
        "Motorcycle,16030\n",
        "Meditating,7075\n",
        "Martial Arts,15430\n",
        "Lying quietly and watching television,7010\n",
        "\"Laundry, fold or hang clothes, put clothes in washer or dryer, packing suitcase\",5090\n",
        "Lacrosse,15460\n",
        "Kickboxing,55002\n",
        "Kettlebell,20053\n",
        "Kayaking,18100\n",
        "Jumping rope,15551\n",
        "\"Jogging, general\",12020\n",
        "Jog/walk combination (jogging component of less than 10 minutes),12010\n",
        "Ironing,5070\n",
        "Interval Workout,20057\n",
        "Indoor climbing,91055\n",
        "\"Implied walking/standing -picking up yard, light, picking flowers or vegetables\",8250\n",
        "Ice skating,19030\n",
        "Hunting,4100\n",
        "Household Chores,90006\n",
        "\"Horseback riding, saddling horse, grooming horse\",15380\n",
        "Horseback riding,15370\n",
        "Hockey,15360\n",
        "\"Hiking, cross country\",17080\n",
        "Hike,90012\n",
        "\"Health club exercise, general\",2060\n",
        "Handball,15320\n",
        "HIIT,91040\n",
        "Gymnastics,15300\n",
        "\"Golf, walking and pulling clubs\",15285\n",
        "Golf,15255\n",
        "\"General cleaning, moderate effort\",11125\n",
        "Gardening,8245\n",
        "Frustrated,10\n",
        "Football,15210\n",
        "Fitstar: Personal Trainer,3104\n",
        "Fitstar: Personal Trainer,3103\n",
        "Fishing,4001\n",
        "Field Hockey,15350\n",
        "Fencing,15200\n",
        "Excited,11\n",
        "\"Elliptical, Low Resistance\",90016\n",
        "Elliptical,20047\n",
        "Elliptical,90017\n",
        "\"Driving heavy truck, tractor, bus\",16050\n",
        "Driving,16010\n",
        "Diving,18090\n",
        "\"Digging, spading, filling garden, composting\",8050\n",
        "Dancing,3031\n",
        "Curling,15170\n",
        "CrossFit,20050\n",
        "CrossFit,91045\n",
        "Cross-country ski,91053\n",
        "Cross Country Skiing,90015\n",
        "Cricket,15150\n",
        "Core training,91046\n",
        "Cooking or food preparation,5052\n",
        "Content,7\n",
        "\"Cleaning, house or cabin, general\",5030\n",
        "\"Cleaning sink and toilet, light effort\",11122\n",
        "Cleaning,5020\n",
        "Circuit Training,2040\n",
        "Carpentry,11040\n",
        "Cardio Sculpt,20051\n",
        "Cardio Kickboxing,20054\n",
        "Canoeing,18080\n",
        "Canoeing,91059\n",
        "Calm,12\n",
        "\"Calisthenics, home exercise, light or moderate effort, general (example: back exercises), going up and down from floor\",2030\n",
        "Calisthenics,2020\n",
        "Boxing,15100\n",
        "Bootcamp,55003\n",
        "Bike,90001\n",
        "Basketball,15040\n",
        "Baseball,15620\n",
        "Barre Class,20052\n",
        "Ballet,3010\n",
        "Badminton,15020\n",
        "Archery,15010\n",
        "Aerobics,90005\n",
        "Aerobics,91047\n",
        "\"Aerobic, general\",3015\n",
        "Aerobic Workout,3001\n",
        "7 Minute Workout,3100\n",
        "10 Minute Abs,3101\n",
        "'''), header=None)\n",
        "\n",
        "log_value_map.columns = ['name', 'id']\n",
        "\n",
        "# Generate dictionary map\n",
        "log_value_map_dict = {\n",
        "    id: name for id, name in zip(log_value_map['id'], log_value_map['name'])\n",
        "}\n",
        "\n",
        "# Show Map\n",
        "log_value_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NiW84UC2pNCQ"
      },
      "outputs": [],
      "source": [
        "# @title Sample Model Config\n",
        "\n",
        "r\"\"\"A config to train a Tiny ViT MAE on LSM 5M dataset.\n",
        "\n",
        "Forked from google3/third_party/py/scenic/projects/multimask/configs/mae_cifar10_tiny.py\n",
        "\n",
        "To run on XManager:\n",
        "gxm third_party/py/scenic/google/xm/launch_xm.py -- \\\n",
        "--binary //experimental/largesensormodels/scenic:main \\\n",
        "--config=experimental/largesensormodels/scenic/configs/mae_lsm_tiny.py \\\n",
        "--platform=vlp_4x4 \\\n",
        "--exp_name=lsm_mae_tier2_TinyShallow_10_5_res \\\n",
        "--workdir=/cns/dz-d/home/xliucs/lsm/xm/\\{xid\\} \\\n",
        "--xm_resource_alloc=group:mobile-dynamic/h2o-ai-gqm-quota \\\n",
        "--priority=200\n",
        "\n",
        "To run locally:\n",
        "./third_party/py/scenic/google/runlocal.sh \\\n",
        "--uptc=\"\" \\\n",
        "--binary=//experimental/largesensormodels/scenic:main \\\n",
        "--config=$(pwd)/experimental/largesensormodels/scenic/configs/mae_lsm_tiny.py:runlocal\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# To set constants.\n",
        "DATASET_NAME = 'lsm_300min_pretraining_165K_n10'\n",
        "CACHE_DATASET = False\n",
        "TRAIN_DATA_SIZE = 1000000  # 100k train samples\n",
        "BATCH_SIZE = 8\n",
        "NUMBER_OF_EPOCH = 100\n",
        "REPEAT_DATA = False\n",
        "\n",
        "# Model variant / patch H (time steps) / patch W (features)\n",
        "VARIANT = 'TiShallow/10/5'\n",
        "LRS = [1e-3]\n",
        "TOKEN_MASK_PROB = 'constant_0.8'\n",
        "LOSS_ONLY_MASKED_TOKENS = True\n",
        "USE_DATETIME_FEATURES = True\n",
        "USE_TRAIN_AUGMENTATIONS = True\n",
        "TRAIN_AUGMENTATIONS = ['stretch', 'flip', 'noise']\n",
        "OHE_LABELS = True\n",
        "\n",
        "# Derived constants.\n",
        "TRAIN_DATA_SIZE = min(\n",
        "    TRAIN_DATA_SIZE,\n",
        "    dataset_constants.lsm_dataset_constants[DATASET_NAME]['num_train_examples']\n",
        ")\n",
        "\n",
        "STEPS_PER_EPOCH = max(1, int(TRAIN_DATA_SIZE / BATCH_SIZE))\n",
        "NUM_TRAIN_STEPS = int(NUMBER_OF_EPOCH * STEPS_PER_EPOCH)\n",
        "\n",
        "LOG_EVAL_SUMMARY_STEPS = STEPS_PER_EPOCH\n",
        "LOG_CHECKPOINT_STEPS = LOG_EVAL_SUMMARY_STEPS * 5\n",
        "MAX_NUM_CHECKPOINTS = int(NUM_TRAIN_STEPS / LOG_CHECKPOINT_STEPS)\n",
        "\n",
        "\n",
        "def get_config_common_few_shot(\n",
        "    batch_size: Optional[int] = None,\n",
        "    target_resolution: int = 224,\n",
        "    resize_resolution: int = 256,\n",
        ") -\u003e ml_collections.ConfigDict:\n",
        "  \"\"\"Returns a standard-ish fewshot eval configuration.\n",
        "\n",
        "  Copied from\n",
        "  third_party/py/scenic/projects/baselines/configs/google/common/common_fewshot.py\n",
        "\n",
        "  Args:\n",
        "    batch_size: The batch size to use for fewshot evaluation.\n",
        "    target_resolution: The target resolution of the fewshot evaluation.\n",
        "    resize_resolution: The resize resolution of the fewshot evaluation.\n",
        "\n",
        "  Returns:\n",
        "    A ConfigDict with the fewshot evaluation configuration.\n",
        "  \"\"\"\n",
        "  config = ml_collections.ConfigDict()\n",
        "  config.batch_size = batch_size\n",
        "  config.representation_layer = 'pre_logits'\n",
        "  config.log_eval_steps = 25_000\n",
        "  config.datasets = {\n",
        "      'birds': ('caltech_birds2011', 'train', 'test'),\n",
        "      'caltech': ('caltech101', 'train', 'test'),\n",
        "      'cars': ('cars196:2.1.0', 'train', 'test'),\n",
        "      'cifar100': ('cifar100', 'train', 'test'),\n",
        "      'col_hist': ('colorectal_histology', 'train[:2000]', 'train[2000:]'),\n",
        "      'dtd': ('dtd', 'train', 'test'),\n",
        "      'imagenet': ('imagenet2012_subset/10pct', 'train', 'validation'),\n",
        "      'pets': ('oxford_iiit_pet', 'train', 'test'),\n",
        "      'uc_merced': ('uc_merced', 'train[:1000]', 'train[1000:]'),\n",
        "  }\n",
        "  config.pp_train = f'decode|resize({resize_resolution})|central_crop({target_resolution})|value_range(-1,1)'\n",
        "  config.pp_eval = f'decode|resize({resize_resolution})|central_crop({target_resolution})|value_range(-1,1)'\n",
        "  config.shots = [1, 5, 10, 25]\n",
        "  config.l2_regs = [2.0**i for i in range(-10, 20)]\n",
        "  config.walk_first = ('imagenet', 10)\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "def get_config(runlocal=''):\n",
        "  \"\"\"Returns the ViT experiment configuration.\"\"\"\n",
        "\n",
        "  runlocal = bool(runlocal)\n",
        "\n",
        "  # Experiment.\n",
        "  config = ml_collections.ConfigDict()\n",
        "  config.experiment_name = f'electrodes-mae-{DATASET_NAME}-{TRAIN_DATA_SIZE}'\n",
        "  config.dataset_name = f'lsm_prod/{DATASET_NAME}'\n",
        "\n",
        "  # Dataset.\n",
        "  config.data_dtype_str = 'float32'\n",
        "  config.dataset_configs = ml_collections.ConfigDict()\n",
        "  config.dataset_configs.dataset = f'lsm_prod/{DATASET_NAME}'\n",
        "  # config.dataset_configs.num_classes = NUM_CLASSES\n",
        "  config.dataset_configs.train_split = 'train'  # train data split\n",
        "  config.dataset_configs.train_num_samples = TRAIN_DATA_SIZE  # train sample\n",
        "  # eval data split - note: this split is used for validation and test.\n",
        "  config.dataset_configs.eval_split = 'test[:64]' if runlocal else 'test'\n",
        "  config.dataset_configs.cache_dataset = CACHE_DATASET\n",
        "  config.dataset_configs.prefetch_to_device = 2\n",
        "  # Shuffle_buffer_size is per host, so small-ish is ok.\n",
        "  config.dataset_configs.shuffle_buffer_size = 250_000\n",
        "  config.dataset_configs.repeat_data = REPEAT_DATA\n",
        "  config.dataset_configs.ohe_labels = OHE_LABELS\n",
        "\n",
        "  # Model.\n",
        "  if len(VARIANT.split('/')) == 3:\n",
        "    version = VARIANT.split('/')[0]  # model variant\n",
        "    patch_h = VARIANT.split('/')[1]  # patch width\n",
        "    patch_w = VARIANT.split('/')[2]  # patch height\n",
        "  elif len(VARIANT.split('/')) == 2:\n",
        "    version = VARIANT.split('/')[0]  # model variant\n",
        "    patch_h = VARIANT.split('/')[1]  # patch width\n",
        "    patch_w = VARIANT.split('/')[1]  # patch height\n",
        "  else:\n",
        "    raise ValueError(f'Invalid model variant: {VARIANT}')\n",
        "\n",
        "  version = 'Deb' if runlocal else version\n",
        "  config.model_name = 'lsm_vit_mae'\n",
        "  config.model = ml_collections.ConfigDict()\n",
        "  # encoder\n",
        "  config.model.hidden_size = model_constants.HIDDEN_SIZES[version]\n",
        "  config.model.patches = ml_collections.ConfigDict()\n",
        "  config.model.patches.size = tuple([int(patch_h), int(patch_w)])\n",
        "  config.model.num_heads = model_constants.NUM_HEADS[version]\n",
        "  config.model.mlp_dim = model_constants.MLP_DIMS[version]\n",
        "  config.model.num_layers = model_constants.NUM_LAYERS[version]\n",
        "  config.model.dropout_rate = 0.0\n",
        "  config.model.classifier = 'none'  # Has to be \"none\" for the autoencoder\n",
        "  config.model.representation_size = None\n",
        "  config.model.positional_embedding = 'sinusoidal_2d'\n",
        "  config.model.positional_embedding_decoder = 'sinusoidal_2d'\n",
        "  # decoder\n",
        "  config.model.decoder_config = ml_collections.ConfigDict()\n",
        "  config.model.decoder_config.hidden_size = (\n",
        "      model_constants.DECODER_HIDDEN_SIZES[version]\n",
        "  )\n",
        "  config.model.decoder_config.mlp_dim = model_constants.DECODER_MLP_DIMS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.num_layers = model_constants.DECODER_NUM_LAYERS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.num_heads = model_constants.DECODER_NUM_HEADS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.dropout_rate = 0.0\n",
        "  config.model.decoder_config.attention_dropout_rate = 0.0\n",
        "\n",
        "  config.masked_feature_loss = ml_collections.ConfigDict()\n",
        "  config.masked_feature_loss.targets_type = 'rgb'\n",
        "  config.masked_feature_loss.token_mask_probability = TOKEN_MASK_PROB\n",
        "  config.masked_feature_loss.loss_only_masked_tokens = LOSS_ONLY_MASKED_TOKENS\n",
        "  config.masked_feature_loss.loss_type = 'squared'  # 'squared' or 'absolute'\n",
        "\n",
        "  # Datetime features.\n",
        "  config.use_datetime_features = USE_DATETIME_FEATURES\n",
        "\n",
        "  # Training.\n",
        "  config.trainer_name = 'lsm_mae_trainer'\n",
        "  config.batch_size = 8 if runlocal else BATCH_SIZE\n",
        "  config.num_training_steps = NUM_TRAIN_STEPS\n",
        "  config.log_eval_steps = LOG_EVAL_SUMMARY_STEPS\n",
        "  config.log_summary_steps = LOG_EVAL_SUMMARY_STEPS\n",
        "  config.rng_seed = 42\n",
        "  config.use_train_augmentations = USE_TRAIN_AUGMENTATIONS\n",
        "  config.train_augmentations = TRAIN_AUGMENTATIONS\n",
        "  sched = ml_collections.ConfigDict()\n",
        "  sched.re = '(.*)'\n",
        "  sched.lr_configs = ml_collections.ConfigDict()\n",
        "  sched.lr_configs.learning_rate_schedule = 'compound'\n",
        "  sched.lr_configs.factors = 'constant * cosine_decay * linear_warmup'\n",
        "  sched.lr_configs.total_steps = NUM_TRAIN_STEPS\n",
        "  sched.lr_configs.steps_per_cycle = sched.lr_configs.total_steps\n",
        "  sched.lr_configs.warmup_steps = STEPS_PER_EPOCH * NUMBER_OF_EPOCH * 0.05\n",
        "  sched.lr_configs.base_learning_rate = LRS[0]\n",
        "  config.schedule = ml_collections.ConfigDict({'all': sched})\n",
        "\n",
        "  # *Single* optimizer.\n",
        "  optim = ml_collections.ConfigDict()\n",
        "  optim.optax_name = 'scale_by_adam'\n",
        "  # optim.optax = dict(mu_dtype='bfloat16')\n",
        "  optim.optax_configs = ml_collections.ConfigDict({  # Optimizer settings.\n",
        "      'b1': 0.9,\n",
        "      'b2': 0.95,\n",
        "  })\n",
        "  config.optax = dict(mu_dtype='bfloat16')\n",
        "  optim.max_grad_norm = 1.0\n",
        "\n",
        "  optim.weight_decay = 1e-4\n",
        "  optim.weight_decay_decouple = True\n",
        "  config.optimizer = optim\n",
        "\n",
        "  # Fewshot.\n",
        "  # TODO(girishvn): This needs to be adapted to electrode dataset\n",
        "  config.fewshot = get_config_common_few_shot(batch_size=config.batch_size)\n",
        "  config.fewshot.datasets = {}\n",
        "  config.fewshot.walk_first = ()\n",
        "  config.fewshot.representation_layer = 'pre_logits'\n",
        "  config.fewshot.log_eval_steps = LOG_EVAL_SUMMARY_STEPS\n",
        "\n",
        "  # Logging.\n",
        "  config.write_summary = True\n",
        "  config.xprof = True  # Profile using xprof.\n",
        "  config.checkpoint = True  # Do checkpointing.\n",
        "  config.checkpoint_steps = LOG_CHECKPOINT_STEPS\n",
        "  config.debug_train = False  # Debug mode during training.\n",
        "  config.debug_eval = False  # Debug mode during eval.\n",
        "  config.max_checkpoints_to_keep = MAX_NUM_CHECKPOINTS\n",
        "  # BEGIN GOOGLE-INTERNAL\n",
        "  if runlocal:\n",
        "    # Current implementation fails with UPTC.\n",
        "    config.count_flops = False\n",
        "  # END GOOGLE-INTERNAL\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "# BEGIN GOOGLE-INTERNAL\n",
        "def get_hyper(hyper):\n",
        "  \"\"\"Defines the hyper-parameters sweeps for doing grid search.\"\"\"\n",
        "  return hyper.product([\n",
        "      hyper.sweep('config.schedule.all.lr_configs.base_learning_rate', LRS),\n",
        "  ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8Ho2Xzoyk5uP"
      },
      "outputs": [],
      "source": [
        "# @title Data Dir\n",
        "\n",
        "data_dir='/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-electrodes/deid/exp/dmcduff/ttl=6w/msa_1_5/lsm_tfds_datasets'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OuvkeLkiEWK"
      },
      "source": [
        "# Mood / Stress Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXrToNgQDzlF"
      },
      "source": [
        "## 2000 Mood Dataset Explorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TEXmvy0TRcN8"
      },
      "outputs": [],
      "source": [
        "# @title Load Dataset\n",
        "\n",
        "DATASET_NAME = 'lsm_300min_2000_mood_balanced'\n",
        "TRAIN_DATA_SIZE = None\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = lsm_tiny_dataset.get_dataset(config, data_rng)\n",
        "\n",
        "print('Processed Dataset Meta Data:\\n')\n",
        "for k in dataset.meta_data.keys():\n",
        "  print(k, dataset.meta_data[k])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY6e3mbASbBg"
      },
      "outputs": [],
      "source": [
        "# @title Dataset Information\n",
        "\n",
        "dataset_name = 'lsm_prod/' + DATASET_NAME\n",
        "ds, info = tfds.load(\n",
        "    dataset_name,\n",
        "    data_dir=data_dir,\n",
        "    shuffle_files=False,  # NOTE: train shuffle is done below.\n",
        "    with_info=True\n",
        ")\n",
        "print('Dataset Information:\\n')\n",
        "print(info)\n",
        "\n",
        "# Sample Data\n",
        "print('\\n\\nExample Sample Data:\\n')\n",
        "explore_sample_data(dataset, OHE_LABELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7klFEL6LSvS6"
      },
      "outputs": [],
      "source": [
        "# @title Mood Label Breakdown\n",
        "\n",
        "dataset = lsm_tiny_dataset.get_dataset(config, data_rng)\n",
        "train_info = dataset_split_information(dataset.train_iter, event='mood', offset=65536)\n",
        "valid_info = dataset_split_information(dataset.valid_iter, event='mood', offset=65536)\n",
        "\n",
        "train_counter = Counter(train_info['log_value'])\n",
        "valid_counter = Counter(valid_info['log_value'])\n",
        "test_counter =  valid_counter\n",
        "\n",
        "print('Num Examples:')\n",
        "print('Train Samples', train_info['num_examples'])\n",
        "print('Valid Samples', valid_info['num_examples'])\n",
        "\n",
        "print('\\nDataset Breakdown:')\n",
        "train_df = pd.DataFrame.from_dict({'log_value': train_counter.keys(), 'train_count': train_counter.values()})\n",
        "test_df = pd.DataFrame.from_dict({'log_value': test_counter.keys(), 'test_count': test_counter.values()})\n",
        "mood_data2000_df = pd.merge(train_df, test_df, on='log_value', how='outer')\n",
        "mood_data2000_df.sort_values(by=['log_value'], ascending=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_uucoFoifDL"
      },
      "source": [
        "## 2000 Stress Dataset Explorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ik4ugS1bg-2H"
      },
      "outputs": [],
      "source": [
        "# @title Load Dataset\n",
        "\n",
        "DATASET_NAME = 'lsm_300min_2000_stress_balanced'\n",
        "TRAIN_DATA_SIZE = None\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = lsm_tiny_dataset.get_dataset(config, data_rng)\n",
        "\n",
        "print('Processed Dataset Meta Data:\\n')\n",
        "for k in dataset.meta_data.keys():\n",
        "  print(k, dataset.meta_data[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wCUX7_B6hOT0"
      },
      "outputs": [],
      "source": [
        "# @title Stress Label Breakdown\n",
        "\n",
        "dataset = lsm_tiny_dataset.get_dataset(config, data_rng)\n",
        "train_info = dataset_split_information(dataset.train_iter, event='stress', offset=0)\n",
        "valid_info = dataset_split_information(dataset.valid_iter, event='stress', offset=0)\n",
        "\n",
        "train_counter = Counter(train_info['log_value'])\n",
        "valid_counter = Counter(valid_info['log_value'])\n",
        "test_counter =  valid_counter\n",
        "\n",
        "print('Num Examples:')\n",
        "print('Train Samples', train_info['num_examples'])\n",
        "print('Valid Samples', valid_info['num_examples'])\n",
        "\n",
        "print('\\nDataset Breakdown:')\n",
        "train_df = pd.DataFrame.from_dict({'log_value': train_counter.keys(), 'train_count': train_counter.values()})\n",
        "test_df = pd.DataFrame.from_dict({'log_value': test_counter.keys(), 'test_count': test_counter.values()})\n",
        "stress_data2000_df = pd.merge(train_df, test_df, on='log_value', how='outer')\n",
        "stress_data2000_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gqoOzzWDgr5"
      },
      "source": [
        "# Activity Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cq4Faer-De2E"
      },
      "outputs": [],
      "source": [
        "# @title Load Dataset\n",
        "\n",
        "DATASET_NAME = 'lsm_300min_600_activities_balanced_v4'\n",
        "TRAIN_DATA_SIZE = None\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = lsm_tiny_dataset.get_dataset(config, data_rng)\n",
        "\n",
        "print('Processed Dataset Meta Data:\\n')\n",
        "for k in dataset.meta_data.keys():\n",
        "  print(k, dataset.meta_data[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SDsnpqzWDzAP"
      },
      "outputs": [],
      "source": [
        "# @title Dataset Information\n",
        "OHE_LABELS = True\n",
        "dataset_name = 'lsm_prod/' + DATASET_NAME\n",
        "ds, info = tfds.load(\n",
        "    dataset_name,\n",
        "    data_dir=data_dir,\n",
        "    shuffle_files=False,  # NOTE: train shuffle is done below.\n",
        "    with_info=True\n",
        ")\n",
        "print('Dataset Information:\\n')\n",
        "print(info)\n",
        "\n",
        "# Sample Data\n",
        "print('\\n\\nExample Sample Data:\\n')\n",
        "explore_sample_data(dataset, OHE_LABELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nwU931awD4UJ"
      },
      "outputs": [],
      "source": [
        "# @title Exercise Label Breakdown\n",
        "\n",
        "dataset = lsm_tiny_dataset.get_dataset(config, data_rng)\n",
        "train_info = dataset_split_information(dataset.train_iter, event='exercise', offset=65536)\n",
        "valid_info = dataset_split_information(dataset.valid_iter, event='exercise', offset=65536)\n",
        "\n",
        "train_counter = Counter(train_info['log_value'])\n",
        "valid_counter = Counter(valid_info['log_value'])\n",
        "\n",
        "print('Num Examples:')\n",
        "print('Train Samples', train_info['num_examples'])\n",
        "print('Valid Samples', valid_info['num_examples'])\n",
        "\n",
        "print('\\nDataset Breakdown:')\n",
        "train_df = pd.DataFrame.from_dict({'log_value': train_counter.keys(), 'train_count': train_counter.values()})\n",
        "test_df = pd.DataFrame.from_dict({'log_value': valid_counter.keys(), 'test_count': valid_counter.values()})\n",
        "data_activity_large_df = pd.merge(train_df, test_df, on='log_value', how='outer')\n",
        "data_activity_large_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JWx0H7UHOCQ"
      },
      "source": [
        "#  Fewshot Exercise Detection (ACTIVITY VS MOOD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbH8c70ZHfU6"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = 'fewshot_lsm_300min_mood_vs_activity'\n",
        "TRAIN_DATA_SIZE = None\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "config.update({'fewshot_samples_per_class': 2})\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = lsm_fewshot_mood_vs_activity_dataset.get_dataset(config, data_rng)\n",
        "\n",
        "print('Processed Dataset Meta Data:\\n')\n",
        "for k in dataset.meta_data.keys():\n",
        "  print(k, dataset.meta_data[k])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6dsmoxEH4Lf"
      },
      "outputs": [],
      "source": [
        "config.update({'fewshot_samples_per_class': 6200})\n",
        "\n",
        "dataset = get_dataset(config, data_rng)\n",
        "train_info = dataset_split_information(dataset.train_iter, event=None, offset=65536)\n",
        "valid_info = dataset_split_information(dataset.valid_iter, event=None, offset=65536)\n",
        "\n",
        "train_counter = Counter(train_info['log_value'])\n",
        "valid_counter = Counter(valid_info['log_value'])\n",
        "test_counter =  valid_counter\n",
        "\n",
        "print('Num Examples:')\n",
        "print('Train Samples', train_info['num_examples'])\n",
        "print('Valid Samples', valid_info['num_examples'])\n",
        "\n",
        "print('\\nDataset Breakdown:')\n",
        "train_df = pd.DataFrame.from_dict({'log_value': train_counter.keys(), 'train_count': train_counter.values()})\n",
        "test_df = pd.DataFrame.from_dict({'log_value': test_counter.keys(), 'test_count': test_counter.values()})\n",
        "mood_data2000_df = pd.merge(train_df, test_df, on='log_value', how='outer')\n",
        "mood_data2000_df.sort_values(by=['log_value'], ascending=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI4nFG-CiNN1"
      },
      "source": [
        "# Biological Sex Dataset (Derived from Activity Dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JbpeduwdxtQx"
      },
      "outputs": [],
      "source": [
        "# @title Get Dataset\n",
        "\n",
        "DATASET_NAME = 'lsm_300min_600_biological_sex'\n",
        "TRAIN_DATA_SIZE = None\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "\n",
        "start_t = time.time()\n",
        "dataset = lsm_biological_sex_dataset.get_dataset(config, data_rng)\n",
        "end_t = time.time()\n",
        "\n",
        "print('Dataset Time', end_t - start_t)\n",
        "\n",
        "print('\\nProcessed Dataset Meta Data:\\n')\n",
        "for k in dataset.meta_data.keys():\n",
        "  print(k, dataset.meta_data[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jsQzQtMwyCcz"
      },
      "outputs": [],
      "source": [
        "# @title Train Sample  Breakdown\n",
        "\n",
        "state_t = time.time()\n",
        "\n",
        "label_list = []\n",
        "batch_count = 0\n",
        "for d in dataset.train_iter:\n",
        "  if batch_count % 1000 == 0:\n",
        "    print(batch_count, time.time())\n",
        "  batch_count += 1\n",
        "\n",
        "  bmask = d['batch_mask']\n",
        "  valid = np.where(bmask == 1)\n",
        "\n",
        "  # OHE label\n",
        "  label = jnp.argmax(d['label'], axis=-1)\n",
        "  label = label[valid]\n",
        "  label = label.tolist()\n",
        "  label_list += label\n",
        "\n",
        "end_t = time.time()\n",
        "\n",
        "print('Time', end_t - state_t)\n",
        "print('\\nTrain Data Splits:')\n",
        "mood_counter = Counter()\n",
        "for l in label_list:\n",
        "  mood_counter[l] += 1\n",
        "\n",
        "for k in mood_counter.keys():\n",
        "  print(k, mood_counter[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mszgDS8OumtF"
      },
      "outputs": [],
      "source": [
        "# @title Valid Sample Breakdown\n",
        "\n",
        "state_t = time.time()\n",
        "\n",
        "label_list = []\n",
        "batch_count = 0\n",
        "for d in dataset.valid_iter:\n",
        "  if batch_count % 1000 == 0:\n",
        "    print(batch_count, time.time())\n",
        "  batch_count += 1\n",
        "\n",
        "  bmask = d['batch_mask']\n",
        "  valid = np.where(bmask == 1)\n",
        "\n",
        "  # OHE label\n",
        "  label = jnp.argmax(d['label'], axis=-1)\n",
        "  label = label[valid]\n",
        "  label = label.tolist()\n",
        "  label_list += label\n",
        "\n",
        "end_t = time.time()\n",
        "\n",
        "print('Time', end_t - state_t)\n",
        "print('\\nValid Data Splits:')\n",
        "mood_counter = Counter()\n",
        "for l in label_list:\n",
        "  mood_counter[l] += 1\n",
        "\n",
        "for k in mood_counter.keys():\n",
        "  print(k, mood_counter[k])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah-dnDCkKoz8"
      },
      "source": [
        "# Age Dataset (Derived from Activity Dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qzR8kQVPWr2f"
      },
      "outputs": [],
      "source": [
        "# @title Get Dataset\n",
        "\n",
        "DATASET_NAME = 'lsm_300min_600_binnned_age'\n",
        "TRAIN_DATA_SIZE = None\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "\n",
        "start_t = time.time()\n",
        "dataset = lsm_binned_age_dataset.get_dataset(config, data_rng)\n",
        "end_t = time.time()\n",
        "\n",
        "print('Dataset Time', end_t - start_t)\n",
        "\n",
        "print('\\nProcessed Dataset Meta Data:\\n')\n",
        "for k in dataset.meta_data.keys():\n",
        "  print(k, dataset.meta_data[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lNwH5gLqW__H"
      },
      "outputs": [],
      "source": [
        "# @title Train Sample  Breakdown\n",
        "\n",
        "state_t = time.time()\n",
        "\n",
        "label_list = []\n",
        "batch_count = 0\n",
        "for d in dataset.train_iter:\n",
        "  if batch_count % 1000 == 0:\n",
        "    print(batch_count, time.time())\n",
        "  batch_count += 1\n",
        "\n",
        "  bmask = d['batch_mask']\n",
        "  valid = np.where(bmask == 1)\n",
        "\n",
        "  # OHE label\n",
        "  label = jnp.argmax(d['label'], axis=-1)\n",
        "  label = label[valid]\n",
        "  label = label.tolist()\n",
        "  label_list += label\n",
        "\n",
        "end_t = time.time()\n",
        "\n",
        "print('Time', end_t - state_t)\n",
        "print('\\nTrain Data Splits:')\n",
        "mood_counter = Counter()\n",
        "for l in label_list:\n",
        "  mood_counter[l] += 1\n",
        "\n",
        "for k in mood_counter.keys():\n",
        "  print(k, mood_counter[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "S9cmi3sxXshK"
      },
      "outputs": [],
      "source": [
        "# @title Test Sample  Breakdown\n",
        "\n",
        "state_t = time.time()\n",
        "\n",
        "label_list = []\n",
        "batch_count = 0\n",
        "for d in dataset.valid_iter:\n",
        "  if batch_count % 1000 == 0:\n",
        "    print(batch_count, time.time())\n",
        "  batch_count += 1\n",
        "\n",
        "  bmask = d['batch_mask']\n",
        "  valid = np.where(bmask == 1)\n",
        "\n",
        "  # OHE label\n",
        "  label = jnp.argmax(d['label'], axis=-1)\n",
        "  label = label[valid]\n",
        "  label = label.tolist()\n",
        "  label_list += label\n",
        "\n",
        "end_t = time.time()\n",
        "\n",
        "print('Time', end_t - state_t)\n",
        "print('\\nTrain Data Splits:')\n",
        "mood_counter = Counter()\n",
        "for l in label_list:\n",
        "  mood_counter[l] += 1\n",
        "\n",
        "for k in mood_counter.keys():\n",
        "  print(k, mood_counter[k])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDWMHHUrkTjr"
      },
      "source": [
        "# Subject Dependent Mood Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XquWIPguxBx"
      },
      "source": [
        "## BUILD DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "awHla7XxEOut"
      },
      "outputs": [],
      "source": [
        "# @title HELPERS\n",
        "\n",
        "def _bytestring_feature(list_of_bytestrings) -\u003e tf.train.Feature:\n",
        "  return tf.train.Feature(\n",
        "      bytes_list=tf.train.BytesList(value=list_of_bytestrings))\n",
        "\n",
        "\n",
        "def _int_feature(list_of_ints) -\u003e tf.train.Feature:\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\n",
        "\n",
        "\n",
        "def _float_feature(list_of_floats) -\u003e tf.train.Feature:\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))\n",
        "\n",
        "\n",
        "def get_useful_fields(example):\n",
        "\n",
        "  return {\n",
        "      'input_signal': example['input_signal'],\n",
        "      'label': example['label'],\n",
        "      'exercise_log': example['metadata']['exercise_log'],\n",
        "      'mood_log': example['metadata']['mood_log'],\n",
        "      'log_value': example['metadata']['log_value'],\n",
        "  }\n",
        "\n",
        "\n",
        "def serialize_example(input_signal, label, exercise_log, mood_log, log_value):\n",
        "\n",
        "    # Convert to a TFRecord-compatible format\n",
        "    examples_features = {\n",
        "        'input_signal': _float_feature(input_signal.numpy().flatten().tolist()),\n",
        "        'label': _int_feature([label]),\n",
        "        'exercise_log': _int_feature([exercise_log]),\n",
        "        'mood_log': _int_feature([mood_log]),\n",
        "        'log_value': _int_feature([log_value]),\n",
        "    }\n",
        "    # Create an Example protobuf\n",
        "    example_proto = tf.train.Example(\n",
        "        features=tf.train.Features(feature=examples_features)\n",
        "    )\n",
        "    return example_proto.SerializeToString()\n",
        "\n",
        "\n",
        "# Wrap the function for TensorFlow compatibility\n",
        "def tf_serialize_example(example):\n",
        "  input_signal = example['input_signal']\n",
        "  label = example['label']\n",
        "  exercise_log = example['exercise_log']\n",
        "  mood_log = example['mood_log']\n",
        "  log_value = example['log_value']\n",
        "\n",
        "  # Call tf.py_function with individual arguments\n",
        "  tf_string = tf.py_function(\n",
        "      serialize_example,\n",
        "      [input_signal, label, exercise_log, mood_log, log_value],\n",
        "      tf.string\n",
        "  )\n",
        "  return tf_string\n",
        "\n",
        "\n",
        "def add_key(example):\n",
        "  dt = example['metadata']['DT']  # this is the number\n",
        "  id = example['metadata']['ID']  # this is the string\n",
        "\n",
        "  number_str = tf.strings.as_string(dt)  # Convert the int to string\n",
        "  combined_key = tf.strings.join([id, number_str], separator=\"_\")  # Combine with separator\n",
        "  example[\"key\"] = combined_key\n",
        "  return example\n",
        "\n",
        "\n",
        "def filter_by_datetime_key(example, allowed_keys):\n",
        "  key = example['key']\n",
        "  keep_example = tf.reduce_any(tf.math.equal(key, allowed_keys))\n",
        "  return keep_example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OCRJihLLkZjl"
      },
      "outputs": [],
      "source": [
        "# @title LOAD DATASET\n",
        "\n",
        "dataset_name = 'lsm_prod/lsm_300min_2000_mood_balanced'\n",
        "data_dir=(\n",
        "    '/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-electrodes/'\n",
        "    'deid/exp/dmcduff/ttl=6w/msa_1_5/lsm_tfds_datasets'\n",
        ")\n",
        "\n",
        "train_ds, info = tfds.load(\n",
        "    dataset_name,\n",
        "    data_dir=data_dir,\n",
        "    split='train',\n",
        "    shuffle_files=False,\n",
        "    with_info=True,\n",
        ")\n",
        "\n",
        "test_ds, _ = tfds.load(\n",
        "    dataset_name,\n",
        "    data_dir=data_dir,\n",
        "    split='test',\n",
        "    shuffle_files=False,\n",
        "    with_info=True,\n",
        ")\n",
        "\n",
        "ds = train_ds.concatenate(test_ds)\n",
        "\n",
        "\n",
        "# DECODE SUBJECT ID\n",
        "\n",
        "def subj_decode_example(example):\n",
        "  subj_id = example['metadata']['ID']\n",
        "  dt = example['metadata']['DT']\n",
        "\n",
        "  def decode_id(id_tensor):\n",
        "    return id_tensor.numpy().decode('utf-8')\n",
        "\n",
        "  def decode_dt(dt_tensor):\n",
        "    return dt_tensor.numpy()\n",
        "\n",
        "  example['metadata']['ID'] = tf.py_function(decode_id, [subj_id], tf.string)\n",
        "  example['metadata']['DT'] = tf.py_function(decode_dt, [dt], tf.int64)\n",
        "  return example\n",
        "\n",
        "ds = ds.map(subj_decode_example)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RdYK87tmlDqn"
      },
      "outputs": [],
      "source": [
        "# @title GET DATASET STATS\n",
        "\n",
        "start_t = time.time()\n",
        "t_wind = 30 * 60  # 30 minutes\n",
        "event_dict = {}\n",
        "\n",
        "for d in ds:\n",
        "  md = d['metadata']\n",
        "  dt = md['DT']\n",
        "  id = md['ID']\n",
        "  id = id.numpy().decode('utf-8')\n",
        "  dt = dt.numpy()\n",
        "\n",
        "  # Check if subject ID in event_dict\n",
        "  if id in event_dict.keys():\n",
        "    event_dict[id]['subj_count'] += 1\n",
        "\n",
        "    # Iterate over existing the event time stamps\n",
        "    dt_overlap = False\n",
        "    for dt_k in event_dict[id].keys():\n",
        "      if dt_k == 'subj_count':\n",
        "        continue\n",
        "\n",
        "      # If stamp stamps are similar\n",
        "      # append to list\n",
        "      if dt \u003c= dt_k + t_wind and dt \u003e= dt_k - t_wind:\n",
        "        dt_overlap = True\n",
        "        event_dict[id][dt_k].append(dt)\n",
        "        break\n",
        "\n",
        "    # If no similar time stamp exists\n",
        "    if not dt_overlap:\n",
        "      event_dict[id][dt] = [dt]\n",
        "\n",
        "  # If not add it to the dictionary\n",
        "  else:\n",
        "    event_dict[id] = {}\n",
        "    event_dict[id]['subj_count'] = 1  # init subject count to 1\n",
        "    event_dict[id][dt] = [dt]  # init a list of date time events in this range\n",
        "\n",
        "end_t = time.time()\n",
        "print('Time', end_t - start_t)\n",
        "\n",
        "\n",
        "# Stats in the form of:\n",
        "# SUBJ ID 1\n",
        "#     subj_count: X\n",
        "#     DT1: []\n",
        "#     DT2: []\n",
        "#     ...\n",
        "#     DTY: []\n",
        "#\n",
        "# ...\n",
        "#\n",
        "# SUBJ ID N\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aCoOvHN_KazB"
      },
      "outputs": [],
      "source": [
        "# @title SPLIT INTO TRAIN AND TEST SAMPLES\n",
        "\n",
        "random.seed(42)\n",
        "min_subj_sample_count = 30\n",
        "min_subj_event_count = 5\n",
        "\n",
        "idx = 0\n",
        "total_samples = 0\n",
        "\n",
        "p_train = 0.8\n",
        "p_test = 0.2\n",
        "\n",
        "train_dt_list = []\n",
        "test_dt_list = []\n",
        "\n",
        "# Iterate through subject IDs\n",
        "for id in event_dict.keys():\n",
        "  sample_count = event_dict[id]['subj_count']  # get number of samples per subject\n",
        "  event_count = len(event_dict[id].keys()) - 1  # get number of events per subject\n",
        "\n",
        "  if (\n",
        "      sample_count \u003e= min_subj_sample_count and\n",
        "      event_count \u003e= min_subj_event_count\n",
        "  ):\n",
        "\n",
        "    # Get list of the events\n",
        "    event_info = event_dict[id]\n",
        "\n",
        "    # Pop Subject Sample Count Off the List\n",
        "    events_dt_list = list(event_info.keys())\n",
        "    events_dt_list.remove('subj_count')\n",
        "\n",
        "    # Take DT Event List\n",
        "    num_events = len(events_dt_list)\n",
        "    num_train = int(p_train*num_events)\n",
        "    num_test = num_events - num_train\n",
        "\n",
        "    # Shuffle events and split\n",
        "    random.shuffle(events_dt_list)\n",
        "    train_keys = events_dt_list[:num_train]\n",
        "    test_kets = events_dt_list[num_train:]\n",
        "\n",
        "    # Assign each datetime per event to either train or test\n",
        "    for k in train_keys:\n",
        "      subj_dt_k = [id + '_' + str(dt) for dt in event_info[k]]\n",
        "      train_dt_list += subj_dt_k\n",
        "      # train_dt_list += event_info[k]\n",
        "    for k in test_kets:\n",
        "      subj_dt_k = [id + '_' + str(dt) for dt in event_info[k]]\n",
        "      test_dt_list += subj_dt_k\n",
        "      # test_dt_list += event_info[k]\n",
        "\n",
        "    # Logging\n",
        "    print(f'{idx}: {id}, {sample_count}, {event_count}')\n",
        "    total_samples += sample_count\n",
        "    idx += 1\n",
        "\n",
        "print('\\nTotal Samples', total_samples)\n",
        "print('Num Train Sample', len(train_dt_list))\n",
        "print('Num Test Sample', len(test_dt_list))\n",
        "\n",
        "count = 0\n",
        "for t in train_dt_list:\n",
        "  if t in test_dt_list:\n",
        "    print(t)\n",
        "    count += 1\n",
        "print('Train Test Overlap Samples', count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "usXpyEnIO7Jo"
      },
      "outputs": [],
      "source": [
        "# @title Filter and Split into Train Test\n",
        "ds = ds.map(add_key)\n",
        "\n",
        "# Splitting Train and Test\n",
        "train_filter_fn = functools.partial(\n",
        "    filter_by_datetime_key, allowed_keys=train_dt_list\n",
        ")\n",
        "test_filter_fn = functools.partial(\n",
        "    filter_by_datetime_key, allowed_keys=test_dt_list\n",
        ")\n",
        "train_ds = ds.filter(train_filter_fn)\n",
        "test_ds = ds.filter(test_filter_fn)\n",
        "\n",
        "# Get useful fields\n",
        "train_ds = train_ds.map(get_useful_fields)\n",
        "test_ds = test_ds.map(get_useful_fields)\n",
        "\n",
        "# Count Samples Per Split\n",
        "train_count = 0\n",
        "test_count = 0\n",
        "for d in train_ds:\n",
        "  train_count += 1\n",
        "for d in test_ds:\n",
        "  test_count += 1\n",
        "print('Train Count', train_count)\n",
        "print('Test Count', test_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Doocz4q4l6tS"
      },
      "outputs": [],
      "source": [
        "# @title Save Dataset Out In Example Form\n",
        "\n",
        "serialized_train_ds = train_ds.map(tf_serialize_example)\n",
        "serialized_test_ds = test_ds.map(tf_serialize_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tvEuJkdxCpd2"
      },
      "outputs": [],
      "source": [
        "# @title Save Examples to TF Records\n",
        "\n",
        "data_dir='/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-electrodes/deid/exp/girishvn/ttl=6w/lsm_processed_datasets/'\n",
        "train_fname = 'processed_subj_dependent_mood_train.tfrecord'\n",
        "test_fname = 'processed_subj_dependent_mood_test.tfrecord'\n",
        "\n",
        "train_fpath = os.path.join(data_dir, train_fname)\n",
        "val_fpath = os.path.join(data_dir, test_fname)\n",
        "\n",
        "train_count = 0\n",
        "val_count = 0\n",
        "\n",
        "start_t = time.time()\n",
        "# Write out train data\n",
        "with tf.io.TFRecordWriter(train_fpath) as writer:\n",
        "  for serialized_example in serialized_train_ds:\n",
        "    writer.write(serialized_example.numpy())\n",
        "\n",
        "    train_count += 1\n",
        "    if train_count % 100 == 0:\n",
        "      print(f'Processed {train_count} examples in {time.time() - start_t} s.')\n",
        "\n",
        "end_t = time.time()\n",
        "print('Train Time', end_t - start_t)\n",
        "\n",
        "start_t = time.time()\n",
        "# Write out test data\n",
        "with tf.io.TFRecordWriter(val_fpath) as writer:\n",
        "  for serialized_example in serialized_test_ds:\n",
        "    writer.write(serialized_example.numpy())\n",
        "\n",
        "    val_count += 1\n",
        "    if val_count % 100 == 0:\n",
        "      print(f'Processed {val_count} examples in {time.time() - start_t} s.')\n",
        "\n",
        "end_t = time.time()\n",
        "print('Test Time', end_t - start_t)\n",
        "print('Train Example Count', train_count)\n",
        "print('Test Example Count', val_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAvQ0MwPrUND"
      },
      "outputs": [],
      "source": [
        "print('Test Time', end_t - start_t)\n",
        "print('Train Example Count', train_count)\n",
        "print('Test Example Count', val_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya884xJEsfV3"
      },
      "source": [
        "## LOAD PREPROCESSED DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQa8DtwPsC-7"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = 'lsm_300min_2000_mood_subject_dependent_preprocessed_40sps'\n",
        "TRAIN_DATA_SIZE = None\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "# config.dataset_configs.update({'samples_per_subject': 40, 'repeat': False})\n",
        "\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "\n",
        "start_t = time.time()\n",
        "dataset = lsm_mood_subj_dependent_preprocessed_40sps_dataset.get_preprocessed_dataset(config, data_rng)\n",
        "end_t = time.time()\n",
        "\n",
        "print('Dataset Time', end_t - start_t)\n",
        "\n",
        "print('\\nProcessed Dataset Meta Data:\\n')\n",
        "for k in dataset.meta_data.keys():\n",
        "  print(k, dataset.meta_data[k])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yYqDmEmH3K6B"
      },
      "outputs": [],
      "source": [
        "# @title Train Sample  Breakdown\n",
        "\n",
        "state_t = time.time()\n",
        "\n",
        "label_list = []\n",
        "batch_count = 0\n",
        "for d in dataset.train_iter:\n",
        "  if batch_count % 1000 == 0:\n",
        "    print(batch_count, time.time())\n",
        "  batch_count += 1\n",
        "\n",
        "  bmask = d['batch_mask']\n",
        "  valid = np.where(bmask == 1)\n",
        "\n",
        "  # OHE label\n",
        "  label = jnp.argmax(d['label'], axis=-1)\n",
        "  label = label[valid]\n",
        "  label = label.tolist()\n",
        "  label_list += label\n",
        "\n",
        "end_t = time.time()\n",
        "\n",
        "print('Time', end_t - state_t)\n",
        "print('\\nTrain Data Splits:')\n",
        "mood_counter = Counter()\n",
        "for l in label_list:\n",
        "  mood_counter[l] += 1\n",
        "\n",
        "for k in mood_counter.keys():\n",
        "  print(k, mood_counter[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fC2tubaW3Xr3"
      },
      "outputs": [],
      "source": [
        "# @title Test Sample  Breakdown\n",
        "\n",
        "state_t = time.time()\n",
        "\n",
        "label_list = []\n",
        "batch_count = 0\n",
        "for d in dataset.valid_iter:\n",
        "  if batch_count % 1000 == 0:\n",
        "    print(batch_count, time.time())\n",
        "  batch_count += 1\n",
        "\n",
        "  bmask = d['batch_mask']\n",
        "  valid = np.where(bmask == 1)\n",
        "\n",
        "  # OHE label\n",
        "  label = jnp.argmax(d['label'], axis=-1)\n",
        "  label = label[valid]\n",
        "  label = label.tolist()\n",
        "  label_list += label\n",
        "\n",
        "end_t = time.time()\n",
        "\n",
        "print('Time', end_t - state_t)\n",
        "print('\\nTrain Data Splits:')\n",
        "mood_counter = Counter()\n",
        "for l in label_list:\n",
        "  mood_counter[l] += 1\n",
        "\n",
        "for k in mood_counter.keys():\n",
        "  print(k, mood_counter[k])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3HwBebOFFIk"
      },
      "source": [
        "# DEPRECATED DATASETS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btKf1SMvhCcT"
      },
      "source": [
        "## DEPRECATED Exercise Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8oZvE5JlKXq"
      },
      "source": [
        "### 600 Sample / Activity Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UANDVpYCpuAT"
      },
      "outputs": [],
      "source": [
        "# @title Load Dataset\n",
        "\n",
        "DATASET_NAME = 'lsm_300min_600_activities_balanced'\n",
        "TRAIN_DATA_SIZE = None\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = lsm_tiny_dataset.get_dataset(config, data_rng)\n",
        "\n",
        "print('Processed Dataset Meta Data:\\n')\n",
        "for k in dataset.meta_data.keys():\n",
        "  print(k, dataset.meta_data[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "v5HOL79OlFJn"
      },
      "outputs": [],
      "source": [
        "# @title Dataset Information\n",
        "\n",
        "dataset_name = 'lsm_prod/' + DATASET_NAME\n",
        "ds, info = tfds.load(\n",
        "    dataset_name,\n",
        "    data_dir=data_dir,\n",
        "    shuffle_files=False,  # NOTE: train shuffle is done below.\n",
        "    with_info=True\n",
        ")\n",
        "print('Dataset Information:\\n')\n",
        "print(info)\n",
        "\n",
        "# Sample Data\n",
        "print('\\n\\nExample Sample Data:\\n')\n",
        "explore_sample_data(dataset, OHE_LABELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qbZ4qYIVM6le"
      },
      "outputs": [],
      "source": [
        "# @title Exercise Label Breakdown\n",
        "\n",
        "dataset = lsm_tiny_dataset.get_dataset(config, data_rng)\n",
        "train_info = dataset_split_information(dataset.train_iter, event='exercise', offset=65536)\n",
        "valid_info = dataset_split_information(dataset.valid_iter, event='exercise', offset=65536)\n",
        "test_info = dataset_split_information(dataset.test_iter, event='exercise', offset=65536)\n",
        "\n",
        "train_counter = Counter(train_info['log_value'])\n",
        "valid_counter = Counter(valid_info['log_value'])\n",
        "test_counter = Counter(test_info['log_value'])\n",
        "test_counter = test_counter + valid_counter\n",
        "\n",
        "print('Num Examples:')\n",
        "print('Train Samples', train_info['num_examples'])\n",
        "print('Valid Samples', valid_info['num_examples'] + test_info['num_examples'])\n",
        "\n",
        "print('\\nDataset Breakdown:')\n",
        "train_df = pd.DataFrame.from_dict({'log_value': train_counter.keys(), 'train_count': train_counter.values()})\n",
        "test_df = pd.DataFrame.from_dict({'log_value': test_counter.keys(), 'test_count': test_counter.values()})\n",
        "data600_df = pd.merge(train_df, test_df, on='log_value', how='outer')\n",
        "data600_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgvmZKi7krLM"
      },
      "source": [
        "### 300 Sample / Activity Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Xbg7aPkJp4Jf"
      },
      "outputs": [],
      "source": [
        "# @title Load Dataset\n",
        "\n",
        "DATASET_NAME = 'lsm_300min_300_activities_balanced'\n",
        "TRAIN_DATA_SIZE = None\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = lsm_tiny_dataset.get_dataset(config, data_rng)\n",
        "\n",
        "print('Processed Dataset Meta Data:\\n')\n",
        "for k in dataset.meta_data.keys():\n",
        "  print(k, dataset.meta_data[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jmgy8wP6p_mH"
      },
      "outputs": [],
      "source": [
        "# @title Dataset Information\n",
        "\n",
        "dataset_name = 'lsm_prod/' + DATASET_NAME\n",
        "ds, info = tfds.load(\n",
        "    dataset_name,\n",
        "    data_dir=data_dir,\n",
        "    shuffle_files=False,  # NOTE: train shuffle is done below.\n",
        "    with_info=True\n",
        ")\n",
        "print('Dataset Information:\\n')\n",
        "print(info)\n",
        "\n",
        "# Sample Data\n",
        "print('\\n\\nExample Sample Data:\\n')\n",
        "explore_sample_data(dataset, OHE_LABELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Dly_uKmcNbfG"
      },
      "outputs": [],
      "source": [
        "# @title Exercise Label Breakdown\n",
        "\n",
        "dataset = lsm_tiny_dataset.get_dataset(config, data_rng)\n",
        "train_info = dataset_split_information(dataset.train_iter, event='exercise', offset=65536)\n",
        "valid_info = dataset_split_information(dataset.valid_iter, event='exercise', offset=65536)\n",
        "test_info = dataset_split_information(dataset.test_iter, event='exercise', offset=65536)\n",
        "\n",
        "train_counter = Counter(train_info['log_value'])\n",
        "valid_counter = Counter(valid_info['log_value'])\n",
        "test_counter = Counter(test_info['log_value'])\n",
        "test_counter = test_counter + valid_counter\n",
        "\n",
        "print('Num Examples:')\n",
        "print('Train Samples', train_info['num_examples'])\n",
        "print('Valid Samples', valid_info['num_examples'] + test_info['num_examples'])\n",
        "\n",
        "print('\\nDataset Breakdown:')\n",
        "train_df = pd.DataFrame.from_dict({'log_value': train_counter.keys(), 'train_count': train_counter.values()})\n",
        "test_df = pd.DataFrame.from_dict({'log_value': test_counter.keys(), 'test_count': test_counter.values()})\n",
        "data300_df = pd.merge(train_df, test_df, on='log_value', how='outer')\n",
        "data300_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQCx12qybWUo"
      },
      "source": [
        "### 600 / 300 Dataset Breakdowns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-gVAjXjbZil"
      },
      "outputs": [],
      "source": [
        "data_df = pd.merge(data600_df, data300_df, on='log_value', how='outer')\n",
        "name_col = [log_value_map_dict[id] for id in data_df['log_value']]\n",
        "data_df['activity_name'] = name_col\n",
        "data_df.columns = ['log_value', 'train600', 'test600', 'train300', 'test300', 'activity_name']\n",
        "data_df = data_df.sort_values(by=['log_value'], ascending=True)\n",
        "\n",
        "data_df[['activity_name', 'log_value', 'train600', 'test600', 'train300', 'test300']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ-Hd7CnF2HZ"
      },
      "outputs": [],
      "source": [
        "label_subset = [90013, 56001, 90014, 90019, 90017, 52000, 90005, 90024, 90009]\n",
        "\n",
        "data_9class_df = data_df[data_df['log_value'].isin(label_subset)]\n",
        "\n",
        "train600_sum = data_9class_df['train600'].sum()\n",
        "test600_sum = data_9class_df['test600'].sum()\n",
        "\n",
        "print('Train 600: ', train600_sum)\n",
        "print('Test 600: ', test600_sum)\n",
        "\n",
        "data_9class_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APbEORyE_2CD"
      },
      "source": [
        "### 600 Sample / Activity Data Subset / 9 Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JyNtTHfbAL87"
      },
      "outputs": [],
      "source": [
        "# @title Load Dataset and Explore Sample Data\n",
        "\n",
        "DATASET_NAME = 'lsm_300min_600_activities_9class_subset'\n",
        "TRAIN_DATA_SIZE = None\n",
        "BATCH_SIZE = 1\n",
        "REPEAT_DATA = False\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = lsm_activity_subset_dataset.get_dataset(config, data_rng)\n",
        "\n",
        "print('Processed Dataset Meta Data:\\n')\n",
        "for k in dataset.meta_data.keys():\n",
        "  print(k, dataset.meta_data[k])\n",
        "\n",
        "# Sample Data\n",
        "print('\\n\\nExample Sample Data:\\n')\n",
        "explore_sample_data(dataset, OHE_LABELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sLsZYMEVu4G2"
      },
      "outputs": [],
      "source": [
        "# @title Exercise Label Breakdown\n",
        "\n",
        "dataset = lsm_activity_subset_dataset.get_dataset(config, data_rng)\n",
        "train_info = dataset_split_information(dataset.train_iter, event='exercise', offset=65536)\n",
        "valid_info = dataset_split_information(dataset.valid_iter, event='exercise', offset=65536)\n",
        "\n",
        "train_counter = Counter(train_info['log_value'])\n",
        "valid_counter = Counter(valid_info['log_value'])\n",
        "test_counter = valid_counter\n",
        "\n",
        "print('Num Examples:')\n",
        "print('Train Samples', train_info['num_examples'])\n",
        "print('Valid Samples', valid_info['num_examples'])\n",
        "\n",
        "print('\\nDataset Breakdown:')\n",
        "train_df = pd.DataFrame.from_dict({'log_value': train_counter.keys(), 'train_count': train_counter.values()})\n",
        "test_df = pd.DataFrame.from_dict({'log_value': test_counter.keys(), 'test_count': test_counter.values()})\n",
        "data600_9class_df = pd.merge(train_df, test_df, on='log_value', how='outer')\n",
        "data600_9class_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuGflYVpBiht"
      },
      "source": [
        "## DEPRECATED Subject Dependent Mood Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDZ-hxFslcDS"
      },
      "source": [
        "### Count Mood Samples By User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icS7SWoinF7o"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'lsm_prod/lsm_300min_2000_mood_balanced'\n",
        "data_dir = '/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-electrodes/deid/exp/dmcduff/ttl=6w/msa_1_5/lsm_tfds_datasets'\n",
        "batch_size = 16\n",
        "\n",
        "train_ds, info = tfds.load(\n",
        "    dataset_name,\n",
        "    data_dir=data_dir,\n",
        "    split='train',\n",
        "    shuffle_files=False,  # NOTE: train shuffle is done below.\n",
        "    with_info=True,\n",
        ")\n",
        "\n",
        "val_ds, info = tfds.load(\n",
        "    dataset_name,\n",
        "    data_dir=data_dir,\n",
        "    split='test',\n",
        "    shuffle_files=True,\n",
        "    with_info=True,\n",
        ")\n",
        "\n",
        "ds = train_ds.concatenate(val_ds)\n",
        "\n",
        "# preprocesses dataset\n",
        "options = tf.data.Options()\n",
        "options.threading.private_threadpool_size = 48\n",
        "ds = ds.with_options(options)\n",
        "\n",
        "ds = ds.batch(batch_size, drop_remainder=False)  # batch\n",
        "ds = ds.prefetch(tf.data.experimental.AUTOTUNE)  # prefetch\n",
        "maybe_pad_batches_train = functools.partial(\n",
        "    dataset_utils.maybe_pad_batch,\n",
        "    train=False,\n",
        "    batch_size=batch_size,\n",
        "    inputs_key='input_signal',\n",
        ")\n",
        "ds = iter(ds)\n",
        "ds = map(dataset_utils.tf_to_numpy, ds)\n",
        "ds = map(maybe_pad_batches_train, ds)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhltFJ4YndYH"
      },
      "outputs": [],
      "source": [
        "state_t = time.time()\n",
        "\n",
        "subj_list = []\n",
        "label_list = []\n",
        "\n",
        "batch_count = 0\n",
        "for d in ds:\n",
        "  if batch_count % 100 == 0:\n",
        "    print(batch_count, time.time())\n",
        "  batch_count += 1\n",
        "\n",
        "  bmask = d['batch_mask']\n",
        "  valid = np.where(bmask == 1)\n",
        "  subjs = d['metadata']['ID'][valid]\n",
        "  log_vals = d['metadata']['log_value'][valid]\n",
        "  subjs = subjs.tolist()\n",
        "  log_vals = log_vals.tolist()\n",
        "\n",
        "  subj_list += subjs\n",
        "  label_list += log_vals\n",
        "\n",
        "end_t = time.time()\n",
        "\n",
        "print('\\nTotal Items:', len(subj_list))\n",
        "print('Traversal Time', end_t - state_t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7Sa3FjxthO9"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({'Subject': subj_list, 'Class': label_list})\n",
        "count_table = df.groupby(['Subject', 'Class']).size().unstack(fill_value=0)\n",
        "count_table['total_labels'] = count_table.sum(axis=1)\n",
        "count_table['all_classes'] = (count_table \u003e 0).all(axis=1)\n",
        "count_table = count_table.sort_values(by=['all_classes', 'total_labels'], ascending=[False, False])\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "count_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ7kv7jFiSai"
      },
      "source": [
        "### Subject Dependent Mood Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WXcJXyY7iTz_"
      },
      "outputs": [],
      "source": [
        "# @title Dataset\n",
        "\n",
        "\"\"\"Electrodes dataset data preprocesser and loader.\n",
        "\n",
        "Adapted from a combination of the following files:\n",
        "google3/third_party/py/scenic/dataset_lib/cifar10_dataset.py\n",
        "google3/third_party/py/scenic/dataset_lib/dataset_utils.py\n",
        "\"\"\"\n",
        "\n",
        "import collections\n",
        "import functools\n",
        "from typing import Any, Optional\n",
        "\n",
        "from absl import logging\n",
        "import jax.numpy as jnp\n",
        "import jax.profiler\n",
        "import ml_collections  # pylint: disable=unused-import\n",
        "from scenic.dataset_lib import dataset_utils\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from google3.experimental.largesensormodels.scenic.datasets import dataset_constants\n",
        "from google3.experimental.largesensormodels.scenic.datasets import lsm_tiny_dataset\n",
        "\n",
        "\n",
        "def filter_allowed_subjects(example, allowed_subjects):\n",
        "  \"\"\"Filter out examples where the label is not in allowed_labels.\"\"\"\n",
        "  subj = example['metadata']['ID']\n",
        "  keep_example = tf.reduce_any(tf.math.equal(subj, allowed_subjects))\n",
        "  return keep_example\n",
        "\n",
        "\n",
        "def update_metadata(\n",
        "    metadata, dataset_name, patch_size, dataset_configs\n",
        "):\n",
        "  \"\"\"Update metadata to reflect resizing and addition of datetime features.\"\"\"\n",
        "  # Setup: Get dataset name, feature shape, and possible datetime features.\n",
        "  metadata_update = dict()\n",
        "  dataset_name = dataset_name.split('/')[-1]\n",
        "  time_features = dataset_constants.lsm_dataset_constants[dataset_name].get(\n",
        "      'datetime_features', None\n",
        "  )\n",
        "  feature_shape = list(metadata['input_shape'][1:])\n",
        "  feature_indices = list(range(feature_shape[1]))\n",
        "\n",
        "  # Split features from time series features\n",
        "  # NOTE: This assumes that the original 'input_signal' field has sensor\n",
        "  # features contactanated to datetime features along the feature (w) dimension.\n",
        "  if time_features is not None:\n",
        "    # Get datetime indicies\n",
        "    time_feature_indices = list(time_features['indices'])\n",
        "    # Remove datetime indices from feature indices\n",
        "    feature_indices = list(set(feature_indices) - set(time_features['indices']))\n",
        "    # Get updated feature and datetime feature shapes.\n",
        "    time_feature_shape = feature_shape.copy()  # update time feature shape\n",
        "    time_feature_shape[1] = len(time_feature_indices)\n",
        "    feature_shape[1] = len(feature_indices)  # update feature shape\n",
        "  else:\n",
        "    time_feature_shape = None\n",
        "\n",
        "  # Padding: Update shape to reflect padding (for perfect patching).\n",
        "  # valid_feats arrays denote which features are valid (1) vs padded (0).\n",
        "  # 1. Update for sensor features\n",
        "  _, pad_w, feat_shape_new = lsm_tiny_dataset.get_height_crop_width_pad(\n",
        "      tuple(feature_shape), patch_size\n",
        "  )\n",
        "  valid_feat_mask = [0] * pad_w[0] + [1] * feature_shape[1] + [0] * pad_w[1]\n",
        "  metadata_update['input_shape'] = tuple([-1] + list(feat_shape_new))\n",
        "  metadata_update['input_valid_feats'] = tuple(valid_feat_mask)\n",
        "\n",
        "  # 2. Update for datetime features\n",
        "  if time_features is not None:\n",
        "    _, time_pad_w, time_feature_shape_new = (\n",
        "        lsm_tiny_dataset.get_height_crop_width_pad(\n",
        "            tuple(time_feature_shape), patch_size\n",
        "        )\n",
        "    )\n",
        "    valid_time_feat_mask = (\n",
        "        [0] * time_pad_w[0] + [1] * time_feature_shape[1] + [0] * time_pad_w[1]\n",
        "    )\n",
        "    metadata_update['datetime_input_shape'] = tuple(\n",
        "        [-1] + list(time_feature_shape_new)\n",
        "    )\n",
        "    metadata_update['datime_valid_feats'] = tuple(valid_time_feat_mask)\n",
        "\n",
        "  else:\n",
        "    metadata_update['datetime_input_shape'] = None\n",
        "    metadata_update['datime_valid_feats'] = None\n",
        "\n",
        "  # Update if dataset it one-hot-encoded or not.\n",
        "  if 'activities' in dataset_name or 'mood' in dataset_name:\n",
        "    metadata_update['target_is_onehot'] = True\n",
        "    metadata_update['num_classes'] = len(\n",
        "        dataset_constants.lsm_dataset_constants[dataset_name]['log_values']\n",
        "    )\n",
        "  elif 'stress' in dataset_name:\n",
        "    metadata_update['target_is_onehot'] = True\n",
        "    metadata_update['num_classes'] = 2\n",
        "\n",
        "  # 4. Add dataset log values and log value names and number of classes.\n",
        "  log_values = dataset_constants.lsm_dataset_constants[dataset_name].get(\n",
        "      'log_values', None\n",
        "  )\n",
        "  log_value_names = dataset_constants.lsm_dataset_constants[dataset_name].get(\n",
        "      'log_value_names', None\n",
        "  )\n",
        "  metadata_update['log_values'] = log_values\n",
        "  metadata_update['log_value_names'] = log_value_names\n",
        "\n",
        "  # 7. Update time cropping:\n",
        "  start, end = dataset_configs.get('relative_time_window', (None, None))\n",
        "  if end is None:\n",
        "    end = 1\n",
        "  if start is None:\n",
        "    start = 0\n",
        "\n",
        "  # Time Crop image based on horizon.\n",
        "  # Get number of patches along time axis (h).\n",
        "  p_h = patch_size[0]\n",
        "  h = feat_shape_new[0]\n",
        "  n_h = h // p_h\n",
        "  start_idx = int(start * n_h) * p_h\n",
        "  end_idx = int(end * n_h) * p_h\n",
        "  metadata_update['input_shape'] = tuple(\n",
        "      [-1] + [end_idx - start_idx] + list(feat_shape_new)[1:]\n",
        "  )\n",
        "\n",
        "  return metadata_update\n",
        "\n",
        "\n",
        "def get_subject_dependent_mood_dataset(\n",
        "    *,\n",
        "    config,\n",
        "    num_shards,\n",
        "    batch_size,\n",
        "    eval_batch_size=None,\n",
        "    dtype_str='float32',\n",
        "    shuffle_seed=0,\n",
        "    rng=None,\n",
        "    shuffle_buffer_size=None,\n",
        "    dataset_service_address: Optional[str] = None,\n",
        "    data_dir='/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-electrodes/deid/exp/dmcduff/ttl=6w/msa_1_5/lsm_tfds_datasets',\n",
        "):\n",
        "  \"\"\"Gets and formats the Subject Dependent Mood Dataset.\n",
        "\n",
        "  Adapted from:\n",
        "  google3/third_party/py/scenic/dataset_lib/cifar10_dataset.py and\n",
        "  google3/third_party/py/scenic/dataset_lib/dataset_utils.py.\n",
        "\n",
        "  Args:\n",
        "    config: ml_collections.ConfigDict; Config for the experiment.\n",
        "    num_shards: int; Number of shards to split the dataset into.\n",
        "    batch_size: int; Batch size for training.\n",
        "    eval_batch_size: int; Batch size for evaluation.\n",
        "    dtype_str: str; Data type of the image.\n",
        "    shuffle_seed: int; Seed for shuffling the dataset.\n",
        "    rng: jax.random.PRNGKey; Random number generator key.\n",
        "    shuffle_buffer_size: int; Size of the shuffle buffer.\n",
        "    dataset_service_address: str; Address of the dataset service.\n",
        "    data_dir: str; Directory of the dataset.\n",
        "\n",
        "  Returns:\n",
        "    A dataset_utils.Dataset object.\n",
        "  \"\"\"\n",
        "\n",
        "  # Setup: General\n",
        "  if rng is None:\n",
        "    rng = jax.random.PRNGKey(config.rng_seed)\n",
        "\n",
        "  # 1. Process information.\n",
        "  p_idx = jax.process_index()  # current process index\n",
        "  p_cnt = jax.process_count()  # process count (number of processes)\n",
        "\n",
        "  aug_rngs = jax.random.split(rng, p_cnt)  # per-device augmentation seeds\n",
        "  aug_rng = aug_rngs[p_idx]  # device augmentation seed\n",
        "  tf_aug_rng = aug_rng[0]  # jax random seeds are arrays, tf expects an int.\n",
        "  del rng\n",
        "\n",
        "  # 2. dataset and data type information.\n",
        "  dataset_configs = config.dataset_configs  # get dataset configurations.\n",
        "  dtype = getattr(tf, dtype_str)  # data dtype\n",
        "  if eval_batch_size is None:  # set eval batch size\n",
        "    eval_batch_size = batch_size\n",
        "\n",
        "  # 3. Used dataset name.\n",
        "  used_dataset_name = 'lsm_prod/lsm_300min_2000_mood_balanced'\n",
        "\n",
        "  # 4. Repeat dataset.\n",
        "  repeat_ds = dataset_configs.get('repeat_data', True)\n",
        "\n",
        "  # Setup: Mapping functions.\n",
        "  # 2. Preprocessing, augmentation, and cropping/padding functions.\n",
        "  preprocess_fn = functools.partial(\n",
        "      lsm_tiny_dataset.preprocess_example,\n",
        "      dataset_name=used_dataset_name,\n",
        "      dtype=dtype\n",
        "  )\n",
        "  # 3. Augmentation function.\n",
        "  augment_fn = functools.partial(\n",
        "      lsm_tiny_dataset.augment_example,\n",
        "      augmentations=config.get('train_augmentations', []),\n",
        "      seed=tf_aug_rng,\n",
        "  )\n",
        "  # 4. Crop and pad features and time features to be patch size compatible.\n",
        "  crop_and_pad_fn = functools.partial(\n",
        "      lsm_tiny_dataset.patch_compatible_resize_example,\n",
        "      patch_size=config.model.patches.size\n",
        "  )\n",
        "\n",
        "  # 5. Time crop data input\n",
        "  start, end = dataset_configs.get('relative_time_window', (None, None))\n",
        "  if (start is not None) or (end is not None):\n",
        "    time_crop_examples = True\n",
        "  else:\n",
        "    time_crop_examples = False\n",
        "  time_crop_fn = functools.partial(\n",
        "      lsm_tiny_dataset.time_crop_example,\n",
        "      patch_size=config.model.patches.size,\n",
        "      start=start,\n",
        "      end=end\n",
        "  )\n",
        "\n",
        "  # Setup: Data splits.\n",
        "  # Load dataset splits.\n",
        "  train_ds = tfds.load(\n",
        "      used_dataset_name,\n",
        "      data_dir=data_dir,\n",
        "      split='train',\n",
        "      shuffle_files=False,  # NOTE: train shuffle is done below.\n",
        "  )\n",
        "  val_ds = tfds.load(\n",
        "      used_dataset_name,\n",
        "      data_dir=data_dir,\n",
        "      split='test',\n",
        "      shuffle_files=False,\n",
        "  )\n",
        "  logging.info(  # pylint:disable=logging-fstring-interpolation\n",
        "      'Loaded combined train + val split '\n",
        "      f'{p_idx}/{p_cnt} from {used_dataset_name}.'\n",
        "  )\n",
        "  ds = train_ds.concatenate(val_ds)\n",
        "\n",
        "  # Data processing and preperation.\n",
        "  # 0. Enable multi threaded workers.\n",
        "  options = tf.data.Options()\n",
        "  options.threading.private_threadpool_size = 48\n",
        "  ds = ds.with_options(options)\n",
        "\n",
        "  # 1. Per-process split: Split splits evenly per worker).\n",
        "  # Count samples per subject.\n",
        "  subj_label_counts = collections.Counter()\n",
        "  for d in ds:\n",
        "    subj = d['metadata']['ID']\n",
        "    subj_label_counts[subj.numpy().decode('utf-8')] += 1\n",
        "\n",
        "  # Filter down to subjects with at least N samples.\n",
        "  allowed_subjs = [\n",
        "      subj for subj, count in subj_label_counts.items()\n",
        "      if count \u003e= dataset_configs.samples_per_subject\n",
        "  ]\n",
        "  filter_fn = functools.partial(\n",
        "      filter_allowed_subjects, allowed_subjects=allowed_subjs\n",
        "  )\n",
        "  ds = ds.filter(filter_fn)\n",
        "\n",
        "  # Split the data into train and val splits.\n",
        "  # Splits each participants data evenly between train and val.\n",
        "  def filter_by_subj(subj):\n",
        "    return lambda x: tf.equal(\n",
        "        x['metadata']['ID'], subj\n",
        "    )\n",
        "\n",
        "  train_subj_splits, valid_subj_splits = [], []\n",
        "  num_train_samples, num_val_samples = 0, 0\n",
        "  for subj in allowed_subjs:\n",
        "    subj_ds = ds.filter(filter_by_subj(subj))\n",
        "    size_subj_ds = sum(1 for _ in subj_ds)\n",
        "\n",
        "    train_size = int(0.8 * size_subj_ds)\n",
        "    num_train_samples += train_size\n",
        "    num_val_samples += size_subj_ds - train_size\n",
        "\n",
        "    subj_train_split = subj_ds.take(train_size)\n",
        "    subj_val_split = subj_ds.skip(train_size)\n",
        "\n",
        "    train_subj_splits.append(subj_train_split)\n",
        "    valid_subj_splits.append(subj_val_split)\n",
        "\n",
        "  # Concat class datasets.\n",
        "  train_ds = train_subj_splits[0]\n",
        "  val_ds = valid_subj_splits[0]\n",
        "  for i in range(1, len(allowed_subjs)):\n",
        "    train_ds = train_ds.concatenate(train_subj_splits[i])\n",
        "    val_ds = val_ds.concatenate(valid_subj_splits[i])\n",
        "\n",
        "  # Get samples per class\n",
        "  spc = collections.Counter()\n",
        "  for d in train_ds:\n",
        "    log_val = int(d['metadata']['log_value'])\n",
        "    spc[log_val] += 1\n",
        "\n",
        "  spc_labels = tf.convert_to_tensor(list(spc.keys()))\n",
        "  spc_label_counts = tf.convert_to_tensor(list(spc.values()))\n",
        "\n",
        "  # Get mood log values\n",
        "  dataset_key = used_dataset_name.split('/')[-1]\n",
        "  offset = tf.cast(\n",
        "      dataset_constants.lsm_dataset_constants[dataset_key]['log_value_offset'],\n",
        "      tf.int32\n",
        "  )\n",
        "  log_val_list = tf.convert_to_tensor(\n",
        "      dataset_constants.lsm_dataset_constants[dataset_key]['log_values']\n",
        "  )\n",
        "  log_val_list = log_val_list - offset  # offset value\n",
        "\n",
        "  sorted_indices_tensor2 = tf.argsort(spc_labels)\n",
        "  sorted_tensor2 = tf.gather(spc_labels, sorted_indices_tensor2)\n",
        "  matching_indices = tf.argsort(tf.argsort(log_val_list))\n",
        "  mapping = tf.gather(sorted_indices_tensor2, matching_indices)\n",
        "  label_counts = tf.gather(spc_label_counts, mapping)\n",
        "\n",
        "  # Split dataset over host devices.\n",
        "  train_ds = train_ds.shard(p_cnt, p_idx)\n",
        "  val_ds = val_ds.shard(p_cnt, p_idx)\n",
        "\n",
        "  # 2. Preprocessing: Applied before `ds.cache()` to re-use it.\n",
        "  train_ds = train_ds.map(\n",
        "      preprocess_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "  val_ds = val_ds.map(\n",
        "      preprocess_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "\n",
        "  # 3. Cache datasets: This can signficantly speed up training.\n",
        "  if dataset_configs.cache_dataset:\n",
        "    train_ds = train_ds.cache()\n",
        "    val_ds = val_ds.cache()\n",
        "\n",
        "  # 4 Train repeats and augmentations.\n",
        "  if repeat_ds:\n",
        "    train_ds = train_ds.repeat()  # repeat\n",
        "  # NOTE: Train augmentations are done after repeat for true randomness.\n",
        "  if config.use_train_augmentations:\n",
        "    train_ds = train_ds.map(  # train data augmentations\n",
        "        augment_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "    )\n",
        "\n",
        "  # 5. Crop and pad for perfect patching.\n",
        "  train_ds = train_ds.map(  # crop/pad for perfect patching\n",
        "      crop_and_pad_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "  val_ds = val_ds.map(  # crop/pad for perfect patching\n",
        "      crop_and_pad_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "\n",
        "  # 6. Time crop input data.\n",
        "  if time_crop_examples:\n",
        "    train_ds = train_ds.map(\n",
        "        time_crop_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "    )\n",
        "    val_ds = val_ds.map(\n",
        "        time_crop_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "    )\n",
        "\n",
        "  # 7. Data preperation (shuffling, augmentations, batching, eval repeat, etc.).\n",
        "  # 7a. Train: Shuffle, batch, prefetch\n",
        "  shuffle_buffer_size = shuffle_buffer_size or (8 * batch_size)\n",
        "  train_ds = train_ds.shuffle(shuffle_buffer_size, seed=shuffle_seed)  # shuffle\n",
        "  train_ds = train_ds.batch(batch_size, drop_remainder=True)  # batch\n",
        "  train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)  # prefetch\n",
        "\n",
        "  # 7b. Validation: Batch, Repeat, Prefetch\n",
        "  val_ds = val_ds.batch(batch_size, drop_remainder=False)  # batch\n",
        "  if repeat_ds:\n",
        "    val_ds = val_ds.repeat()  # repeat\n",
        "  val_ds = val_ds.prefetch(tf.data.experimental.AUTOTUNE)  # prefetch\n",
        "\n",
        "  # Ensure that no seed is set if dataset_service_address is defined.\n",
        "  if dataset_service_address:\n",
        "    if shuffle_seed is not None:\n",
        "      raise ValueError(\n",
        "          'Using dataset service with a random seed causes each '\n",
        "          'worker to produce exactly the same data. Add '\n",
        "          'config.shuffle_seed = None to your config if you '\n",
        "          'want to run with dataset service.'\n",
        "      )\n",
        "    train_ds = dataset_utils.distribute(train_ds, dataset_service_address)\n",
        "    logging.info('Using the tf.data service at %s', dataset_service_address)\n",
        "\n",
        "  # Other mappings\n",
        "  # 1. Set up batch padding: If batch remainders are NOT dropped batches may be\n",
        "  # padded to allow for an enough patches to contain all samples.\n",
        "  maybe_pad_batches_train = functools.partial(\n",
        "      dataset_utils.maybe_pad_batch,\n",
        "      train=True,\n",
        "      batch_size=batch_size,\n",
        "      inputs_key='input_signal',\n",
        "  )\n",
        "  maybe_pad_batches_eval = functools.partial(\n",
        "      dataset_utils.maybe_pad_batch,\n",
        "      train=False,\n",
        "      batch_size=eval_batch_size,\n",
        "      inputs_key='input_signal',\n",
        "  )\n",
        "\n",
        "  # 2. Set up batch sharding: Shard batches to be processed by multiple devices.\n",
        "  shard_batches = functools.partial(dataset_utils.shard, n_devices=num_shards)\n",
        "\n",
        "  # 3. Apply other mappings and Iter dataset\n",
        "  train_iter = iter(train_ds)\n",
        "  train_iter = map(dataset_utils.tf_to_numpy, train_iter)\n",
        "  train_iter = map(maybe_pad_batches_train, train_iter)\n",
        "  train_iter = map(shard_batches, train_iter)\n",
        "\n",
        "  val_iter = iter(val_ds)\n",
        "  val_iter = map(dataset_utils.tf_to_numpy, val_iter)\n",
        "  val_iter = map(maybe_pad_batches_eval, val_iter)\n",
        "  val_iter = map(shard_batches, val_iter)\n",
        "\n",
        "  # Save meta data\n",
        "  info = tfds.builder(used_dataset_name, data_dir=data_dir, try_gcs=True).info\n",
        "  input_shape = tuple([-1] + list(info.features['input_signal'].shape))\n",
        "  meta_data = {\n",
        "      'input_shape': input_shape,\n",
        "      'num_train_examples': num_train_samples,\n",
        "      'num_val_examples': num_val_samples,\n",
        "      'num_test_examples': 0,\n",
        "      'input_dtype': getattr(jnp, dtype_str),\n",
        "      'label_counts': label_counts,\n",
        "      # The following two fields are set as defaults and may be updated in the\n",
        "      # update_metadata function below.\n",
        "      'target_is_onehot': False,\n",
        "      'num_classes': None,\n",
        "  }\n",
        "\n",
        "  # Update metadata to reflect preprocessing, and paddings\n",
        "  # (Changes in shape, and features).\n",
        "  meta_data.update(\n",
        "      update_metadata(\n",
        "          meta_data,\n",
        "          dataset_name=used_dataset_name,\n",
        "          patch_size=config.model.patches.size,\n",
        "          dataset_configs=dataset_configs,\n",
        "      )\n",
        "  )\n",
        "\n",
        "  # Return dataset structure.\n",
        "  return dataset_utils.Dataset(train_iter, val_iter, None, meta_data)\n",
        "\n",
        "\n",
        "def get_dataset(\n",
        "    config: Any,\n",
        "    data_rng: jnp.ndarray,\n",
        "    *,\n",
        "    num_local_shards: Optional[int] = None,\n",
        "    dataset_service_address: Optional[str] = None,\n",
        "    **kwargs: Any,\n",
        ") -\u003e dataset_utils.Dataset:\n",
        "  \"\"\"Adapted from: google3/third_party/py/scenic/train_lib/train_utils.py.\"\"\"\n",
        "\n",
        "  # Get device count\n",
        "  device_count = jax.device_count()\n",
        "  logging.info('device_count: %d', device_count)\n",
        "  logging.info('num_hosts : %d', jax.process_count())\n",
        "  logging.info('host_id : %d', jax.process_index())\n",
        "\n",
        "  # Set the dataset builder functions\n",
        "  # Get list of supported, non-deprecated datasets.\n",
        "  dataset_name = config.dataset_configs.dataset\n",
        "  dataset_suported_list = ['lsm_300min_2000_mood_subject_dependant']\n",
        "  if dataset_name.split('/')[1] in dataset_suported_list:\n",
        "    dataset_builder = get_subject_dependent_mood_dataset\n",
        "  else:\n",
        "    raise ValueError(f'Dataset {dataset_name} is not supported.')\n",
        "\n",
        "  # Get batch size\n",
        "  batch_size = config.batch_size\n",
        "  if batch_size % device_count \u003e 0:\n",
        "    raise ValueError(\n",
        "        f'Batch size ({batch_size}) must be divisible by the '\n",
        "        f'number of devices ({device_count})'\n",
        "    )\n",
        "\n",
        "  local_batch_size = batch_size // jax.process_count()\n",
        "  device_batch_size = batch_size // device_count\n",
        "  logging.info('local_batch_size : %d', local_batch_size)\n",
        "  logging.info('device_batch_size : %d', device_batch_size)\n",
        "\n",
        "  # Get shuffle seed - ensure it exists\n",
        "  shuffle_seed = config.get('shuffle_seed', None)\n",
        "  if dataset_service_address and shuffle_seed is not None:\n",
        "    raise ValueError(\n",
        "        'Using dataset service with a random seed causes each '\n",
        "        'worker to produce exactly the same data. Add '\n",
        "        'config.shuffle_seed = None to your config if you want '\n",
        "        'to run with dataset service.'\n",
        "    )\n",
        "\n",
        "  # Get shuffle buffer size.\n",
        "  shuffle_buffer_size = config.get('shuffle_buffer_size', None)\n",
        "  # Local shard count.\n",
        "  num_local_shards = num_local_shards or jax.local_device_count()\n",
        "\n",
        "  # Build the dataset\n",
        "  ds = dataset_builder(\n",
        "      config=config,\n",
        "      num_shards=num_local_shards,\n",
        "      batch_size=local_batch_size,\n",
        "      dtype_str=config.data_dtype_str,\n",
        "      shuffle_seed=shuffle_seed,\n",
        "      rng=data_rng,\n",
        "      shuffle_buffer_size=shuffle_buffer_size,\n",
        "      dataset_service_address=dataset_service_address,\n",
        "      **kwargs,\n",
        "  )\n",
        "\n",
        "  return ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neN16gHZiZcu"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = 'lsm_300min_2000_mood_subject_dependant'\n",
        "TRAIN_DATA_SIZE = None\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "config.dataset_configs.update({'samples_per_subject': 40, 'repeat': False})\n",
        "\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "\n",
        "start_t = time.time()\n",
        "dataset = get_dataset(config, data_rng)\n",
        "end_t = time.time()\n",
        "\n",
        "print('Dataset Time', end_t - start_t)\n",
        "\n",
        "print('\\nProcessed Dataset Meta Data:\\n')\n",
        "for k in dataset.meta_data.keys():\n",
        "  print(k, dataset.meta_data[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Kn1kCpsYkQ0h"
      },
      "outputs": [],
      "source": [
        "# @title Train Sample  Breakdown\n",
        "\n",
        "state_t = time.time()\n",
        "\n",
        "label_list = []\n",
        "batch_count = 0\n",
        "for d in dataset.train_iter:\n",
        "  if batch_count % 1000 == 0:\n",
        "    print(batch_count, time.time())\n",
        "  batch_count += 1\n",
        "\n",
        "  bmask = d['batch_mask']\n",
        "  valid = np.where(bmask == 1)\n",
        "  log_vals = d['log_value'][valid]\n",
        "  log_vals = log_vals.tolist()\n",
        "  label_list += log_vals\n",
        "\n",
        "end_t = time.time()\n",
        "\n",
        "print('Time', end_t - state_t)\n",
        "print('\\nTrain Data Splits:')\n",
        "mood_counter = Counter()\n",
        "for l in label_list:\n",
        "  mood_counter[l] += 1\n",
        "\n",
        "for k in mood_counter.keys():\n",
        "  print(k, mood_counter[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WJu3kmyen0kv"
      },
      "outputs": [],
      "source": [
        "# @title Valid Sample  Breakdown\n",
        "\n",
        "state_t = time.time()\n",
        "\n",
        "label_list = []\n",
        "batch_count = 0\n",
        "for d in dataset.valid_iter:\n",
        "  if batch_count % 1000 == 0:\n",
        "    print(batch_count, time.time())\n",
        "  batch_count += 1\n",
        "\n",
        "  bmask = d['batch_mask']\n",
        "  valid = np.where(bmask == 1)\n",
        "  log_vals = d['log_value'][valid]\n",
        "  log_vals = log_vals.tolist()\n",
        "  label_list += log_vals\n",
        "\n",
        "end_t = time.time()\n",
        "\n",
        "print('Time', end_t - state_t)\n",
        "print('\\nValid Data Splits:')\n",
        "mood_counter = Counter()\n",
        "for l in label_list:\n",
        "  mood_counter[l] += 1\n",
        "\n",
        "for k in mood_counter.keys():\n",
        "  print(k, mood_counter[k])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucSx2lTkg7jU"
      },
      "source": [
        "### Save and Load Processed Mood Dataset\n",
        "This is required because running traversals of the dataset on XM is EXTREMELY slow and causing jobs to be killed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDgHxTq5sY23"
      },
      "source": [
        "### Preprocess and Save Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RoJguXbLHGxy"
      },
      "outputs": [],
      "source": [
        "# @title Format To-Save Processed Mood Dataset\n",
        "\n",
        "\"\"\"Electrodes dataset data preprocesser and loader.\n",
        "\n",
        "Adapted from a combination of the following files:\n",
        "google3/third_party/py/scenic/dataset_lib/cifar10_dataset.py\n",
        "google3/third_party/py/scenic/dataset_lib/dataset_utils.py\n",
        "\"\"\"\n",
        "\n",
        "import collections\n",
        "import functools\n",
        "from typing import Any, Optional\n",
        "\n",
        "from absl import logging\n",
        "import jax.numpy as jnp\n",
        "import jax.profiler\n",
        "import ml_collections  # pylint: disable=unused-import\n",
        "from scenic.dataset_lib import dataset_utils\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from google3.experimental.largesensormodels.scenic.datasets import dataset_constants\n",
        "from google3.experimental.largesensormodels.scenic.datasets import lsm_tiny_dataset\n",
        "\n",
        "\n",
        "def _bytestring_feature(list_of_bytestrings) -\u003e tf.train.Feature:\n",
        "  return tf.train.Feature(\n",
        "      bytes_list=tf.train.BytesList(value=list_of_bytestrings))\n",
        "\n",
        "\n",
        "def _int_feature(list_of_ints) -\u003e tf.train.Feature:\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\n",
        "\n",
        "\n",
        "def _float_feature(list_of_floats) -\u003e tf.train.Feature:\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))\n",
        "\n",
        "\n",
        "def serialize_example(input_signal, label, exercise_log, mood_log, log_value):\n",
        "\n",
        "    # Convert to a TFRecord-compatible format\n",
        "    examples_features = {\n",
        "        'input_signal': _float_feature(input_signal.numpy().flatten().tolist()),\n",
        "        'label': _int_feature([label]),\n",
        "        'exercise_log': _int_feature([exercise_log]),\n",
        "        'mood_log': _int_feature([mood_log]),\n",
        "        'log_value': _int_feature([log_value]),\n",
        "    }\n",
        "    # Create an Example protobuf\n",
        "    example_proto = tf.train.Example(\n",
        "        features=tf.train.Features(feature=examples_features)\n",
        "    )\n",
        "    return example_proto.SerializeToString()\n",
        "\n",
        "# Wrap the function for TensorFlow compatibility\n",
        "def tf_serialize_example(example):\n",
        "  input_signal = example['input_signal']\n",
        "  label = example['label']\n",
        "  exercise_log = example['exercise_log']\n",
        "  mood_log = example['mood_log']\n",
        "  log_value = example['log_value']\n",
        "\n",
        "  # Call tf.py_function with individual arguments\n",
        "  tf_string = tf.py_function(\n",
        "      serialize_example,\n",
        "      [input_signal, label, exercise_log, mood_log, log_value],\n",
        "      tf.string\n",
        "  )\n",
        "  return tf_string\n",
        "\n",
        "\n",
        "def filter_allowed_subjects(example, allowed_subjects):\n",
        "  \"\"\"Filter out examples where the label is not in allowed_labels.\"\"\"\n",
        "  subj = example['metadata']['ID']\n",
        "  keep_example = tf.reduce_any(tf.math.equal(subj, allowed_subjects))\n",
        "  return keep_example\n",
        "\n",
        "\n",
        "def get_useful_fields(example):\n",
        "\n",
        "  return {\n",
        "      'input_signal': example['input_signal'],\n",
        "      'label': example['label'],\n",
        "      'exercise_log': example['metadata']['exercise_log'],\n",
        "      'mood_log': example['metadata']['mood_log'],\n",
        "      'log_value': example['metadata']['log_value'],\n",
        "  }\n",
        "\n",
        "\n",
        "def get_subject_dependent_mood_dataset(\n",
        "    *,\n",
        "    config,\n",
        "    num_shards,\n",
        "    batch_size,\n",
        "    eval_batch_size=None,\n",
        "    dtype_str='float32',\n",
        "    shuffle_seed=0,\n",
        "    rng=None,\n",
        "    shuffle_buffer_size=None,\n",
        "    dataset_service_address: Optional[str] = None,\n",
        "    data_dir='/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-electrodes/deid/exp/dmcduff/ttl=6w/msa_1_5/lsm_tfds_datasets',\n",
        "):\n",
        "  \"\"\"Gets and formats the Subject Dependent Mood Dataset.\n",
        "\n",
        "  Adapted from:\n",
        "  google3/third_party/py/scenic/dataset_lib/cifar10_dataset.py and\n",
        "  google3/third_party/py/scenic/dataset_lib/dataset_utils.py.\n",
        "\n",
        "  Args:\n",
        "    config: ml_collections.ConfigDict; Config for the experiment.\n",
        "    num_shards: int; Number of shards to split the dataset into.\n",
        "    batch_size: int; Batch size for training.\n",
        "    eval_batch_size: int; Batch size for evaluation.\n",
        "    dtype_str: str; Data type of the image.\n",
        "    shuffle_seed: int; Seed for shuffling the dataset.\n",
        "    rng: jax.random.PRNGKey; Random number generator key.\n",
        "    shuffle_buffer_size: int; Size of the shuffle buffer.\n",
        "    dataset_service_address: str; Address of the dataset service.\n",
        "    data_dir: str; Directory of the dataset.\n",
        "\n",
        "  Returns:\n",
        "    A dataset_utils.Dataset object.\n",
        "  \"\"\"\n",
        "\n",
        "  # Setup: General\n",
        "  if rng is None:\n",
        "    rng = jax.random.PRNGKey(config.rng_seed)\n",
        "\n",
        "  # 1. Process information.\n",
        "  p_idx = jax.process_index()  # current process index\n",
        "  p_cnt = jax.process_count()  # process count (number of processes)\n",
        "\n",
        "  aug_rngs = jax.random.split(rng, p_cnt)  # per-device augmentation seeds\n",
        "  aug_rng = aug_rngs[p_idx]  # device augmentation seed\n",
        "  tf_aug_rng = aug_rng[0]  # jax random seeds are arrays, tf expects an int.\n",
        "  del rng\n",
        "\n",
        "  # 2. dataset and data type information.\n",
        "  dataset_configs = config.dataset_configs  # get dataset configurations.\n",
        "  dtype = getattr(tf, dtype_str)  # data dtype\n",
        "  if eval_batch_size is None:  # set eval batch size\n",
        "    eval_batch_size = batch_size\n",
        "\n",
        "  # 3. Used dataset name.\n",
        "  used_dataset_name = 'lsm_prod/lsm_300min_2000_mood_balanced'\n",
        "\n",
        "  # Setup: Data splits.\n",
        "  # Load dataset splits.\n",
        "  train_ds = tfds.load(\n",
        "      used_dataset_name,\n",
        "      data_dir=data_dir,\n",
        "      split='train',\n",
        "      shuffle_files=False,  # NOTE: train shuffle is done below.\n",
        "  )\n",
        "  val_ds = tfds.load(\n",
        "      used_dataset_name,\n",
        "      data_dir=data_dir,\n",
        "      split='test',\n",
        "      shuffle_files=False,\n",
        "  )\n",
        "  logging.info(  # pylint:disable=logging-fstring-interpolation\n",
        "      'Loaded combined train + val split '\n",
        "      f'{p_idx}/{p_cnt} from {used_dataset_name}.'\n",
        "  )\n",
        "  ds = train_ds.concatenate(val_ds)\n",
        "\n",
        "  # Data processing and preperation.\n",
        "  # 0. Enable multi threaded workers.\n",
        "  options = tf.data.Options()\n",
        "  options.threading.private_threadpool_size = 48\n",
        "  ds = ds.with_options(options)\n",
        "\n",
        "  # 1. Per-process split: Split splits evenly per worker).\n",
        "  # Count samples per subject.\n",
        "  subj_label_counts = collections.Counter()\n",
        "  for d in ds:\n",
        "    subj = d['metadata']['ID']\n",
        "    subj_label_counts[subj.numpy().decode('utf-8')] += 1\n",
        "\n",
        "  # Filter down to subjects with at least N samples.\n",
        "  allowed_subjs = [\n",
        "      subj for subj, count in subj_label_counts.items()\n",
        "      if count \u003e= dataset_configs.samples_per_subject\n",
        "  ]\n",
        "  filter_fn = functools.partial(\n",
        "      filter_allowed_subjects, allowed_subjects=allowed_subjs\n",
        "  )\n",
        "  ds = ds.filter(filter_fn)\n",
        "\n",
        "  # Split the data into train and val splits.\n",
        "  # Splits each participants data evenly between train and val.\n",
        "  def filter_by_subj(subj):\n",
        "    return lambda x: tf.equal(\n",
        "        x['metadata']['ID'], subj\n",
        "    )\n",
        "\n",
        "  train_subj_splits, valid_subj_splits = [], []\n",
        "  num_train_samples, num_val_samples = 0, 0\n",
        "  for subj in allowed_subjs:\n",
        "    subj_ds = ds.filter(filter_by_subj(subj))\n",
        "    size_subj_ds = sum(1 for _ in subj_ds)\n",
        "\n",
        "    train_size = int(0.8 * size_subj_ds)\n",
        "    num_train_samples += train_size\n",
        "    num_val_samples += size_subj_ds - train_size\n",
        "\n",
        "    subj_train_split = subj_ds.take(train_size)\n",
        "    subj_val_split = subj_ds.skip(train_size)\n",
        "\n",
        "    train_subj_splits.append(subj_train_split)\n",
        "    valid_subj_splits.append(subj_val_split)\n",
        "\n",
        "  # Concat class datasets.\n",
        "  train_ds = train_subj_splits[0]\n",
        "  val_ds = valid_subj_splits[0]\n",
        "  for i in range(1, len(allowed_subjs)):\n",
        "    train_ds = train_ds.concatenate(train_subj_splits[i])\n",
        "    val_ds = val_ds.concatenate(valid_subj_splits[i])\n",
        "\n",
        "  # Get samples per class\n",
        "  spc = collections.Counter()\n",
        "  for d in train_ds:\n",
        "    log_val = int(d['metadata']['log_value'])\n",
        "    spc[log_val] += 1\n",
        "\n",
        "  spc_labels = tf.convert_to_tensor(list(spc.keys()))\n",
        "  spc_label_counts = tf.convert_to_tensor(list(spc.values()))\n",
        "\n",
        "  # Get mood log values\n",
        "  dataset_key = used_dataset_name.split('/')[-1]\n",
        "  offset = tf.cast(\n",
        "      dataset_constants.lsm_dataset_constants[dataset_key]['log_value_offset'],\n",
        "      tf.int32\n",
        "  )\n",
        "  log_val_list = tf.convert_to_tensor(\n",
        "      dataset_constants.lsm_dataset_constants[dataset_key]['log_values']\n",
        "  )\n",
        "  log_val_list = log_val_list - offset  # offset value\n",
        "\n",
        "  sorted_indices_tensor2 = tf.argsort(spc_labels)\n",
        "  sorted_tensor2 = tf.gather(spc_labels, sorted_indices_tensor2)\n",
        "  matching_indices = tf.argsort(tf.argsort(log_val_list))\n",
        "  mapping = tf.gather(sorted_indices_tensor2, matching_indices)\n",
        "  label_counts = tf.gather(spc_label_counts, mapping)\n",
        "\n",
        "  # Only get useful data fields\n",
        "  train_ds = train_ds.map(get_useful_fields)\n",
        "  val_ds = val_ds.map(get_useful_fields)\n",
        "\n",
        "  return train_ds, val_ds\n",
        "\n",
        "\n",
        "def get_dataset2(\n",
        "    config: Any,\n",
        "    data_rng: jnp.ndarray,\n",
        "    *,\n",
        "    num_local_shards: Optional[int] = None,\n",
        "    dataset_service_address: Optional[str] = None,\n",
        "    **kwargs: Any,\n",
        ") -\u003e dataset_utils.Dataset:\n",
        "  \"\"\"Adapted from: google3/third_party/py/scenic/train_lib/train_utils.py.\"\"\"\n",
        "\n",
        "  # Get device count\n",
        "  device_count = jax.device_count()\n",
        "  logging.info('device_count: %d', device_count)\n",
        "  logging.info('num_hosts : %d', jax.process_count())\n",
        "  logging.info('host_id : %d', jax.process_index())\n",
        "\n",
        "  # Set the dataset builder functions\n",
        "  # Get list of supported, non-deprecated datasets.\n",
        "  dataset_name = config.dataset_configs.dataset\n",
        "  dataset_suported_list = ['lsm_300min_2000_mood_subject_dependant']\n",
        "  if dataset_name.split('/')[1] in dataset_suported_list:\n",
        "    dataset_builder = get_subject_dependent_mood_dataset\n",
        "  else:\n",
        "    raise ValueError(f'Dataset {dataset_name} is not supported.')\n",
        "\n",
        "  # Get batch size\n",
        "  batch_size = config.batch_size\n",
        "  if batch_size % device_count \u003e 0:\n",
        "    raise ValueError(\n",
        "        f'Batch size ({batch_size}) must be divisible by the '\n",
        "        f'number of devices ({device_count})'\n",
        "    )\n",
        "\n",
        "  local_batch_size = batch_size // jax.process_count()\n",
        "  device_batch_size = batch_size // device_count\n",
        "  logging.info('local_batch_size : %d', local_batch_size)\n",
        "  logging.info('device_batch_size : %d', device_batch_size)\n",
        "\n",
        "  # Get shuffle seed - ensure it exists\n",
        "  shuffle_seed = config.get('shuffle_seed', None)\n",
        "  if dataset_service_address and shuffle_seed is not None:\n",
        "    raise ValueError(\n",
        "        'Using dataset service with a random seed causes each '\n",
        "        'worker to produce exactly the same data. Add '\n",
        "        'config.shuffle_seed = None to your config if you want '\n",
        "        'to run with dataset service.'\n",
        "    )\n",
        "\n",
        "  # Get shuffle buffer size.\n",
        "  shuffle_buffer_size = config.get('shuffle_buffer_size', None)\n",
        "  # Local shard count.\n",
        "  num_local_shards = num_local_shards or jax.local_device_count()\n",
        "\n",
        "  # Build the dataset\n",
        "  train_ds, val_ds = dataset_builder(\n",
        "      config=config,\n",
        "      num_shards=num_local_shards,\n",
        "      batch_size=local_batch_size,\n",
        "      dtype_str=config.data_dtype_str,\n",
        "      shuffle_seed=shuffle_seed,\n",
        "      rng=data_rng,\n",
        "      shuffle_buffer_size=shuffle_buffer_size,\n",
        "      dataset_service_address=dataset_service_address,\n",
        "      **kwargs,\n",
        "  )\n",
        "\n",
        "  return train_ds, val_ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-51iB1rodVp"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = 'lsm_300min_2000_mood_subject_dependant'\n",
        "TRAIN_DATA_SIZE = None\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "config.dataset_configs.update({'samples_per_subject': 40, 'repeat': False})\n",
        "\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "\n",
        "start_t = time.time()\n",
        "dataset = get_dataset2(config, data_rng)\n",
        "train_ds, val_ds = dataset\n",
        "end_t = time.time()\n",
        "\n",
        "print('Dataset Time', end_t - start_t)\n",
        "\n",
        "# Apply serialization function to each example in the dataset\n",
        "serialized_train_ds = train_ds.map(tf_serialize_example)\n",
        "serialized_val_ds = val_ds.map(tf_serialize_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ubuwCD8zDBCR"
      },
      "outputs": [],
      "source": [
        "# @title Save Examples to TF Records\n",
        "\n",
        "data_dir='/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-electrodes/deid/exp/girishvn/ttl=6w/lsm_processed_datasets/'\n",
        "train_fname = 'processed_mood_2000_train.tfrecord'\n",
        "test_fname = 'processed_mood_2000_test.tfrecord'\n",
        "\n",
        "train_fpath = os.path.join(data_dir, train_fname)\n",
        "val_fpath = os.path.join(data_dir, test_fname)\n",
        "\n",
        "train_count = 0\n",
        "val_count = 0\n",
        "\n",
        "start_t = time.time()\n",
        "# Write out train data\n",
        "with tf.io.TFRecordWriter(train_fpath) as writer:\n",
        "  for serialized_example in serialized_train_ds:\n",
        "    writer.write(serialized_example.numpy())\n",
        "\n",
        "    train_count += 1\n",
        "    if train_count % 100 == 0:\n",
        "      print(f'Processed {train_count} examples in {time.time() - start_t} s.')\n",
        "\n",
        "end_t = time.time()\n",
        "print('Train Time', end_t - start_t)\n",
        "\n",
        "start_t = time.time()\n",
        "# Write out test data\n",
        "with tf.io.TFRecordWriter(val_fpath) as writer:\n",
        "  for serialized_example in serialized_val_ds:\n",
        "    writer.write(serialized_example.numpy())\n",
        "\n",
        "    val_count += 1\n",
        "    if val_count % 100 == 0:\n",
        "      print(f'Processed {val_count} examples in {time.time() - start_t} s.')\n",
        "\n",
        "end_t = time.time()\n",
        "print('Test Time', end_t - start_t)\n",
        "\n",
        "\n",
        "print('DONE DONE DONE')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BtjFRGHIYhGW"
      },
      "outputs": [],
      "source": [
        "# @title Example Counts\n",
        "print('Train Count', train_count)\n",
        "print('Test Count', val_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXytsMwssMxZ"
      },
      "source": [
        "### Load Preprocessed and Saved Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4UcifXQYqQWj"
      },
      "outputs": [],
      "source": [
        "# @title Data Loader to Load this Saved Data\n",
        "\n",
        "\"\"\"Electrodes dataset data preprocesser and loader.\n",
        "\n",
        "Adapted from a combination of the following files:\n",
        "google3/third_party/py/scenic/dataset_lib/cifar10_dataset.py\n",
        "google3/third_party/py/scenic/dataset_lib/dataset_utils.py\n",
        "\n",
        "NOTE: This dataset is a HACKY implementation of the\n",
        "lsm_mood_subj_dependent_dataset which specifically loads a preprpocessed dataset\n",
        "where subject dependent splits are already created, and where only subjects with\n",
        "40+ samples are included.\n",
        "\n",
        "This was created as part of the ICLR '25 Rebuttal for the LSM paper.\n",
        "This hacky implementation is necessary as lsm_mood_subj_dependent_dataset.py is\n",
        "made extremely slow on XM (causing idle failures) due to the need to traverse\n",
        "the dataset to create the subject dependent splits.\n",
        "\n",
        "If you are interested in using this dataset, please consider using the\n",
        "lsm_mood_subj_dependent_dataset.py instead, and / or re-implemting this.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import functools\n",
        "import os\n",
        "from typing import Any, Optional\n",
        "\n",
        "from absl import logging\n",
        "import jax.numpy as jnp\n",
        "import jax.profiler\n",
        "import ml_collections  # pylint: disable=unused-import\n",
        "from scenic.dataset_lib import dataset_utils\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from google3.experimental.largesensormodels.scenic.datasets import dataset_constants\n",
        "from google3.experimental.largesensormodels.scenic.datasets import lsm_tiny_dataset\n",
        "\n",
        "\n",
        "def preprocess_example(example, dataset_name, dtype=tf.float32):\n",
        "  \"\"\"Preprocesses the given example.\n",
        "\n",
        "  Adapted from /largesensormodels/scenic/datasets/lsm_tiny_dataset.py.\n",
        "  This function is modified to work with the pre-processed dataset, which has\n",
        "  slight differences from the original dataset (e.g. not have 'metadata' field).\n",
        "\n",
        "  Args:\n",
        "    example: dict; Example that has an 'image' and a 'label'.\n",
        "    dataset_name: str; Name of the dataset. This is used to extract the\n",
        "      datetime features.\n",
        "    dtype: Tensorflow data type; Data type of the image.\n",
        "\n",
        "  Returns:\n",
        "    A preprocessed example.\n",
        "\n",
        "  NOTE: This assumes that the image is in the shape [H, W, C],\n",
        "    where H is the Time axis, and W is the feature axis.\n",
        "  \"\"\"\n",
        "  dataset_name = dataset_name.split('/')[-1]\n",
        "  features = tf.cast(example['input_signal'], dtype=dtype)\n",
        "  time_features = dataset_constants.lsm_dataset_constants[dataset_name].get(\n",
        "      'datetime_features', None\n",
        "  )\n",
        "\n",
        "  if time_features is None:\n",
        "    raise ValueError(dataset_name)\n",
        "\n",
        "  # Split input into inputs and time-features\n",
        "  feature_indices = list(range(features.shape[1]))\n",
        "  if time_features is not None:\n",
        "\n",
        "    # Get the inidices of datetime_features,\n",
        "    # and split them from the indicies of other features.\n",
        "    time_feature_indices = list(time_features['indices'])\n",
        "    feature_indices = list(set(feature_indices) - set(time_features['indices']))\n",
        "    time_feature_indices = tf.convert_to_tensor(time_feature_indices)\n",
        "    feature_indices = tf.convert_to_tensor(feature_indices)\n",
        "\n",
        "    # Using the above indices, split the feature tensor.\n",
        "    time_features = tf.gather(features, time_feature_indices, axis=1)\n",
        "    features = tf.gather(features, feature_indices, axis=1)\n",
        "  else:\n",
        "    time_features = None\n",
        "\n",
        "  # Stress / Mood / Activity Labels:\n",
        "  # A) Binary label of stress (0/1).\n",
        "  stress_label = tf.cast(example['label'][0], dtype=tf.int32)  # pylint: disable=unused-variable\n",
        "  # B) Boolean logs (True/False) of an logged exercise or mood event.\n",
        "  # (exercise and mood events are mutally exclusive).\n",
        "  exercise_log = example['exercise_log'][0]\n",
        "  mood_log = example['mood_log'][0]\n",
        "  # C) The log value (int 64 log code) for an excercise or mood event.\n",
        "  # NOTE: that exercise and mood events DO NOT occur simultaneously\n",
        "  log_value = tf.cast(example['log_value'][0], tf.int32)\n",
        "\n",
        "  # Return preprocessed feature and desired labels.\n",
        "  # A) If activities or mood dataset: the log value is indexed [0, n classes],\n",
        "  # one-hot encoded, and returned as the label.\n",
        "  # a) offset value of log_value - an artifact of dataset creation.\n",
        "  log_value_offset = tf.cast(\n",
        "      dataset_constants.lsm_dataset_constants[dataset_name][\n",
        "          'log_value_offset'\n",
        "      ],\n",
        "      tf.int32\n",
        "  )\n",
        "  # b) list of possible labels (log_values) for a dataset.\n",
        "  log_value_label_list = tf.convert_to_tensor(\n",
        "      dataset_constants.lsm_dataset_constants[dataset_name]['log_values']\n",
        "  )\n",
        "  # c) offset log_value.\n",
        "  log_value_label_list = log_value_label_list - log_value_offset\n",
        "  n_classes = len(log_value_label_list)  # number of classes in label map\n",
        "  # d) generate label map.\n",
        "  lookup_initializer = tf.lookup.KeyValueTensorInitializer(\n",
        "      keys=log_value_label_list, values=tf.range(n_classes)\n",
        "  )\n",
        "  label_map = tf.lookup.StaticHashTable(lookup_initializer, default_value=-1)\n",
        "  # e) get label index from label map.\n",
        "  label_idx = label_map.lookup(log_value)\n",
        "  return {\n",
        "      'input_signal': features,\n",
        "      'datetime_signal': time_features,\n",
        "      'label': tf.one_hot(label_idx, n_classes),\n",
        "      'exercise_log': exercise_log,\n",
        "      'mood_log': mood_log,\n",
        "      'log_value': log_value,\n",
        "  }\n",
        "\n",
        "\n",
        "def parse_tfexample_fn(example):\n",
        "  \"\"\"Parses features from serialized tf example.\"\"\"\n",
        "  # The dataset has more labels than we use.\n",
        "  feature_spec = {\n",
        "      'input_signal': tf.io.FixedLenFeature(\n",
        "          shape=[300, 30, 1], dtype=tf.float32\n",
        "      ),\n",
        "      'label': tf.io.FixedLenFeature(\n",
        "          shape=1, dtype=tf.int64\n",
        "      ),\n",
        "      'exercise_log': tf.io.FixedLenFeature(\n",
        "          shape=1, dtype=tf.int64\n",
        "      ),\n",
        "      'mood_log': tf.io.FixedLenFeature(\n",
        "          shape=1, dtype=tf.int64\n",
        "      ),\n",
        "      'log_value': tf.io.FixedLenFeature(\n",
        "          shape=1, dtype=tf.int64\n",
        "      ),\n",
        "  }\n",
        "  parsed_example = tf.io.parse_single_example(example, feature_spec)\n",
        "  parsed_example['exercise_log'] = tf.cast(\n",
        "      parsed_example['exercise_log'], tf.bool\n",
        "  )\n",
        "  parsed_example['mood_log'] = tf.cast(parsed_example['mood_log'], tf.bool)\n",
        "  return parsed_example\n",
        "\n",
        "\n",
        "def update_metadata(\n",
        "    metadata, dataset_name, patch_size, dataset_configs\n",
        "):\n",
        "  \"\"\"Update metadata to reflect resizing and addition of datetime features.\"\"\"\n",
        "  # Setup: Get dataset name, feature shape, and possible datetime features.\n",
        "  metadata_update = dict()\n",
        "  dataset_name = dataset_name.split('/')[-1]\n",
        "  time_features = dataset_constants.lsm_dataset_constants[dataset_name].get(\n",
        "      'datetime_features', None\n",
        "  )\n",
        "  feature_shape = list(metadata['input_shape'][1:])\n",
        "  feature_indices = list(range(feature_shape[1]))\n",
        "\n",
        "  # Split features from time series features\n",
        "  # NOTE: This assumes that the original 'input_signal' field has sensor\n",
        "  # features contactanated to datetime features along the feature (w) dimension.\n",
        "  if time_features is not None:\n",
        "    # Get datetime indicies\n",
        "    time_feature_indices = list(time_features['indices'])\n",
        "    # Remove datetime indices from feature indices\n",
        "    feature_indices = list(set(feature_indices) - set(time_features['indices']))\n",
        "    # Get updated feature and datetime feature shapes.\n",
        "    time_feature_shape = feature_shape.copy()  # update time feature shape\n",
        "    time_feature_shape[1] = len(time_feature_indices)\n",
        "    feature_shape[1] = len(feature_indices)  # update feature shape\n",
        "  else:\n",
        "    time_feature_shape = None\n",
        "\n",
        "  # Padding: Update shape to reflect padding (for perfect patching).\n",
        "  # valid_feats arrays denote which features are valid (1) vs padded (0).\n",
        "  # 1. Update for sensor features\n",
        "  _, pad_w, feat_shape_new = lsm_tiny_dataset.get_height_crop_width_pad(\n",
        "      tuple(feature_shape), patch_size\n",
        "  )\n",
        "  valid_feat_mask = [0] * pad_w[0] + [1] * feature_shape[1] + [0] * pad_w[1]\n",
        "  metadata_update['input_shape'] = tuple([-1] + list(feat_shape_new))\n",
        "  metadata_update['input_valid_feats'] = tuple(valid_feat_mask)\n",
        "\n",
        "  # 2. Update for datetime features\n",
        "  if time_features is not None:\n",
        "    _, time_pad_w, time_feature_shape_new = (\n",
        "        lsm_tiny_dataset.get_height_crop_width_pad(\n",
        "            tuple(time_feature_shape), patch_size\n",
        "        )\n",
        "    )\n",
        "    valid_time_feat_mask = (\n",
        "        [0] * time_pad_w[0] + [1] * time_feature_shape[1] + [0] * time_pad_w[1]\n",
        "    )\n",
        "    metadata_update['datetime_input_shape'] = tuple(\n",
        "        [-1] + list(time_feature_shape_new)\n",
        "    )\n",
        "    metadata_update['datime_valid_feats'] = tuple(valid_time_feat_mask)\n",
        "\n",
        "  else:\n",
        "    metadata_update['datetime_input_shape'] = None\n",
        "    metadata_update['datime_valid_feats'] = None\n",
        "\n",
        "  # Update if dataset it one-hot-encoded or not.\n",
        "  metadata_update['target_is_onehot'] = True\n",
        "  metadata_update['num_classes'] = len(\n",
        "      dataset_constants.lsm_dataset_constants[dataset_name]['log_values']\n",
        "  )\n",
        "\n",
        "  # 4. Add dataset log values and log value names and number of classes.\n",
        "  log_values = dataset_constants.lsm_dataset_constants[dataset_name].get(\n",
        "      'log_values', None\n",
        "  )\n",
        "  log_value_names = dataset_constants.lsm_dataset_constants[dataset_name].get(\n",
        "      'log_value_names', None\n",
        "  )\n",
        "  metadata_update['log_values'] = log_values\n",
        "  metadata_update['log_value_names'] = log_value_names\n",
        "\n",
        "  # 7. Update time cropping:\n",
        "  start, end = dataset_configs.get('relative_time_window', (None, None))\n",
        "  if end is None:\n",
        "    end = 1\n",
        "  if start is None:\n",
        "    start = 0\n",
        "\n",
        "  # Time Crop image based on horizon.\n",
        "  # Get number of patches along time axis (h).\n",
        "  p_h = patch_size[0]\n",
        "  h = feat_shape_new[0]\n",
        "  n_h = h // p_h\n",
        "  start_idx = int(start * n_h) * p_h\n",
        "  end_idx = int(end * n_h) * p_h\n",
        "  metadata_update['input_shape'] = tuple(\n",
        "      [-1] + [end_idx - start_idx] + list(feat_shape_new)[1:]\n",
        "  )\n",
        "\n",
        "  return metadata_update\n",
        "\n",
        "\n",
        "def get_subject_dependent_mood_dataset(\n",
        "    *,\n",
        "    config,\n",
        "    num_shards,\n",
        "    batch_size,\n",
        "    eval_batch_size=None,\n",
        "    dtype_str='float32',\n",
        "    shuffle_seed=0,\n",
        "    rng=None,\n",
        "    shuffle_buffer_size=None,\n",
        "    dataset_service_address: Optional[str] = None,\n",
        "    data_dir='/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-electrodes/deid/exp/dmcduff/ttl=6w/msa_1_5/lsm_tfds_datasets',\n",
        "):\n",
        "  \"\"\"Gets and formats the Subject Dependent Mood Dataset.\n",
        "\n",
        "  Adapted from:\n",
        "  google3/third_party/py/scenic/dataset_lib/cifar10_dataset.py and\n",
        "  google3/third_party/py/scenic/dataset_lib/dataset_utils.py.\n",
        "\n",
        "  Args:\n",
        "    config: ml_collections.ConfigDict; Config for the experiment.\n",
        "    num_shards: int; Number of shards to split the dataset into.\n",
        "    batch_size: int; Batch size for training.\n",
        "    eval_batch_size: int; Batch size for evaluation.\n",
        "    dtype_str: str; Data type of the image.\n",
        "    shuffle_seed: int; Seed for shuffling the dataset.\n",
        "    rng: jax.random.PRNGKey; Random number generator key.\n",
        "    shuffle_buffer_size: int; Size of the shuffle buffer.\n",
        "    dataset_service_address: str; Address of the dataset service.\n",
        "    data_dir: str; Directory of the dataset.\n",
        "\n",
        "  Returns:\n",
        "    A dataset_utils.Dataset object.\n",
        "  \"\"\"\n",
        "\n",
        "  # START HARDCODED SECTION\n",
        "  # As explained in the file header: this is hardcoded to read from a\n",
        "  # pre-processed version of the subject dependent mood dataset.\n",
        "\n",
        "  processed_data_dir = (\n",
        "      '/namespace/fitbit-medical-sandboxes/partner/encrypted/'\n",
        "      'chr-ards-electrodes/deid/exp/girishvn/ttl=6w/lsm_processed_datasets/'\n",
        "  )\n",
        "  train_fname = 'processed_mood_2000_train.tfrecord'\n",
        "  test_fname = 'processed_mood_2000_test.tfrecord'\n",
        "\n",
        "  # Reference dataset name, used to query for dataset_constants.\n",
        "  used_dataset_name = 'lsm_prod/lsm_300min_2000_mood_balanced'\n",
        "\n",
        "  # Pre-computed label counts per class.\n",
        "  label_counts = [739, 472, 434, 787, 1138]\n",
        "\n",
        "  # Pre-computed train and test sample counts.\n",
        "  num_train_samples = 3570\n",
        "  num_val_samples = 912\n",
        "  # END HARDCODED SECTION\n",
        "\n",
        "  train_fpath = os.path.join(processed_data_dir, train_fname)\n",
        "  val_fpath = os.path.join(processed_data_dir, test_fname)\n",
        "\n",
        "  # Setup: General\n",
        "  if rng is None:\n",
        "    rng = jax.random.PRNGKey(config.rng_seed)\n",
        "\n",
        "  # 1. Process information.\n",
        "  p_idx = jax.process_index()  # current process index\n",
        "  p_cnt = jax.process_count()  # process count (number of processes)\n",
        "\n",
        "  aug_rngs = jax.random.split(rng, p_cnt)  # per-device augmentation seeds\n",
        "  aug_rng = aug_rngs[p_idx]  # device augmentation seed\n",
        "  tf_aug_rng = aug_rng[0]  # jax random seeds are arrays, tf expects an int.\n",
        "  del rng\n",
        "\n",
        "  # 2. dataset and data type information.\n",
        "  dataset_configs = config.dataset_configs  # get dataset configurations.\n",
        "  dtype = getattr(tf, dtype_str)  # data dtype\n",
        "  if eval_batch_size is None:  # set eval batch size\n",
        "    eval_batch_size = batch_size\n",
        "\n",
        "  # 4. Repeat dataset.\n",
        "  repeat_ds = dataset_configs.get('repeat_data', True)\n",
        "\n",
        "  # Setup: Mapping functions.\n",
        "  # 2. Preprocessing, augmentation, and cropping/padding functions.\n",
        "  preprocess_fn = functools.partial(\n",
        "      preprocess_example,\n",
        "      dataset_name=used_dataset_name,\n",
        "      dtype=dtype\n",
        "  )\n",
        "  # 3. Augmentation function.\n",
        "  augment_fn = functools.partial(\n",
        "      lsm_tiny_dataset.augment_example,\n",
        "      augmentations=config.get('train_augmentations', []),\n",
        "      seed=tf_aug_rng,\n",
        "  )\n",
        "  # 4. Crop and pad features and time features to be patch size compatible.\n",
        "  crop_and_pad_fn = functools.partial(\n",
        "      lsm_tiny_dataset.patch_compatible_resize_example,\n",
        "      patch_size=config.model.patches.size\n",
        "  )\n",
        "\n",
        "  # 5. Time crop data input\n",
        "  start, end = dataset_configs.get('relative_time_window', (None, None))\n",
        "  if (start is not None) or (end is not None):\n",
        "    time_crop_examples = True\n",
        "  else:\n",
        "    time_crop_examples = False\n",
        "  time_crop_fn = functools.partial(\n",
        "      lsm_tiny_dataset.time_crop_example,\n",
        "      patch_size=config.model.patches.size,\n",
        "      start=start,\n",
        "      end=end\n",
        "  )\n",
        "\n",
        "  # Setup: Data splits.\n",
        "  # Load dataset splits.\n",
        "  train_ds = tf.data.TFRecordDataset(train_fpath)\n",
        "  val_ds = tf.data.TFRecordDataset(val_fpath)\n",
        "  train_ds = train_ds.map(parse_tfexample_fn)\n",
        "  val_ds = val_ds.map(parse_tfexample_fn)\n",
        "\n",
        "  # Data processing and preperation.\n",
        "  # 0. Enable multi threaded workers.\n",
        "  options = tf.data.Options()\n",
        "  options.threading.private_threadpool_size = 48\n",
        "  train_ds = train_ds.with_options(options)\n",
        "  val_ds = val_ds.with_options(options)\n",
        "\n",
        "  # Split dataset over host devices.\n",
        "  train_ds = train_ds.shard(p_cnt, p_idx)\n",
        "  val_ds = val_ds.shard(p_cnt, p_idx)\n",
        "\n",
        "  # 2. Preprocessing: Applied before `ds.cache()` to re-use it.\n",
        "  train_ds = train_ds.map(\n",
        "      preprocess_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "  val_ds = val_ds.map(\n",
        "      preprocess_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "\n",
        "  # 3. Cache datasets: This can signficantly speed up training.\n",
        "  if dataset_configs.cache_dataset:\n",
        "    train_ds = train_ds.cache()\n",
        "    val_ds = val_ds.cache()\n",
        "\n",
        "  # 4 Train repeats and augmentations.\n",
        "  if repeat_ds:\n",
        "    train_ds = train_ds.repeat()  # repeat\n",
        "  # NOTE: Train augmentations are done after repeat for true randomness.\n",
        "  if config.use_train_augmentations:\n",
        "    train_ds = train_ds.map(  # train data augmentations\n",
        "        augment_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "    )\n",
        "\n",
        "  # 5. Crop and pad for perfect patching.\n",
        "  train_ds = train_ds.map(  # crop/pad for perfect patching\n",
        "      crop_and_pad_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "  val_ds = val_ds.map(  # crop/pad for perfect patching\n",
        "      crop_and_pad_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "\n",
        "  # 6. Time crop input data.\n",
        "  if time_crop_examples:\n",
        "    train_ds = train_ds.map(\n",
        "        time_crop_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "    )\n",
        "    val_ds = val_ds.map(\n",
        "        time_crop_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "    )\n",
        "\n",
        "  # 7. Data preperation (shuffling, augmentations, batching, eval repeat, etc.).\n",
        "  # 7a. Train: Shuffle, batch, prefetch\n",
        "  shuffle_buffer_size = shuffle_buffer_size or (8 * batch_size)\n",
        "  train_ds = train_ds.shuffle(shuffle_buffer_size, seed=shuffle_seed)  # shuffle\n",
        "  train_ds = train_ds.batch(batch_size, drop_remainder=True)  # batch\n",
        "  train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)  # prefetch\n",
        "\n",
        "  # 7b. Validation: Batch, Repeat, Prefetch\n",
        "  val_ds = val_ds.batch(batch_size, drop_remainder=False)  # batch\n",
        "  if repeat_ds:\n",
        "    val_ds = val_ds.repeat()  # repeat\n",
        "  val_ds = val_ds.prefetch(tf.data.experimental.AUTOTUNE)  # prefetch\n",
        "\n",
        "  # Ensure that no seed is set if dataset_service_address is defined.\n",
        "  if dataset_service_address:\n",
        "    if shuffle_seed is not None:\n",
        "      raise ValueError(\n",
        "          'Using dataset service with a random seed causes each '\n",
        "          'worker to produce exactly the same data. Add '\n",
        "          'config.shuffle_seed = None to your config if you '\n",
        "          'want to run with dataset service.'\n",
        "      )\n",
        "    train_ds = dataset_utils.distribute(train_ds, dataset_service_address)\n",
        "    logging.info('Using the tf.data service at %s', dataset_service_address)\n",
        "\n",
        "  # Other mappings\n",
        "  # 1. Set up batch padding: If batch remainders are NOT dropped batches may be\n",
        "  # padded to allow for an enough patches to contain all samples.\n",
        "  maybe_pad_batches_train = functools.partial(\n",
        "      dataset_utils.maybe_pad_batch,\n",
        "      train=True,\n",
        "      batch_size=batch_size,\n",
        "      inputs_key='input_signal',\n",
        "  )\n",
        "  maybe_pad_batches_eval = functools.partial(\n",
        "      dataset_utils.maybe_pad_batch,\n",
        "      train=False,\n",
        "      batch_size=eval_batch_size,\n",
        "      inputs_key='input_signal',\n",
        "  )\n",
        "\n",
        "  # 2. Set up batch sharding: Shard batches to be processed by multiple devices.\n",
        "  shard_batches = functools.partial(dataset_utils.shard, n_devices=num_shards)\n",
        "\n",
        "  # 3. Apply other mappings and Iter dataset\n",
        "  train_iter = iter(train_ds)\n",
        "  train_iter = map(dataset_utils.tf_to_numpy, train_iter)\n",
        "  train_iter = map(maybe_pad_batches_train, train_iter)\n",
        "  train_iter = map(shard_batches, train_iter)\n",
        "\n",
        "  val_iter = iter(val_ds)\n",
        "  val_iter = map(dataset_utils.tf_to_numpy, val_iter)\n",
        "  val_iter = map(maybe_pad_batches_eval, val_iter)\n",
        "  val_iter = map(shard_batches, val_iter)\n",
        "\n",
        "  # Save meta data\n",
        "  info = tfds.builder(used_dataset_name, data_dir=data_dir, try_gcs=True).info\n",
        "  input_shape = tuple([-1] + list(info.features['input_signal'].shape))\n",
        "  meta_data = {\n",
        "      'input_shape': input_shape,\n",
        "      'num_train_examples': num_train_samples,\n",
        "      'num_val_examples': num_val_samples,\n",
        "      'num_test_examples': 0,\n",
        "      'input_dtype': getattr(jnp, dtype_str),\n",
        "      'label_counts': label_counts,\n",
        "      # The following two fields are set as defaults and may be updated in the\n",
        "      # update_metadata function below.\n",
        "      'target_is_onehot': False,\n",
        "      'num_classes': None,\n",
        "  }\n",
        "\n",
        "  # Update metadata to reflect preprocessing, and paddings\n",
        "  # (Changes in shape, and features).\n",
        "  meta_data.update(\n",
        "      update_metadata(\n",
        "          meta_data,\n",
        "          dataset_name=used_dataset_name,\n",
        "          patch_size=config.model.patches.size,\n",
        "          dataset_configs=dataset_configs,\n",
        "      )\n",
        "  )\n",
        "\n",
        "  # Return dataset structure.\n",
        "  return dataset_utils.Dataset(train_iter, val_iter, None, meta_data)\n",
        "\n",
        "\n",
        "def get_preprocessed_dataset(\n",
        "    config: Any,\n",
        "    data_rng: jnp.ndarray,\n",
        "    *,\n",
        "    num_local_shards: Optional[int] = None,\n",
        "    dataset_service_address: Optional[str] = None,\n",
        "    **kwargs: Any,\n",
        ") -\u003e dataset_utils.Dataset:\n",
        "  \"\"\"Adapted from: google3/third_party/py/scenic/train_lib/train_utils.py.\"\"\"\n",
        "\n",
        "  # Get device count\n",
        "  device_count = jax.device_count()\n",
        "  logging.info('device_count: %d', device_count)\n",
        "  logging.info('num_hosts : %d', jax.process_count())\n",
        "  logging.info('host_id : %d', jax.process_index())\n",
        "\n",
        "  # Set the dataset builder functions\n",
        "  # Get list of supported, non-deprecated datasets.\n",
        "  dataset_name = config.dataset_configs.dataset\n",
        "  dataset_suported_list = [\n",
        "      'lsm_300min_2000_mood_subject_dependent_preprocessed_40spc'\n",
        "  ]\n",
        "  if dataset_name.split('/')[1] in dataset_suported_list:\n",
        "    dataset_builder = get_subject_dependent_mood_dataset\n",
        "  else:\n",
        "    raise ValueError(f'Dataset {dataset_name} is not supported.')\n",
        "\n",
        "  # Get batch size\n",
        "  batch_size = config.batch_size\n",
        "  if batch_size % device_count \u003e 0:\n",
        "    raise ValueError(\n",
        "        f'Batch size ({batch_size}) must be divisible by the '\n",
        "        f'number of devices ({device_count})'\n",
        "    )\n",
        "\n",
        "  local_batch_size = batch_size // jax.process_count()\n",
        "  device_batch_size = batch_size // device_count\n",
        "  logging.info('local_batch_size : %d', local_batch_size)\n",
        "  logging.info('device_batch_size : %d', device_batch_size)\n",
        "\n",
        "  # Get shuffle seed - ensure it exists\n",
        "  shuffle_seed = config.get('shuffle_seed', None)\n",
        "  if dataset_service_address and shuffle_seed is not None:\n",
        "    raise ValueError(\n",
        "        'Using dataset service with a random seed causes each '\n",
        "        'worker to produce exactly the same data. Add '\n",
        "        'config.shuffle_seed = None to your config if you want '\n",
        "        'to run with dataset service.'\n",
        "    )\n",
        "\n",
        "  # Get shuffle buffer size.\n",
        "  shuffle_buffer_size = config.get('shuffle_buffer_size', None)\n",
        "  # Local shard count.\n",
        "  num_local_shards = num_local_shards or jax.local_device_count()\n",
        "\n",
        "  # Build the dataset\n",
        "  ds = dataset_builder(\n",
        "      config=config,\n",
        "      num_shards=num_local_shards,\n",
        "      batch_size=local_batch_size,\n",
        "      dtype_str=config.data_dtype_str,\n",
        "      shuffle_seed=shuffle_seed,\n",
        "      rng=data_rng,\n",
        "      shuffle_buffer_size=shuffle_buffer_size,\n",
        "      dataset_service_address=dataset_service_address,\n",
        "      **kwargs,\n",
        "  )\n",
        "\n",
        "  return ds\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAyBywlupZGI"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = 'lsm_300min_2000_mood_subject_dependent_preprocessed_40spc'\n",
        "TRAIN_DATA_SIZE = None\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "config = get_config(runlocal=False)  # must be false to get full dataset\n",
        "config.dataset_configs.update({'samples_per_subject': 40, 'repeat': False})\n",
        "\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "\n",
        "start_t = time.time()\n",
        "dataset = get_preprocessed_dataset(config, data_rng)\n",
        "end_t = time.time()\n",
        "\n",
        "print('Dataset Time', end_t - start_t)\n",
        "\n",
        "print('\\nProcessed Dataset Meta Data:\\n')\n",
        "for k in dataset.meta_data.keys():\n",
        "  print(k, dataset.meta_data[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BSIyKt9QpoDw"
      },
      "outputs": [],
      "source": [
        "# @title Train Sample  Breakdown\n",
        "\n",
        "state_t = time.time()\n",
        "\n",
        "label_list = []\n",
        "batch_count = 0\n",
        "for d in dataset.train_iter:\n",
        "  if batch_count % 1000 == 0:\n",
        "    print(batch_count, time.time())\n",
        "  batch_count += 1\n",
        "\n",
        "  bmask = d['batch_mask']\n",
        "  valid = np.where(bmask == 1)\n",
        "  log_vals = d['log_value'][valid]\n",
        "  log_vals = log_vals.tolist()\n",
        "  label_list += log_vals\n",
        "\n",
        "end_t = time.time()\n",
        "\n",
        "print('Time', end_t - state_t)\n",
        "print('\\nTrain Data Splits:')\n",
        "mood_counter = Counter()\n",
        "for l in label_list:\n",
        "  mood_counter[l] += 1\n",
        "\n",
        "for k in mood_counter.keys():\n",
        "  print(k, mood_counter[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "v0cmsF9-utS1"
      },
      "outputs": [],
      "source": [
        "# @title Valid Sample  Breakdown\n",
        "\n",
        "state_t = time.time()\n",
        "\n",
        "label_list = []\n",
        "batch_count = 0\n",
        "for d in dataset.valid_iter:\n",
        "  if batch_count % 1000 == 0:\n",
        "    print(batch_count, time.time())\n",
        "  batch_count += 1\n",
        "\n",
        "  bmask = d['batch_mask']\n",
        "  valid = np.where(bmask == 1)\n",
        "  log_vals = d['log_value'][valid]\n",
        "  log_vals = log_vals.tolist()\n",
        "  label_list += log_vals\n",
        "\n",
        "end_t = time.time()\n",
        "\n",
        "print('Time', end_t - state_t)\n",
        "print('\\nValid Data Splits:')\n",
        "mood_counter = Counter()\n",
        "for l in label_list:\n",
        "  mood_counter[l] += 1\n",
        "\n",
        "for k in mood_counter.keys():\n",
        "  print(k, mood_counter[k])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-GZBYUXeEUbg",
        "4OuvkeLkiEWK",
        "bXrToNgQDzlF",
        "C_uucoFoifDL",
        "9gqoOzzWDgr5",
        "tUrADbk2Ds-l",
        "4JWx0H7UHOCQ",
        "btKf1SMvhCcT",
        "-8oZvE5JlKXq",
        "PgvmZKi7krLM",
        "fQCx12qybWUo",
        "EI4nFG-CiNN1",
        "D-DHXGVax1U7",
        "Ah-dnDCkKoz8",
        "hDZ-hxFslcDS",
        "EJ7kv7jFiSai",
        "pXytsMwssMxZ"
      ],
      "last_runtime": {
        "build_target": "//fitbit/research/sensing/electrodes/colab:rl_colab",
        "kind": "shared"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
