{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNLa2VjaS7At"
      },
      "source": [
        "https://grants.corp.google.com/#/grants?request=20h%2Fchr-ards-dwb-deid-eng-policy:r\u0026reason=b%2F264556558%20-%20DWB%20RQ%20and%20Analysis\n",
        "\n",
        "https://grants.corp.google.com/#/grants?request=20h%2Fchr-ards-dwb-deid-colab-jobs\u0026reason=b%2F264556558%20-%20DWB%20RQ%20and%20Analysis\n",
        "\n",
        "https://pantheon.corp.google.com/storage/browser/health-studies-digital-wellbeing-export/processed_data_files_for_ari?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3MuXBCnCFKm"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e8d1tzwqK46"
      },
      "outputs": [],
      "source": [
        "from absl import app\n",
        "import apache_beam as beam\n",
        "from ast import literal_eval\n",
        "from collections.abc import Sequence\n",
        "import csv\n",
        "import datetime\n",
        "import fnmatch\n",
        "from google3.pyglib import gfile  # This is repeated, you might want to remove one\n",
        "from colabtools import googlefiles\n",
        "import json\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.dates as mdates\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib.patches import Patch, Rectangle\n",
        "import matplotlib.pyplot as plt\n",
        "import multiprocessing\n",
        "import multiprocessing.pool\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "import pdb\n",
        "import random  # This is repeated, you might want to remove one\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import shapiro\n",
        "from scipy.stats import zscore\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.graphics.gofplots import qqplot\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from time import sleep, time\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "NORMALIZATION_PARAMETERS = {\n",
        "    'HR': [82.406911, 13.9461201],\n",
        "    'eda_level_real': [4.116634, 3.878952961],\n",
        "    'leads_contact_counts': [230.76297, 52.76303698],\n",
        "    'steps': [7.952935, 18.53001124],\n",
        "    'jerk_auto': [203.441044, 33.11101136],\n",
        "    'step_count': [11.440943, 15.95296346],\n",
        "    'log_energy': [60.306033, 42.84693899],\n",
        "    'covariance': [44.81157, 12.63844836],\n",
        "    'log_energy_ratio': [44.714925, 21.32527317],\n",
        "    'zero_crossing_std': [160.085565, 28.10161215],\n",
        "    'zero_crossing_avg': [51.270075, 34.04430198],\n",
        "    'axis_mean': [119.768427, 23.58453469],\n",
        "    'altim_std': [0.005178, 0.0581546286],\n",
        "    'kurtosis': [108.645938, 60.38419486],\n",
        "    'sleep_coefficient': [8.706734, 4.003582277],\n",
        "    'wrist_temperatures': [30.921362, 2.817617692],\n",
        "    'hrv_shannon_entropy_rr': [3.277522, 0.468409277],\n",
        "    'hrv_shannon_entropy_rrd': [2.974838, 0.4999503109],\n",
        "    'hrv_percentage_of_nn_30': [0.348379, 0.1961256813],\n",
        "    'ceda_magnitude_real_micro_siemens': [43.071381, 24.11546345],\n",
        "    'ceda_slope_real_micro_siemens': [3.294176, 1.828755314],\n",
        "    'rmssd_percentile_0595': [34.038394, 24.86136018],\n",
        "    'sdnn_percentile_0595': [44.233053, 25.04521794],\n",
        "    'msa_probability': [48.120677, 14.23343678],\n",
        "    'hrv_percent_good': [0.2716, 0.2760073968],\n",
        "    'hrv_rr_80th_percentile_mean': [821.738396, 105.621134],\n",
        "    'hrv_rr_20th_percentile_mean': [731.996986, 84.6384433],\n",
        "    'hrv_rr_median': [776.111350, 90.3199562],\n",
        "    'hrv_rr_mean': [781.280325, 87.08971004],\n",
        "    'hr_at_rest_mean': [83.199721, 10.66796299],\n",
        "    'skin_temperature_magnitude': [26.393339, 10.98900771],\n",
        "    'skin_temperature_slope': [0.267523, 17.79474941],\n",
        "}\n",
        "\n",
        "FEATURES_TO_INCLUDE = [\n",
        "    'HR',\n",
        "    'eda_level_real',\n",
        "    'leads_contact_counts',\n",
        "    'steps',\n",
        "    'jerk_auto',\n",
        "    'step_count',\n",
        "    'log_energy',\n",
        "    'covariance',\n",
        "    'log_energy_ratio',\n",
        "    'zero_crossing_std',\n",
        "    'zero_crossing_avg',\n",
        "    'axis_mean',\n",
        "    'altim_std',\n",
        "    'kurtosis',\n",
        "    'sleep_coefficient',\n",
        "    'wrist_temperatures',\n",
        "    'hrv_shannon_entropy_rr',\n",
        "    'hrv_shannon_entropy_rrd',\n",
        "    'hrv_percentage_of_nn_30',\n",
        "    'ceda_magnitude_real_micro_siemens',\n",
        "    'ceda_slope_real_micro_siemens',\n",
        "    'rmssd_percentile_0595',\n",
        "    'sdnn_percentile_0595',\n",
        "    'msa_probability',\n",
        "    'hrv_percent_good',\n",
        "    'hrv_rr_80th_percentile_mean',\n",
        "    'hrv_rr_20th_percentile_mean',\n",
        "    'hrv_rr_median',\n",
        "    'hrv_rr_mean',\n",
        "    'hr_at_rest_mean',\n",
        "    'skin_temperature_magnitude',\n",
        "    'skin_temperature_slope'\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vodaFlKuVNj-"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "\n",
        "data.append({'type':  'steps',\n",
        "     'raw_file': 'STEPS_COMPACT_DATA',\n",
        "     'features_to_extract': ['steps'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "data.append({'type':  'momentary_stress_algorithm',\n",
        "     'raw_file': 'MOMENTARY_STRESS_ALGORITHM_DATA',\n",
        "     'features_to_extract': ['hrv_shannon_entropy_rr','hrv_shannon_entropy_rrd','hrv_percentage_of_nn_30','ceda_magnitude_real_micro_siemens','ceda_slope_real_micro_siemens','rmssd_percentile_0595','sdnn_percentile_0595','msa_probability','hrv_percent_good','hrv_rr_80th_percentile_mean','hrv_rr_20th_percentile_mean','hrv_rr_median','hrv_rr_mean','hr_at_rest_mean','skin_temperature_magnitude','skin_temperature_slope'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "data.append({'type':  'ceda',\n",
        "     'raw_file': 'CONTINUOUS_EDA_DATA',\n",
        "     'features_to_extract': ['eda_level_real','leads_contact_counts'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "data.append({'type':  'wrist_temperature',\n",
        "     'raw_file': 'WRIST_TEMPERATURE_DATA',\n",
        "     'features_to_extract': ['wrist_temperatures'],\n",
        "     'timezone_offset_column': 'tz_offset_minutes'})\n",
        "\n",
        "data.append({'type':  'sleep_coefficient',\n",
        "     'raw_file': 'SLEEP_COEFFICIENT_COMPACT_DATA',\n",
        "     'features_to_extract': ['sleep_coefficient','is_on_wrist'],\n",
        "     'timezone_offset_column': 'tz_offset_minutes'})\n",
        "\n",
        "data.append({'type':  'spo2',\n",
        "     'raw_file': 'ABSOLUTE_SPO2_DATA',\n",
        "     'features_to_extract': ['value','confidence','coverage','valid'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "data.append({'type':  'grok',\n",
        "     'raw_file': 'GROK_FEATURE_DATA',\n",
        "     'features_to_extract': ['jerk_auto','step_count','log_energy','covariance',\n",
        "                             'log_energy_ratio','zero_crossing_std',\n",
        "                             'zero_crossing_avg','axis_mean','altim_std','kurtosis'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "data.append({'type':  'heart_rate',\n",
        "     'raw_file': 'HEART_RATE_DATA',\n",
        "     'features_to_extract': ['bpm','confidence'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bttHwlawCHfG"
      },
      "outputs": [],
      "source": [
        "def visualize_features(array_feature):\n",
        "\n",
        "  fig = plt.figure(figsize=(20, 7))\n",
        "  ax1 = plt.subplot2grid((1, 12), (0, 0), colspan=12)\n",
        "  group = array_feature.numpy()\n",
        "\n",
        "  ax1 = sns.heatmap(group.T, cmap=\"Reds\", cbar=True, linewidths=0.0,\n",
        "                    linecolor='black', alpha=0.8, ax=ax1, yticklabels=True)\n",
        "\n",
        "  for tick in ax1.get_xticklabels():\n",
        "      tick.set_fontname('Ubuntu')\n",
        "      tick.set_style('italic')\n",
        "  ax1.tick_params(axis='x', labelsize=10.5)\n",
        "\n",
        "  for tick in ax1.get_yticklabels():\n",
        "      tick.set_fontname('Ubuntu')\n",
        "  ax1.tick_params(axis='y', labelsize=10.5)\n",
        "\n",
        "  plt.xticks(rotation=45)  # Rotate labels for better readability\n",
        "  plt.tight_layout()\n",
        "\n",
        "  ax1.set_ylabel(\"Feature\", fontname='Ubuntu', fontsize=14)\n",
        "\n",
        "  ax1.axhline(y=0, color='k',linewidth=1, alpha=1)\n",
        "  ax1.axhline(y=group.shape[1], color='k', alpha=1,linewidth=1)\n",
        "  ax1.axvline(x=0, color='k',linewidth=1, alpha=1)\n",
        "  ax1.axvline(x=group.shape[0], color='k', alpha=1,linewidth=1);\n",
        "\n",
        "  for i in np.arange(0,group.shape[0],60):\n",
        "    ax1.axvline(x=i, color='k', alpha=0.4,linewidth=1);\n",
        "  for i in np.arange(0,group.shape[1],1):\n",
        "    ax1.axhline(y=i, color='k', alpha=0.4,linewidth=1);\n",
        "\n",
        "  fig.savefig(f'example_heatmap.pdf', format='pdf', bbox_inches=\"tight\")\n",
        "  %download_file example_heatmap.pdf\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90_V1P0oh_7Y"
      },
      "outputs": [],
      "source": [
        "tf_record_folder = \"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_tfrecords_v6\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUtmxWeJyCb3"
      },
      "source": [
        "# STEP 0: Move Data from Selected Snapshot:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP_N0DKA3LTT"
      },
      "source": [
        "## Load First Time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjkLmK34oBse"
      },
      "outputs": [],
      "source": [
        "gfile.MakeDirs(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPzVyh43qNXf"
      },
      "outputs": [],
      "source": [
        "## Import survey and phone data from vico files:\n",
        "vico_folder = pathlib.PurePosixPath('/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/raw/vico/dwb')\n",
        "vico_files = gfile.ListDir(vico_folder)\n",
        "with gfile.Open(vico_folder / vico_files[-10], 'rb') as f:\n",
        "  members = tarfile.open(fileobj=f, mode='r:gz').getmembers()\n",
        "survey_phone_data = {}\n",
        "with gfile.Open(vico_folder / vico_files[-1], 'rb') as f:\n",
        "  with tarfile.open(fileobj=f, mode='r:gz') as tf:\n",
        "    members = tf.getmembers()\n",
        "    for member in members:\n",
        "      survey_phone_data[member.name] = (pd.read_csv(tf.extractfile(member), delimiter='\\t'))\n",
        "      with googlefiles.OpenGoogleFiles():\n",
        "        with open('/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/x_vico_'+member.name, 'w') as fs:\n",
        "          survey_phone_data[member.name].to_csv(fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvPIMowKZnGV"
      },
      "outputs": [],
      "source": [
        "fitbit_folder = pathlib.PurePosixPath('/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/raw/fitbit/dwb')\n",
        "fitbit_snapshots = gfile.ListDir(fitbit_folder)\n",
        "fitbit_files = gfile.ListDir(fitbit_folder / fitbit_snapshots[-1])\n",
        "\n",
        "fitbit_content = {}\n",
        "for data_type in fitbit_files:\n",
        "  with gfile.Open(fitbit_folder / fitbit_snapshots[-1] / data_type, 'r') as f:\n",
        "      fitbit_content[data_type] = pd.read_csv(f)\n",
        "      with googlefiles.OpenGoogleFiles():\n",
        "        with open('/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/'+data_type+'.csv', 'w') as fs:\n",
        "          fitbit_content[data_type].to_csv(fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px8PoTte4dEE"
      },
      "outputs": [],
      "source": [
        "data_type = 'HEART_RATE_DATA.csv'\n",
        "fitbit_folder = pathlib.PurePosixPath('/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/raw/fitbit/dwb')\n",
        "fitbit_snapshots = gfile.ListDir(fitbit_folder)\n",
        "fitbit_content = {}\n",
        "print(fitbit_snapshots[-140])\n",
        "gfile.Copy(fitbit_folder / fitbit_snapshots[-140] / data_type,'/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/'+data_type+'2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjQ1wBKFEAig"
      },
      "outputs": [],
      "source": [
        "data_type = 'CONTINUOUS_EDA_DATA.csv'\n",
        "fitbit_folder = pathlib.PurePosixPath('/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/raw/fitbit/dwb')\n",
        "fitbit_snapshots = gfile.ListDir(fitbit_folder)\n",
        "fitbit_content = {}\n",
        "print(fitbit_snapshots[-140])\n",
        "gfile.Copy(fitbit_folder / fitbit_snapshots[-140] / data_type,'/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/'+data_type+'2.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lfcwi0pJoEW"
      },
      "source": [
        "# STEP 1: Prepare Individual Participant Sessions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJDZH9DRgDcq"
      },
      "outputs": [],
      "source": [
        "for d in data:\n",
        "  print(\"BY_SUBJECT_\"+d['type'], len(gfile.ListDir(os.path.join(root_folder, \"BY_SUBJECT_\"+d['type']))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_PlJMjLmrO1"
      },
      "outputs": [],
      "source": [
        "root_folder = \"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfO0_lhlRj-z"
      },
      "outputs": [],
      "source": [
        "def get_arrays(row,column,type=float):\n",
        "  list_of_strings = row[column]\n",
        "  list_of_strings = list_of_strings[1:-1]\n",
        "  list_of_integers = list_of_strings.split(',')\n",
        "  series = pd.Series(list_of_integers)\n",
        "  if type == bool:\n",
        "    return series.astype(bool)\n",
        "\n",
        "  try:\n",
        "    return series.astype(float)\n",
        "  except:\n",
        "    print(\"Could not convert millis to float in array conversion.\")\n",
        "    return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rV8P-MpoCHOv"
      },
      "outputs": [],
      "source": [
        "def split_save(L):\n",
        "  df2 = df[df['participant_id']==L]\n",
        "\n",
        "  if gfile.Exists(os.path.join(target_dir,d['type']+\"_\"+str(L)+\".csv\")):\n",
        "    print(L, ' Already exists.')\n",
        "    return\n",
        "\n",
        "  list_arrays = []\n",
        "\n",
        "  ## Loop through raw file and de-nest:\n",
        "  if d['type'] == 'spo2':\n",
        "    df2['activity_time'] = pd.to_datetime(df2['activity_time'], format='mixed')\n",
        "    df2['valid'] = df2['valid'].astype('int')\n",
        "    tmp = df2\n",
        "    tmp['millis_from_start_time'] = 0\n",
        "  else:\n",
        "    for i, row in df2.iterrows():\n",
        "\n",
        "      tmp = pd.DataFrame()\n",
        "\n",
        "      if d['type'] == 'steps' or d['type'] == 'wrist_temperature' or d['type'] == 'sleep_coefficient':\n",
        "        tmp['millis_from_start_time'] = pd.Series(range(0,1440))*1000*60\n",
        "      if d['type'] == 'heart_rate':\n",
        "        tmp['millis_from_start_time'] = pd.Series(range(0,60*60*24))*1000\n",
        "      if d['type'] == 'momentary_stress_algorithm':\n",
        "        tmp['millis_from_start_time'] = get_arrays(row,'offsets')*1000*60\n",
        "      if d['type'] == 'ceda':\n",
        "          tmp['millis_from_start_time'] = get_arrays(row,'millis_from_start_time')\n",
        "      if d['type'] == 'grok':\n",
        "          tmp['millis_from_start_time'] = get_arrays(row,'millis_from_start_of_day')\n",
        "\n",
        "      for feature in d['features_to_extract']:\n",
        "        if feature == 'is_on_wrist':\n",
        "          tmp[feature] = get_arrays(row,feature,bool)\n",
        "        else:\n",
        "          tmp[feature] = get_arrays(row,feature)\n",
        "      tmp['activity_time'] = row['activity_time']\n",
        "      list_arrays.append(tmp)\n",
        "\n",
        "    tmp = pd.concat(list_arrays, ignore_index=True)\n",
        "\n",
        "    tmp['participant_id'] = row['participant_id']\n",
        "    tmp['activity_tm_timezone_offset'] = row[d['timezone_offset_column']]\n",
        "\n",
        "    ## Convert time to LOCAL:\n",
        "    tmp['activity_time'] = pd.to_datetime(tmp['activity_time'])\n",
        "\n",
        "    if d['type'] == 'ceda':\n",
        "      tmp['activity_time_local'] = tmp['activity_time'] + tmp['activity_tm_timezone_offset'].astype('timedelta64[m]') + tmp['activity_tm_timezone_offset'].astype('timedelta64[m]') + tmp['millis_from_start_time'].astype('timedelta64[ms]')\n",
        "    else:\n",
        "      tmp['activity_time_local'] = tmp['activity_time'] + tmp['activity_tm_timezone_offset'].astype('timedelta64[m]') + tmp['millis_from_start_time'].astype('timedelta64[ms]')\n",
        "\n",
        "    ## Rename columns:\n",
        "    tmp.rename(columns={'activity_time_local': 'DT', 'participant_id': 'ID'}, inplace=True)\n",
        "\n",
        "    cols = d['features_to_extract'].copy()\n",
        "    cols.append('ID')\n",
        "    cols.append('DT')\n",
        "    tmp = tmp[cols]\n",
        "\n",
        "  tmp.to_csv(gfile.Open(os.path.join(target_dir,d['type']+\"_\"+str(L)+\".csv\"), 'w'))\n",
        "  print(L, ' successfully saved.')\n",
        "\n",
        "for d in data:\n",
        "\n",
        "  print(d['type'])\n",
        "\n",
        "  ## Load raw file:\n",
        "  with gfile.Open(os.path.join(root_folder, d['raw_file']+'.csv.csv'), 'r') as f:\n",
        "    df = pd.read_csv(f)\n",
        "  df.reset_index(inplace=True)\n",
        "\n",
        "  if len(df) == 0:\n",
        "    continue\n",
        "\n",
        "  ## Make output dir and save per subject files:\n",
        "  target_dir = os.path.join(root_folder,\"BY_SUBJECT_\"+d['type'])\n",
        "  if gfile.Exists(target_dir):\n",
        "    print(target_dir)\n",
        "    #gfile.DeleteRecursively(target_dir)\n",
        "    #gfile.MakeDirs(target_dir)\n",
        "  else:\n",
        "    gfile.MakeDirs(target_dir)\n",
        "\n",
        "  #split_save(19395)\n",
        "\n",
        "  WORKER_COUNT = 20\n",
        "  L = pd.unique(df.participant_id)\n",
        "  with multiprocessing.pool.ThreadPool(WORKER_COUNT) as pool:\n",
        "    output = list(pool.map(split_save, L))\n",
        "    pool.close()\n",
        "    pool.join()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWOqq2WpimXj"
      },
      "source": [
        "# STEP 2: Sessionize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NSM9h6ij1rB7"
      },
      "outputs": [],
      "source": [
        "#@title Data session class definition\n",
        "\n",
        "import abc\n",
        "import dataclasses\n",
        "import functools as ft\n",
        "import jaxtyping as jt\n",
        "from scipy.stats import zscore\n",
        "from google3.fitbit.research.sensing.common.proto import data_key_pb2\n",
        "\n",
        "class Sensor(abc.ABC):\n",
        "\n",
        "  def resample(timeseries_data, input_timestamp_units='s', output_timestamp_units='1min'):\n",
        "    \"\"\"Downsamples a pandas dataframe with unknown frequency into a minutely frequency, using the column 't'.\n",
        "\n",
        "    Args:\n",
        "      timeseries_data: A pandas dataframe with a column 't' of timestamps to use\n",
        "      for downsampling.\n",
        "      timestamp_units: The units to use for the timestamps.\n",
        "\n",
        "    Returns:\n",
        "      A pandas dataframe with a minutely frequency.\n",
        "    \"\"\"\n",
        "\n",
        "    timeseries_data['DT'] = pd.to_datetime(\n",
        "        timeseries_data['t'], unit=input_timestamp_units\n",
        "    )\n",
        "    timeseries_data.drop(columns=['t'], inplace=True)\n",
        "    timeseries_data = timeseries_data.resample(output_timestamp_units, on='DT').mean()\n",
        "    return timeseries_data\n",
        "\n",
        "\n",
        "class HeartRate(Sensor):\n",
        "  \"\"\"Heart rate sensor data.\"\"\"\n",
        "\n",
        "  sessions: list\n",
        "\n",
        "  def __init__(self, data, sensor_key, input_timestamp_units='s', output_timestamp_units='1min'):\n",
        "\n",
        "\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "      data.rename(columns={'bpm': 'HR'}, inplace=True)\n",
        "      data['DT'] = pd.to_datetime(data['DT'])\n",
        "      self.hr = data[['DT','HR']].set_index('DT')\n",
        "    elif isinstance(sensor_key, data_key_pb2.DataKey) and sensor_key in data.data.keys():\n",
        "      sessions = data.data[sensor_key]\n",
        "      hr = []\n",
        "      for session in sessions:\n",
        "        times = pd.date_range(datetime.datetime.fromtimestamp(session.activity_tm.seconds, tz=datetime.timezone.utc), periods=60*60*24, freq='1s')\n",
        "        hr_day = pd.DataFrame({'t': times, 'HR': session.bpm})\n",
        "        hr.append(hr_day)\n",
        "      self.hr = Sensor.resample(pd.concat(hr), input_timestamp_units='s', output_timestamp_units='1min')\n",
        "    else:\n",
        "      self.hr = pd.DataFrame(columns=['DT',\n",
        "                                      'HR']).set_index('DT')\n",
        "      print(ValueError(sensor_key + ' not found in data.data.keys()'))\n",
        "\n",
        "\n",
        "\n",
        "class ContinuousEDA(Sensor):\n",
        "  \"\"\"Continuous EDA sensor data.\"\"\"\n",
        "  sessions: list\n",
        "\n",
        "  def __init__(self, data, sensor_key, input_timestamp_units='s', output_timestamp_units='1min'):\n",
        "\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "      data['DT'] = pd.to_datetime(data['DT'])\n",
        "      self.continuous_eda = data[['DT',\n",
        "                                  'eda_level_real',\n",
        "                                  'eda_level_imaginary',\n",
        "                                  'eda_slope_real',\n",
        "                                  'eda_slope_imaginary',\n",
        "                                  'leads_contact_counts']].set_index('DT')\n",
        "    elif isinstance(sensor_key, data_key_pb2.DataKey) and sensor_key in data.data.keys():\n",
        "      sessions = data.data[sensor_key]\n",
        "      continuous_eda = []\n",
        "      for session in sessions:\n",
        "        t = []\n",
        "        for i in session.millis_from_start_time:\n",
        "          t.append(datetime.datetime.fromtimestamp(i/1000 + session.activity_tm_timezone_offset*60 + session.activity_tm.seconds, tz=datetime.timezone.utc))\n",
        "        times = pd.DatetimeIndex(t)\n",
        "        continuous_eda_day = pd.DataFrame({'t': times,\n",
        "                                          'eda_level_real': session.eda_level_real,\n",
        "                                          'eda_level_imaginary': session.eda_level_imaginary,\n",
        "                                          'eda_slope_real': session.eda_slope_real,\n",
        "                                          'eda_slope_imaginary': session.eda_slope_imaginary,\n",
        "                                          'leads_contact_counts': session.leads_contact_counts})\n",
        "        continuous_eda.append(continuous_eda_day)\n",
        "      self.continuous_eda = Sensor.resample(pd.concat(continuous_eda), input_timestamp_units='s', output_timestamp_units='1min')\n",
        "    else:\n",
        "      self.continuous_eda = pd.DataFrame(columns=['DT',\n",
        "                                                  'eda_level_real',\n",
        "                                                  'eda_level_imaginary',\n",
        "                                                  'eda_slope_real',\n",
        "                                                  'eda_slope_imaginary',\n",
        "                                                  'leads_contact_counts']).set_index('DT')\n",
        "      print(ValueError(sensor_key + ' not found in data.data.keys()'))\n",
        "\n",
        "class Steps(Sensor):\n",
        "  \"\"\"Steps sensor data.\"\"\"\n",
        "  sessions: list\n",
        "\n",
        "  def __init__(self, data, sensor_key, input_timestamp_units='s', output_timestamp_units='1min'):\n",
        "\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "      data['DT'] = pd.to_datetime(data['DT'])\n",
        "      self.steps = data[['DT',\n",
        "                        'steps']].set_index('DT')\n",
        "    elif isinstance(sensor_key, data_key_pb2.DataKey) and sensor_key in data.data.keys():\n",
        "      sessions = data.data[sensor_key]\n",
        "      steps = []\n",
        "      for session in sessions:\n",
        "        times = pd.date_range(datetime.datetime.fromtimestamp(session.activity_tm.seconds, tz=datetime.timezone.utc), periods=60*24, freq='1min')\n",
        "        steps_day = pd.DataFrame({'t': times, 'steps': session.steps})\n",
        "        steps.append(steps_day)\n",
        "      self.steps = Sensor.resample(pd.concat(steps), input_timestamp_units='s', output_timestamp_units='1min')\n",
        "    else:\n",
        "      self.steps = pd.DataFrame(columns=['DT',\n",
        "                                         'steps']).set_index('DT')\n",
        "      print(ValueError(sensor_key + ' not found in data.data.keys()'))\n",
        "\n",
        "\n",
        "class Grok(Sensor):\n",
        "  \"\"\"Grok sensor data.\"\"\"\n",
        "  sessions: list\n",
        "\n",
        "  def __init__(self, data, sensor_key, input_timestamp_units='s', output_timestamp_units='1min'):\n",
        "\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "      data['DT'] = pd.to_datetime(data['DT'])\n",
        "      self.grok = data[['DT',\n",
        "                        'jerk_auto',\n",
        "                        'step_count',\n",
        "                        'log_energy',\n",
        "                        'covariance',\n",
        "                        'log_energy_ratio',\n",
        "                        'zero_crossing_std',\n",
        "                        'zero_crossing_avg',\n",
        "                        'axis_mean',\n",
        "                        'altim_std',\n",
        "                        'kurtosis']].set_index('DT')\n",
        "    elif isinstance(sensor_key, data_key_pb2.DataKey) and sensor_key in data.data.keys():\n",
        "      sessions = data.data[sensor_key]\n",
        "      grok = []\n",
        "      for session in data.data['grok_feature_data_with_dupes']:\n",
        "        t = []\n",
        "        for i in session.activity_tms:\n",
        "          t.append(datetime.datetime.fromtimestamp(i.seconds, tz=datetime.timezone.utc))\n",
        "        times = pd.DatetimeIndex(t)\n",
        "        grok_day = pd.DataFrame({'t': times,\n",
        "                                'jerk_auto': session.jerk_auto,\n",
        "                                'step_count': session.step_count,\n",
        "                                'log_energy': session.log_energy,\n",
        "                                'covariance': session.covariance,\n",
        "                                'log_energy_ratio': session.log_energy_ratio,\n",
        "                                'zero_crossing_std': session.zero_crossing_std,\n",
        "                                'zero_crossing_avg': session.zero_crossing_avg,\n",
        "                                'axis_mean': session.axis_mean,\n",
        "                                'altim_std': session.altim_std,\n",
        "                                'kurtosis': session.kurtosis})\n",
        "        grok.append(grok_day)\n",
        "      self.grok = Sensor.resample(pd.concat(grok), input_timestamp_units='s', output_timestamp_units='1min')\n",
        "    else:\n",
        "      self.grok = pd.DataFrame(columns=['DT',\n",
        "                                        'jerk_auto',\n",
        "                                        'step_count',\n",
        "                                        'log_energy',\n",
        "                                        'covariance',\n",
        "                                        'log_energy_ratio',\n",
        "                                        'zero_crossing_std',\n",
        "                                        'zero_crossing_avg',\n",
        "                                        'axis_mean',\n",
        "                                        'altim_std',\n",
        "                                        'kurtosis']).set_index('DT')\n",
        "      print(ValueError(sensor_key + ' not found in data.data.keys()'))\n",
        "\n",
        "class SleepCoefficient(Sensor):\n",
        "  \"\"\"Sleep coefficient sensor data.\"\"\"\n",
        "  sessions: list\n",
        "\n",
        "  def __init__(self, data, sensor_key, input_timestamp_units='s', output_timestamp_units='1min'):\n",
        "\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "      data['DT'] = pd.to_datetime(data['DT'])\n",
        "      self.sleep_coefficient = data[['DT',\n",
        "                                     'sleep_coefficient',\n",
        "                                     'is_on_wrist']].set_index('DT')\n",
        "    elif isinstance(sensor_key, data_key_pb2.DataKey) and sensor_key in data.data.keys():\n",
        "      sessions = data.data[sensor_key]\n",
        "      sleep_coefficient = []\n",
        "      for session in data.data['sleep_coefficient_compact']:\n",
        "        times = pd.date_range(datetime.datetime.fromtimestamp(session.activity_tm.seconds, tz=datetime.timezone.utc), periods=60*24*2, freq='30s')\n",
        "        sleep_coefficient_day = pd.DataFrame({'t': times,\n",
        "                                              'sleep_coefficient': session.sleep_coefficient,\n",
        "                                              'is_on_wrist': session.is_on_wrist})\n",
        "        sleep_coefficient.append(sleep_coefficient_day)\n",
        "      self.sleep_coefficient = Sensor.resample(pd.concat(sleep_coefficient), input_timestamp_units='s', output_timestamp_units='1min')\n",
        "    else:\n",
        "      self.sleep_coefficient = pd.DataFrame(columns=['DT',\n",
        "                                        'sleep_coefficient',\n",
        "                                        'is_on_wrist']).set_index('DT')\n",
        "      print(ValueError(sensor_key + ' not found in data.data.keys()'))\n",
        "\n",
        "class SkinTemp(Sensor):\n",
        "  \"\"\"Skin temperature sensor data.\"\"\"\n",
        "  sessions: list\n",
        "\n",
        "  def __init__(self, data, sensor_key, input_timestamp_units='s', output_timestamp_units='1min'):\n",
        "\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "      data['DT'] = pd.to_datetime(data['DT'])\n",
        "      self.skin_temp = data[['DT',\n",
        "                              'wrist_temperatures']].set_index('DT')\n",
        "    elif isinstance(sensor_key, data_key_pb2.DataKey) and sensor_key in data.data.keys():\n",
        "      sessions = data.data[sensor_key]\n",
        "      skin_temp = []\n",
        "      for session in sessions:\n",
        "        times = pd.date_range(datetime.datetime.fromtimestamp(session.activity_tm.seconds, tz=datetime.timezone.utc), periods=60*24, freq='1min')\n",
        "        skintemp_day = pd.DataFrame({'t': times, 'wrist_temperatures': session.wrist_temperatures})\n",
        "        skin_temp.append(skintemp_day)\n",
        "      self.skin_temp = Sensor.resample(pd.concat(skin_temp), input_timestamp_units='s', output_timestamp_units='1min')\n",
        "    else:\n",
        "      self.skin_temp = pd.DataFrame(columns=['DT',\n",
        "                                        'wrist_temperatures']).set_index('DT')\n",
        "      print(ValueError(sensor_key + ' not found in data.data.keys()'))\n",
        "\n",
        "class MomentaryStressAlgorithm(Sensor):\n",
        "  \"\"\"Momentary stress algorithm sensor data.\"\"\"\n",
        "  sessions: list\n",
        "\n",
        "  def __init__(self, data, sensor_key, input_timestamp_units='s', output_timestamp_units='1min'):\n",
        "\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "      data['DT'] = pd.to_datetime(data['DT'])\n",
        "      self.momentary_stress_algorithm = data[['DT',\n",
        "                                              'hrv_shannon_entropy_rr',\n",
        "                                              'hrv_shannon_entropy_rrd',\n",
        "                                              'hrv_percentage_of_nn_30',\n",
        "                                              'ceda_magnitude_real_micro_siemens',\n",
        "                                              'ceda_slope_real_micro_siemens',\n",
        "                                              'rmssd_percentile_0595',\n",
        "                                              'sdnn_percentile_0595',\n",
        "                                              'msa_probability',\n",
        "                                              'hrv_percent_good',\n",
        "                                              'hrv_rr_80th_percentile_mean',\n",
        "                                              'hrv_rr_20th_percentile_mean',\n",
        "                                              'hrv_rr_median',\n",
        "                                              'hrv_rr_mean',\n",
        "                                              'hr_at_rest_mean',\n",
        "                                              'skin_temperature_magnitude',\n",
        "                                              'skin_temperature_slope']].set_index('DT')\n",
        "    elif isinstance(sensor_key, data_key_pb2.DataKey) and sensor_key in data.data.keys():\n",
        "      sessions = data.data[sensor_key]\n",
        "      momentary_stress_algorithm = []\n",
        "      for session in sessions:\n",
        "        t = []\n",
        "        for i in session.offsets:\n",
        "          t.append(datetime.datetime.fromtimestamp(i*60 + session.activity_tm.seconds, tz=datetime.timezone.utc))\n",
        "        times = pd.DatetimeIndex(t)\n",
        "        msa_day = pd.DataFrame({'t': times,\n",
        "                                'hrv_shannon_entropy_rr': session.hrv_shannon_entropy_rr,\n",
        "                                'hrv_shannon_entropy_rrd': session.hrv_shannon_entropy_rrd,\n",
        "                                'hrv_percentage_of_nn_30': session.hrv_percentage_of_nn_30,\n",
        "                                'ceda_magnitude_real_micro_siemens': session.ceda_magnitude_real_micro_siemens,\n",
        "                                'ceda_slope_real_micro_siemens': session.ceda_slope_real_micro_siemens,\n",
        "                                'rmssd_percentile_0595': session.rmssd_percentile_0595,\n",
        "                                'sdnn_percentile_0595': session.sdnn_percentile_0595,\n",
        "                                'msa_probability': session.msa_probability,\n",
        "                                'hrv_percent_good': session.hrv_percent_good,\n",
        "                                'hrv_rr_80th_percentile_mean': session.hrv_rr_80th_percentile_mean,\n",
        "                                'hrv_rr_20th_percentile_mean': session.hrv_rr_20th_percentile_mean,\n",
        "                                'hrv_rr_median': session.hrv_rr_median,\n",
        "                                'hrv_rr_mean': session.hrv_rr_mean,\n",
        "                                'hr_at_rest_mean': session.hr_at_rest_mean,\n",
        "                                'skin_temperature_magnitude': session.skin_temperature_magnitude,\n",
        "                                'skin_temperature_slope': session.skin_temperature_slope})\n",
        "        momentary_stress_algorithm.append(msa_day)\n",
        "      self.momentary_stress_algorithm = Sensor.resample(pd.concat(momentary_stress_algorithm), input_timestamp_units='s', output_timestamp_units='1min')\n",
        "    else:\n",
        "      self.momentary_stress_algorithm = pd.DataFrame(columns=['DT',\n",
        "                                                              'hrv_shannon_entropy_rr',\n",
        "                                                              'hrv_shannon_entropy_rrd',\n",
        "                                                              'hrv_percentage_of_nn_30',\n",
        "                                                              'ceda_magnitude_real_micro_siemens',\n",
        "                                                              'ceda_slope_real_micro_siemens',\n",
        "                                                              'rmssd_percentile_0595',\n",
        "                                                              'sdnn_percentile_0595',\n",
        "                                                              'msa_probability',\n",
        "                                                              'hrv_percent_good',\n",
        "                                                              'hrv_rr_80th_percentile_mean',\n",
        "                                                              'hrv_rr_20th_percentile_mean',\n",
        "                                                              'hrv_rr_median',\n",
        "                                                              'hrv_rr_mean',\n",
        "                                                              'hr_at_rest_mean',\n",
        "                                                              'skin_temperature_magnitude',\n",
        "                                                              'skin_temperature_slope']).set_index('DT')\n",
        "\n",
        "    print(ValueError(sensor_key + ' not found in data.data.keys()'))\n",
        "\n",
        "\n",
        "@dataclasses.dataclass(frozen=True)\n",
        "class ProdSession:\n",
        "\n",
        "  # A session specific identifier for a 24hr period of data collection.\n",
        "  session_id: str\n",
        "  # Heart rate table data.\n",
        "  hr: HeartRate\n",
        "  # Continuous heart rate table data.\n",
        "  continuous_eda: ContinuousEDA\n",
        "  # Steps table data.\n",
        "  steps: Steps\n",
        "  # Grok table data.\n",
        "  grok: Grok\n",
        "  # Sleep Coefficient table data.\n",
        "  sleep_coefficient: SleepCoefficient\n",
        "  # Skin Temp table data.\n",
        "  skin_temp: SkinTemp\n",
        "  # MSA table data.\n",
        "  momentary_stress_algorithm: MomentaryStressAlgorithm\n",
        "\n",
        "  def join(self) -\u003e pd.DataFrame:\n",
        "\n",
        "    dfs = [self.hr, self.continuous_eda, self.steps, self.grok, self.sleep_coefficient, self.skin_temp, self.momentary_stress_algorithm]\n",
        "    session = ft.reduce(lambda left, right: pd.merge(left, right, on='DT', how='outer'), dfs)\n",
        "    if 'is_on_wrist' in session.columns:\n",
        "      session.loc[(session.is_on_wrist == 0), :] = np.nan\n",
        "      #session = session.apply(lambda col: zscore(col, nan_policy='omit') if col.notna().any() else col)\n",
        "      #session = session.clip(-3,3)\n",
        "      return session\n",
        "    else:\n",
        "      return pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lT3Js0X7iofd"
      },
      "outputs": [],
      "source": [
        "root_folder = \"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm\"\n",
        "files = gfile.ListDir(os.path.join(root_folder,\"BY_SUBJECT_momentary_stress_algorithm\"))\n",
        "ids = [s[-9:] for s in files]\n",
        "print(ids)\n",
        "\n",
        "all_features = []\n",
        "for d in data:\n",
        "  all_features.extend(d['features_to_extract'])\n",
        "\n",
        "types = list(map(lambda x : x['type'], data))\n",
        "cnt=0\n",
        "\n",
        "def window(ids: list[str], window_length: str, timestamp_units: str):\n",
        "  inputs = []\n",
        "  mask = []\n",
        "\n",
        "  for i in ids:\n",
        "    print('ID: ', i)\n",
        "    d = {}\n",
        "    dfs = []\n",
        "    for table in data:\n",
        "      t = table['type']\n",
        "\n",
        "      try:\n",
        "        d[t] = pd.read_csv(gfile.Open(os.path.join(root_folder,\"BY_SUBJECT_\"+t,t+\"_\"+i), 'r'))\n",
        "        d[t].rename(columns={'DT': 't'}, inplace=True)\n",
        "        d[t]['t'] = pd.to_datetime(d[t]['t'])\n",
        "      except:\n",
        "        print(t + ' not found.')\n",
        "        continue\n",
        "\n",
        "      cols = table['features_to_extract'].copy()\n",
        "      cols.append('t')\n",
        "      d[t] = d[t][cols]\n",
        "\n",
        "      if t == 'heart_rate':\n",
        "        #d[t]['bpm'][d[t]['bpm'] == -1] = np.nan\n",
        "        d[t].loc[d[t]['bpm'] == -1,'bpm'] = np.nan\n",
        "\n",
        "      if t == 'ceda':\n",
        "        d[t].loc[d[t]['eda_level_real'] \u003e 60, \"eda_level_real\"] = 60\n",
        "        d[t].loc[d[t]['eda_level_real'] \u003c 0, \"eda_level_real\"] = 0\n",
        "\n",
        "      if t == 'momentary_stress_algorithm':\n",
        "        d[t].loc[d[t]['ceda_slope_real_micro_siemens'] \u003e 5, \"ceda_slope_real_micro_siemens\"] = 5\n",
        "        d[t].loc[d[t]['ceda_slope_real_micro_siemens'] \u003c -5, \"ceda_slope_real_micro_siemens\"] = -5\n",
        "\n",
        "      if t == 'sleep_coefficient':\n",
        "        d[t].loc[d[t]['sleep_coefficient'] == -1, \"sleep_coefficient\"] = np.nan\n",
        "\n",
        "      if t == 'wrist_temperature':\n",
        "        d[t].loc[:,'wrist_temperatures'] = d[t]['wrist_temperatures']/20000\n",
        "        d[t].loc[d[t]['wrist_temperatures'] \u003e 41, \"wrist_temperatures\"] = 41\n",
        "        d[t].loc[d[t]['wrist_temperatures'] \u003c 0, \"wrist_temperatures\"] = np.nan\n",
        "\n",
        "      if t == 'grok':\n",
        "        d[t].loc[:,'altim_std'] = d[t]['altim_std']/255\n",
        "\n",
        "      if len(d[t]) \u003e 0:\n",
        "        d[t] = Sensor.resample(d[t], input_timestamp_units='s', output_timestamp_units=timestamp_units)\n",
        "        dfs.append(d[t])\n",
        "\n",
        "    if len(dfs) \u003e 0:\n",
        "      session = ft.reduce(lambda left, right: pd.merge(left, right, on='DT', how='outer'), dfs)\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "    for feature in FEATURES_TO_INCLUDE:\n",
        "      if feature not in session.columns:\n",
        "        session.loc[:,feature] = np.nan\n",
        "\n",
        "    session = session[FEATURES_TO_INCLUDE]\n",
        "    for feature in FEATURES_TO_INCLUDE:\n",
        "      session.loc[:,feature] = (\n",
        "          session[feature] - NORMALIZATION_PARAMETERS[feature][0]\n",
        "      ) / (NORMALIZATION_PARAMETERS[feature][1])\n",
        "    session = session.clip(-5, 5)\n",
        "\n",
        "    df_grouped = session.groupby(pd.Grouper(freq=window_length))\n",
        "    for name, group in df_grouped:\n",
        "      nan_mask = np.isnan(group.to_numpy())\n",
        "      missingness_ratio = np.sum(nan_mask) / (\n",
        "          nan_mask.shape[0] * nan_mask.shape[1]\n",
        "      )\n",
        "      if group.shape[0] == 168*60 and group.shape[1] == len(FEATURES_TO_INCLUDE):\n",
        "        if missingness_ratio\u003e0.8:\n",
        "          print('.   Too much missingness.')\n",
        "        else:\n",
        "          group = np.nan_to_num(group)\n",
        "          yield name, {\n",
        "              'id': i,\n",
        "              'input': group,#.values,\n",
        "              'mask': nan_mask,\n",
        "          }\n",
        "\n",
        "w = window(ids[:10], '168h', '1min')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKmmlf9UmOR6"
      },
      "source": [
        "# STEP 3: Load and Process Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQW39IcH3gvZ"
      },
      "outputs": [],
      "source": [
        "# @title Load and Process Surveys\n",
        "demo = pd.read_csv(gfile.Open(os.path.join(root_folder,'x_vico_demographic_questionnaire_responses.csv')))\n",
        "try:\n",
        "  demo.set_index([\"#study_participant_id\"], inplace=True)\n",
        "except:\n",
        "  'Index already reset.'\n",
        "\n",
        "joined_df = demo\n",
        "\n",
        "# BFI Responses\n",
        "bfi = pd.read_csv(gfile.Open(os.path.join(root_folder,'x_vico_intake_survey_bfi_questionnaire_responses.csv')))\n",
        "# Code BFI:\n",
        "#Extraversion: 1R, 5 #Agreeableness: 2, 7R #Conscientiousness: 3R, 8 #Neuroticism: 4R, 9 #Openness to Experience: 5R, 10\n",
        "mapping = {'Disagree strongly': 1, 'Disagree a little': 2, 'Neither agree nor disagree': 3, 'Agree a little': 4, 'Agree strongly': 5}\n",
        "for c in range(1,11):\n",
        "  bfi['intake_survey_-_bfi-10_q'+str(c)+'_group_score'] = bfi['intake_survey_-_bfi-10_q'+str(c)+'_group'].map(mapping)\n",
        "\n",
        "bfi['extraversion_score'] = -bfi['intake_survey_-_bfi-10_q1_group_score'] + bfi['intake_survey_-_bfi-10_q5_group_score']\n",
        "bfi['agreeableness_score'] = bfi['intake_survey_-_bfi-10_q2_group_score'] - bfi['intake_survey_-_bfi-10_q7_group_score']\n",
        "bfi['conscientiousness_score'] = -bfi['intake_survey_-_bfi-10_q3_group_score'] + bfi['intake_survey_-_bfi-10_q8_group_score']\n",
        "bfi['neuroticism_score'] = -bfi['intake_survey_-_bfi-10_q4_group_score'] + bfi['intake_survey_-_bfi-10_q9_group_score']\n",
        "bfi['openness_score'] = -bfi['intake_survey_-_bfi-10_q5_group_score'] + bfi['intake_survey_-_bfi-10_q10_group_score']\n",
        "\n",
        "bfi.reset_index(inplace=True)\n",
        "bfi.set_index([\"#study_participant_id\"], inplace=True)\n",
        "joined_df = joined_df.join(bfi, lsuffix='bfi_')\n",
        "\n",
        "# PHQ Responses\n",
        "# Intake:\n",
        "phq = pd.read_csv(gfile.Open(os.path.join(root_folder,'x_vico_phq_8_intake_questionnaire_responses.csv')))\n",
        "mapping = {'Not at all': 0, 'Several days': 1, 'More than half the days': 2, 'Nearly every day': 3}\n",
        "columns = ['little_interest','depression','sleep','tired','appetite','failure','trouble_concentrating','restlessness']\n",
        "for c in columns:\n",
        "  phq[c+'_intake_score'] = phq[c].map(mapping)\n",
        "columns = ['little_interest_intake_score','depression_intake_score','sleep_intake_score',\n",
        "           'tired_intake_score','appetite_intake_score','failure_intake_score',\n",
        "           'trouble_concentrating_intake_score','restlessness_intake_score']\n",
        "phq['phq_intake_score'] = phq[columns].mean(axis=1)*8\n",
        "phq = phq[['#study_participant_id',\n",
        "           'little_interest_intake_score','depression_intake_score','sleep_intake_score',\n",
        "           'tired_intake_score','appetite_intake_score','failure_intake_score',\n",
        "           'trouble_concentrating_intake_score','restlessness_intake_score','phq_intake_score']]\n",
        "\n",
        "phq.reset_index(inplace=True)\n",
        "phq.set_index([\"#study_participant_id\"], inplace=True)\n",
        "joined_df = joined_df.join(phq, lsuffix='phq_')\n",
        "\n",
        "# Completion:\n",
        "phq = pd.read_csv(gfile.Open(os.path.join(root_folder,'x_vico_phq_8_complete_questionnaire_responses.csv')))\n",
        "mapping = {'Not at all': 0, 'Several days': 1, 'More than half the days': 2, 'Nearly every day': 3}\n",
        "columns = ['little_interest','depression','sleep','tired','appetite','failure','trouble_concentrating','restlessness']\n",
        "for c in columns:\n",
        "  phq[c+'_complete_score'] = phq[c].map(mapping)\n",
        "columns = ['little_interest_complete_score','depression_complete_score','sleep_complete_score',\n",
        "           'tired_complete_score','appetite_complete_score','failure_complete_score',\n",
        "           'trouble_concentrating_complete_score','restlessness_complete_score']\n",
        "phq['phq_complete_score'] = phq[columns].mean(axis=1)*8\n",
        "phq = phq[['#study_participant_id',\n",
        "           'little_interest_complete_score','depression_complete_score','sleep_complete_score',\n",
        "           'tired_complete_score','appetite_complete_score','failure_complete_score',\n",
        "           'trouble_concentrating_complete_score','restlessness_complete_score','phq_complete_score']]\n",
        "\n",
        "phq.reset_index(inplace=True)\n",
        "phq.set_index([\"#study_participant_id\"], inplace=True)\n",
        "phq.drop(columns=['index'], inplace=True)\n",
        "joined_df = joined_df.join(phq, lsuffix='phq_')\n",
        "\n",
        "\n",
        "# GAD Responses\n",
        "# Intake:\n",
        "gad = pd.read_csv(gfile.Open(os.path.join(root_folder,'x_vico_gad_7_intake_questionnaire_responses.csv')))\n",
        "mapping = {'Not at all': 0, 'Several days': 1, 'More than half the days': 2,\n",
        "           'Nearly every day': 3}\n",
        "columns = ['anxiety','cannot_stop_worry','too_much_worry','trouble_relaxing','restlessness','irritability','fear']\n",
        "for c in columns:\n",
        "  gad['gad_'+c+'_intake_score'] = gad[c].map(mapping)\n",
        "columns = ['gad_anxiety_intake_score','gad_cannot_stop_worry_intake_score','gad_too_much_worry_intake_score','gad_trouble_relaxing_intake_score','gad_restlessness_intake_score','gad_irritability_intake_score','gad_fear_intake_score']\n",
        "gad['gad_intake_score'] = gad[columns].mean(axis=1)*7\n",
        "gad = gad[['#study_participant_id','gad_anxiety_intake_score','gad_cannot_stop_worry_intake_score','gad_too_much_worry_intake_score','gad_trouble_relaxing_intake_score','gad_restlessness_intake_score','gad_irritability_intake_score','gad_fear_intake_score','gad_intake_score']]\n",
        "\n",
        "gad.reset_index(inplace=True)\n",
        "gad.set_index([\"#study_participant_id\"], inplace=True)\n",
        "gad.drop(columns=['index'], inplace=True)\n",
        "joined_df = joined_df.join(gad, lsuffix='gad_')\n",
        "\n",
        "# Completion:\n",
        "gad = pd.read_csv(gfile.Open(os.path.join(root_folder,'x_vico_gad_7_complete_questionnaire_responses.csv')))\n",
        "mapping = {'Not at all': 0, 'Several days': 1, 'More than half the days': 2,\n",
        "           'Nearly every day': 3}\n",
        "columns = ['anxiety','cannot_stop_worry','too_much_worry','trouble_relaxing','restlessness','irritability','fear']\n",
        "for c in columns:\n",
        "  gad['gad_'+c+'_complete_score'] = gad[c].map(mapping)\n",
        "columns = ['gad_anxiety_complete_score','gad_cannot_stop_worry_complete_score','gad_too_much_worry_complete_score','gad_trouble_relaxing_complete_score','gad_restlessness_complete_score','gad_irritability_complete_score','gad_fear_complete_score']\n",
        "gad['gad_complete_score'] = gad[columns].mean(axis=1)*7\n",
        "gad = gad[['#study_participant_id','gad_anxiety_complete_score','gad_cannot_stop_worry_complete_score','gad_too_much_worry_complete_score','gad_trouble_relaxing_complete_score','gad_restlessness_complete_score','gad_irritability_complete_score','gad_fear_complete_score','gad_complete_score']]\n",
        "\n",
        "gad.reset_index(inplace=True)\n",
        "gad.set_index([\"#study_participant_id\"], inplace=True)\n",
        "gad.drop(columns=['index'], inplace=True)\n",
        "joined_df = joined_df.join(gad, lsuffix='gad_')\n",
        "\n",
        "# Sleep disturbance Responses\n",
        "sleep_disturbance = pd.read_csv(gfile.Open(os.path.join(root_folder,'x_vico_sleep_disturbance_intake_questionnaire_responses.csv')))\n",
        "\n",
        "mapping = {'Not at all': 1, 'A little bit': 2, 'Somewhat': 3, 'Quite a bit': 4, 'Very much': 5}\n",
        "columns = ['restless','satisfied', 'refreshing','trouble_falling_asleep']\n",
        "for c in columns:\n",
        "  sleep_disturbance[c+'_score'] = sleep_disturbance[c].map(mapping)\n",
        "\n",
        "mapping = {'Never': 1, 'Rarely': 2, 'Sometimes': 3, 'Often': 4, 'Always': 5}\n",
        "columns = ['trouble_staying_asleep', 'trouble_sleeping',\n",
        "       'enough_sleep']\n",
        "for c in columns:\n",
        "  sleep_disturbance[c+'_score'] = sleep_disturbance[c].map(mapping)\n",
        "\n",
        "mapping = {'Very poor': 1, 'Poor': 2, 'Fair': 3, 'Good': 4, 'Very good': 5}\n",
        "columns = ['quality']\n",
        "for c in columns:\n",
        "  sleep_disturbance[c+'_score'] = sleep_disturbance[c].map(mapping)\n",
        "\n",
        "sleep_disturbance['sleep_disturbance_score'] = (sleep_disturbance['restless_score'] + (5 - sleep_disturbance['satisfied_score']) + (5 - sleep_disturbance['refreshing_score']) + sleep_disturbance['trouble_falling_asleep_score'] + sleep_disturbance['trouble_staying_asleep_score'] + sleep_disturbance['trouble_sleeping_score'] + (5 - sleep_disturbance['enough_sleep_score']) + (5 - sleep_disturbance['quality_score']) )\n",
        "sleep_disturbance.reset_index(inplace=True)\n",
        "sleep_disturbance.set_index([\"#study_participant_id\"], inplace=True)\n",
        "joined_df = joined_df.join(sleep_disturbance, lsuffix='sleepdisturbance_')\n",
        "\n",
        "# Sleep Impairment Responses\n",
        "sleep_impairment = pd.read_csv(gfile.Open(os.path.join(root_folder,'x_vico_sleep_impairment_intake_questionnaire_responses.csv')))\n",
        "\n",
        "mapping = {'Not at all': 1, 'A little bit': 2, 'Somewhat': 3, 'Quite a bit': 4, 'Very much': 5}\n",
        "columns = ['trouble_productivity', 'alertness', 'tiredness',\n",
        "       'problems', 'trouble_concentrating', 'irritability',\n",
        "       'sleepy_during_daytime', 'trouble_staying_awake']\n",
        "for c in columns:\n",
        "  sleep_impairment[c+'_score'] = sleep_impairment[c].map(mapping)\n",
        "\n",
        "sleep_impairment['sleep_impairment_score'] = (sleep_impairment['trouble_productivity_score'] + (5 - sleep_impairment['alertness_score']) + sleep_impairment['tiredness_score'] + sleep_impairment['problems_score'] + sleep_impairment['trouble_concentrating_score'] + sleep_impairment['irritability_score'] + sleep_impairment['sleepy_during_daytime_score'] + sleep_impairment['trouble_staying_awake_score'] )\n",
        "\n",
        "sleep_impairment.reset_index(inplace=True)\n",
        "sleep_impairment.set_index([\"#study_participant_id\"], inplace=True)\n",
        "joined_df = joined_df.join(sleep_impairment, lsuffix='sleepimpairment_')\n",
        "\n",
        "# PSS Responses\n",
        "pss = pd.read_csv(gfile.Open(os.path.join(root_folder,'x_vico_pss_intake_questionnaire_responses.csv')))\n",
        "\n",
        "mapping_1 = {'Never': 0, 'Almost Never': 1, 'Sometimes': 2, 'Fairly Often': 3, 'Very Often': 4}\n",
        "mapping_2 = {'Never': 4, 'Almost Never': 3, 'Sometimes': 2, 'Fairly Often': 1, 'Very Often': 0}\n",
        "\n",
        "# 1. In the last month, how often have you been upset because of something that happened unexpectedly?\n",
        "# 2. In the last month, how often have you felt that you were unable to control the important things in your life?\n",
        "# 3. In the last month, how often have you felt nervous and stressed?\n",
        "# 4. In the last month, how often have you felt confident about your ability to handle your personal problems?\n",
        "# 5. In the last month, how often have you felt that things were going your way?\n",
        "# 6. In the last month, how often have you found that you could not cope with all the things that you had to do?\n",
        "# 7. In the last month, how often have you been able to control irritations in your life?\n",
        "# 8. In the last month, how often have you felt that you were on top of things?\n",
        "# 9. In the last month, how often have you been angered because of things that happened that were outside of your control?\n",
        "# 10. In the last month, how often have you felt difficulties were piling up so high that you could not overcome them?\n",
        "\n",
        "columns = ['upset','no_control','stress','handle_personal_problems','things_positive','cannot_cope','control_irritation','on_top_of_things','anger','overwhelm']\n",
        "newcolumns = []\n",
        "for c in columns:\n",
        "  if c in ['handle_personal_problems','things_positive','control_irritation','on_top_of_things']:\n",
        "    pss['pss_'+c+'_score'] = pss[c].map(mapping_2)\n",
        "  else:\n",
        "    pss['pss_'+c+'_score'] = pss[c].map(mapping_1)\n",
        "  newcolumns.append('pss_'+c+'_score')\n",
        "\n",
        "columns = newcolumns\n",
        "pss['pss_score'] = pss[columns].sum(axis=1)\n",
        "columns.append('pss_score')\n",
        "columns.append('#study_participant_id')\n",
        "pss = pss[columns]\n",
        "\n",
        "pss.reset_index(inplace=True)\n",
        "pss.set_index([\"#study_participant_id\"], inplace=True)\n",
        "joined_df = joined_df.join(pss, lsuffix='pss_')\n",
        "\n",
        "plt.figure(figsize=(25, 3))\n",
        "plt.plot(joined_df.isna().sum())\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "joined_df.reset_index(inplace=True)\n",
        "joined_df.index = joined_df.index.astype(int)\n",
        "demo = joined_df\n",
        "demo['phq_delta'] = demo['phq_complete_score'] - demo['phq_intake_score']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r7oRyHPFLmm"
      },
      "source": [
        "# STEP 4: Create TFrecords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_bhMDTwAfDo"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import time\n",
        "import enum\n",
        "\n",
        "def _bytes_feature(value):\n",
        "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "  if isinstance(value, type(tf.constant(0))):\n",
        "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _float_feature(value):\n",
        "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "def _int64_feature(value):\n",
        "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "\n",
        "def numpy_example(array, labels):\n",
        "  feature = {\n",
        "      'phq_intake_score': _int64_feature(labels[0]),\n",
        "      'gad_intake_score': _int64_feature(labels[1]),\n",
        "      'pss_score': _int64_feature(labels[2]),\n",
        "      'sleep_disturbance_score': _int64_feature(labels[3]),\n",
        "      'sleep_impairment_score': _int64_feature(labels[4]),\n",
        "      'extraversion_score': _int64_feature(labels[5]),\n",
        "      'age': _int64_feature(labels[6]),\n",
        "      'gender_group': _int64_feature(labels[7]),\n",
        "      'array_raw': _bytes_feature(tf.io.serialize_tensor(array)),\n",
        "  }\n",
        "  return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "gfile.MakeDirs(tf_record_folder)\n",
        "\n",
        "joined_df.loc[joined_df['gender']=='\u003cskipped\u003e','gender_group'] = np.nan\n",
        "mapping = {'Female': 'Female', 'Male': 'Male', 'Genderqueer/Gender Non Conforming': 'Queer', 'Trans Female/Trans Woman': 'Trans', 'Trans Male/Trans Man': 'Trans', 'Different Identity': 'Trans'}\n",
        "joined_df['gender_clustered'] = joined_df['gender'].map(mapping)\n",
        "codes, uniques = pd.factorize(joined_df['gender_clustered'])\n",
        "print(uniques)\n",
        "joined_df['gender_group'] = codes\n",
        "\n",
        "# Iterate over the dataset and write each example to the TFRecord file\n",
        "for i in ids:\n",
        "  id = i[0:-4]\n",
        "  w = window([i], '168h', '1min')\n",
        "  output_file = 'dwb_' + id +'.tfrecords'\n",
        "  with tf.io.TFRecordWriter(os.path.join(tf_record_folder,output_file)) as writer:\n",
        "    for key, result in w:\n",
        "      labels = []\n",
        "      print(key)\n",
        "      try:\n",
        "        labels.append(int(joined_df[joined_df['#study_participant_id']==int(id)]['phq_intake_score'].values[0]))\n",
        "        labels.append(int(joined_df[joined_df['#study_participant_id']==int(id)]['gad_intake_score'].values[0]))\n",
        "        labels.append(int(joined_df[joined_df['#study_participant_id']==int(id)]['pss_score'].values[0]))\n",
        "        labels.append(int(joined_df[joined_df['#study_participant_id']==int(id)]['sleep_disturbance_score'].values[0]))\n",
        "        labels.append(int(joined_df[joined_df['#study_participant_id']==int(id)]['sleep_impairment_score'].values[0]))\n",
        "        labels.append(int(joined_df[joined_df['#study_participant_id']==int(id)]['extraversion_score'].values[0]))\n",
        "        labels.append(int(joined_df[joined_df['#study_participant_id']==int(id)]['age'].values[0]))\n",
        "        labels.append(int(joined_df[joined_df['#study_participant_id']==int(id)]['gender_group'].values[0]))\n",
        "      except:\n",
        "        print('Label missing.')\n",
        "        continue\n",
        "      tf_example = numpy_example(result['input'], labels)\n",
        "      writer.write(tf_example.SerializeToString())\n",
        "    print(f'TFRecord file created: {os.path.join(tf_record_folder,output_file)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lE-GzHfML2i4"
      },
      "outputs": [],
      "source": [
        "for key, result in w:\n",
        "  print(key)\n",
        "  print(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMvXt7mfzX4q"
      },
      "outputs": [],
      "source": [
        "#def parse_tfrecord(example):\n",
        "  feature = {\n",
        "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'array_raw': tf.io.FixedLenFeature([], tf.string),\n",
        "  }\n",
        "  example = tf.io.parse_single_example(example, feature)\n",
        "  array_feature = tf.io.parse_tensor(example['array_raw'], out_type=tf.double)\n",
        "  return label, array_feature\n",
        "\n",
        "# Create a TFRecordDataset\n",
        "dataset = tf.data.TFRecordDataset([\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_tfrecords/dwb_18457.tfrecords\"])\n",
        "dataset = dataset.map(parse_tfrecord)\n",
        "\n",
        "# Iterate over the dataset\n",
        "for label, array_feature in dataset:\n",
        "  visualize_features(array_feature)\n",
        "  print(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ac8GNG0Bpi7"
      },
      "outputs": [],
      "source": [
        "visualize_features(array_feature)\n",
        "print(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WtN1cieIThq7"
      },
      "outputs": [],
      "source": [
        "# @title Visualize\n",
        "\n",
        "root_folder = \"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm\"\n",
        "files = gfile.ListDir(os.path.join(root_folder,\"BY_SUBJECT_momentary_stress_algorithm\"))\n",
        "ids = [s[-9:] for s in files]\n",
        "print(ids)\n",
        "\n",
        "ids = ['18780.csv']\n",
        "w = window(ids, '168h', '1min')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import seaborn as sns\n",
        "from matplotlib.patches import Patch\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib.patches import Rectangle\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "featureNameMapping = {'HR': 'Heart Rate',\n",
        "    'eda_level_real': 'SCL',\n",
        "    'leads_contact_counts': 'Leads Contact Cnt.',\n",
        "    'steps': 'Steps (Min)',\n",
        "    'jerk_auto': 'Acc. Jerk',\n",
        "    'step_count': 'Steps (Min)',\n",
        "    'log_energy': 'Acc. Log Energy',\n",
        "    'covariance': 'Acc. Covariance',\n",
        "    'log_energy_ratio': 'Acc. Log Energy Ratio',\n",
        "    'zero_crossing_std': 'Acc. Zero Cross Std.',\n",
        "    'zero_crossing_avg': 'Acc. Zero Cross Avg.',\n",
        "    'axis_mean': 'Acc. Axis Mean',\n",
        "    'altim_std': 'Altimeter Std.',\n",
        "    'kurtosis': 'Acc. Kurtosis',\n",
        "    'sleep_coefficient': 'Sleep Prob.',\n",
        "    'wrist_temperatures': 'Skin Temperature',\n",
        "    'hrv_shannon_entropy_rr': 'HRV Shannon Ent.',\n",
        "    'hrv_shannon_entropy_rrd': 'HRV Shannon Ent. Diffs.',\n",
        "    'hrv_percentage_of_nn_30': 'HRV % NN30',\n",
        "    'ceda_magnitude_real_micro_siemens': 'SCL Mag.',\n",
        "    'ceda_slope_real_micro_siemens': 'SCL Slope',\n",
        "    'rmssd_percentile_0595': 'HRV RMSSD',\n",
        "    'sdnn_percentile_0595': 'HRV SDNN',\n",
        "    'msa_probability': 'MSA Prob.',\n",
        "    'hrv_percent_good': 'HRV % Good',\n",
        "    'hrv_rr_80th_percentile_mean': 'HRV RR 80th',\n",
        "    'hrv_rr_20th_percentile_mean': 'HRV RR 20th',\n",
        "    'hrv_rr_median': 'HRV RR Median',\n",
        "    'hrv_rr_mean': 'HRV RR Mean',\n",
        "    'hr_at_rest_mean': 'Resting Heart Rate',\n",
        "    'skin_temperature_magnitude': 'Skin Temp. Mag',\n",
        "    'skin_temperature_slope': 'Skin Temp. Slope'\n",
        "}\n",
        "\n",
        "for x in w:\n",
        "\n",
        "  fig = plt.figure(figsize=(20, 7))\n",
        "  ax1 = plt.subplot2grid((1, 12), (0, 0), colspan=12)\n",
        "\n",
        "  #ax1.set_title('Antimicrobial resistance genes',size=12, pad=30, fontname='Ubuntu')\n",
        "\n",
        "  #x[1]['input'].index = x[1]['input'].index.round('H')\n",
        "  #x[1]['input'].index = x[1]['input'].index.floor('5T')\n",
        "  group = x[1]['input']#.groupby(by='DT').mean()\n",
        "  group.rename(columns=featureNameMapping, inplace=True)\n",
        "  group = group.sort_index(axis=1)\n",
        "\n",
        "  ax1 = sns.heatmap(group.T, cmap=\"Reds\", cbar=True, linewidths=0.0,\n",
        "                    linecolor='black', alpha=0.8, ax=ax1, yticklabels=True)\n",
        "\n",
        "  for tick in ax1.get_xticklabels():\n",
        "      tick.set_fontname('Ubuntu')\n",
        "      tick.set_style('italic')\n",
        "  ax1.tick_params(axis='x', labelsize=10.5)\n",
        "\n",
        "  for tick in ax1.get_yticklabels():\n",
        "      tick.set_fontname('Ubuntu')\n",
        "  ax1.tick_params(axis='y', labelsize=10.5)\n",
        "\n",
        "  #date_format = mdates.DateFormatter('%Y-%m-%d')\n",
        "  #ax1.xaxis.set_major_formatter(date_format)\n",
        "  plt.xticks(rotation=45)  # Rotate labels for better readability\n",
        "  plt.tight_layout()\n",
        "\n",
        "  ax1.set_ylabel(\"Feature\", fontname='Ubuntu', fontsize=14)\n",
        "\n",
        "  ax1.axhline(y=0, color='k',linewidth=1, alpha=1)\n",
        "  ax1.axhline(y=group.shape[1], color='k', alpha=1,linewidth=1)\n",
        "  ax1.axvline(x=0, color='k',linewidth=1, alpha=1)\n",
        "  ax1.axvline(x=group.shape[0], color='k', alpha=1,linewidth=1);\n",
        "\n",
        "  for i in np.arange(0,group.shape[0],60):\n",
        "    ax1.axvline(x=i, color='k', alpha=0.4,linewidth=1);\n",
        "  for i in np.arange(0,group.shape[1],1):\n",
        "    ax1.axhline(y=i, color='k', alpha=0.4,linewidth=1);\n",
        "  ax1.set_xticklabels([])\n",
        "\n",
        "  fig.savefig(f'example_heatmap.png', format='png', bbox_inches=\"tight\")\n",
        "  %download_file example_heatmap.png\n",
        "  plt.show()\n",
        "  break\n",
        "\n",
        "\n",
        "  '''\n",
        "  group = x[1]['input']\n",
        "  fig, ax = plt.subplots(figsize=(25,5))\n",
        "  x_lims = [group.index[0], group.index[-1]]\n",
        "  y_lims = [0, len(group.columns)]\n",
        "  im = ax.imshow(np.flip(np.flip(group.to_numpy()[0:-1,:].T,axis=0),axis=1), interpolation='nearest', aspect='auto', extent = [x_lims[0], x_lims[1],  y_lims[0], y_lims[1]])\n",
        "  ax.set_yticks(range(0,len(group.columns)))\n",
        "  ax.set_yticklabels(group.columns.to_list())\n",
        "\n",
        "  ax.axhline(y=0, color='k',linewidth=1)\n",
        "  ax.axhline(y=group.shape[1], color='k',linewidth=1)\n",
        "  ax.axvline(x=0, color='k',linewidth=1)\n",
        "  ax.axvline(x=group.shape[1], color='k',linewidth=1);\n",
        "\n",
        "  fig.colorbar(im)\n",
        "  plt.show()\n",
        "  '''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "m3MuXBCnCFKm",
        "CUtmxWeJyCb3",
        "BP_N0DKA3LTT",
        "5lfcwi0pJoEW",
        "BWOqq2WpimXj",
        "eKmmlf9UmOR6"
      ],
      "last_runtime": {
        "build_target": "//fitbit/research/sensing/dwb/colab:rl_colab",
        "kind": "shared"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/downstream_tasks/[LSM]_LSM_Preprocessing_DWB.ipynb",
          "timestamp": 1738038313662
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/downstream_tasks/[LSM]_LSM_Preprocessing_DWB.ipynb",
          "timestamp": 1737501986894
        },
        {
          "file_id": "1FlWXm9vMR5MaUHL8TsxWKR7c9EcLqfUW",
          "timestamp": 1735840284354
        },
        {
          "file_id": "11UdNvUeAS6o3tfrz49CE4UNNpPX3OygR",
          "timestamp": 1729539561855
        },
        {
          "file_id": "1yZ8pQR8l2O65aCH6xYYYyQVSDnxtp44F",
          "timestamp": 1710526257742
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
