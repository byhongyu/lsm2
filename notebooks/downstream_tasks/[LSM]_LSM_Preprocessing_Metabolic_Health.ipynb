{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNLa2VjaS7At"
      },
      "source": [
        "# Metabolic Health ARDS Permissions\n",
        "\n",
        "Prior to running the notebook, please request [AoD to Metabolic Heatch ARDS](https://grants.corp.google.com/#/grants?request=20h%2Fchr-ards-metabolichealth-deid-eng-team-sphinx:r\u0026reason=b%2F283774208)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qf6SvJU9ZtwL"
      },
      "outputs": [],
      "source": [
        "# @title Kernel Reproducibility\n",
        "from colabtools import kernelinfo\n",
        "_KERNEL = \"Ranklab (PY3) Metabolic Health Colab\"\n",
        "kernelinfo.show_info_links()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3MuXBCnCFKm"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e8d1tzwqK46"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pathlib\n",
        "import tarfile\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from google3.pyglib import gfile\n",
        "from colabtools import googlefiles\n",
        "import fnmatch\n",
        "import json\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import random\n",
        "from ast import literal_eval\n",
        "import datetime\n",
        "pd.set_option('display.max_columns', None)\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import zscore\n",
        "from statsmodels.graphics.gofplots import qqplot\n",
        "import random\n",
        "from scipy.stats import shapiro\n",
        "\n",
        "from collections.abc import Sequence\n",
        "import multiprocessing.pool\n",
        "import os\n",
        "from absl import app\n",
        "from google3.pyglib import gfile\n",
        "from time import sleep, time\n",
        "\n",
        "import multiprocessing\n",
        "import pdb\n",
        "from google3.pyglib import gfile\n",
        "# This option may not work if we are in a different runtime!\n",
        "gfile.LEGACY_GROUP_WRITABLE_WORLD_READABLE\n",
        "from time import sleep, time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import tensorflow as tf\n",
        "\n",
        "NORMALIZATION_PARAMETERS = {\n",
        "    'HR': [82.406911, 13.9461201],\n",
        "    'eda_level_real': [4.116634, 3.878952961],\n",
        "    'leads_contact_counts': [230.76297, 52.76303698],\n",
        "    'steps': [7.952935, 18.53001124],\n",
        "    'jerk_auto': [203.441044, 33.11101136],\n",
        "    'step_count': [11.440943, 15.95296346],\n",
        "    'log_energy': [60.306033, 42.84693899],\n",
        "    'covariance': [44.81157, 12.63844836],\n",
        "    'log_energy_ratio': [44.714925, 21.32527317],\n",
        "    'zero_crossing_std': [160.085565, 28.10161215],\n",
        "    'zero_crossing_avg': [51.270075, 34.04430198],\n",
        "    'axis_mean': [119.768427, 23.58453469],\n",
        "    'altim_std': [0.005178, 0.0581546286],\n",
        "    'kurtosis': [108.645938, 60.38419486],\n",
        "    'sleep_coefficient': [8.706734, 4.003582277],\n",
        "    'wrist_temperatures': [30.921362, 2.817617692],\n",
        "    'hrv_shannon_entropy_rr': [3.277522, 0.468409277],\n",
        "    'hrv_shannon_entropy_rrd': [2.974838, 0.4999503109],\n",
        "    'hrv_percentage_of_nn_30': [0.348379, 0.1961256813],\n",
        "    'ceda_magnitude_real_micro_siemens': [43.071381, 24.11546345],\n",
        "    'ceda_slope_real_micro_siemens': [3.294176, 1.828755314],\n",
        "    'rmssd_percentile_0595': [34.038394, 24.86136018],\n",
        "    'sdnn_percentile_0595': [44.233053, 25.04521794],\n",
        "    'msa_probability': [48.120677, 14.23343678],\n",
        "    'hrv_percent_good': [0.2716, 0.2760073968],\n",
        "    'hrv_rr_80th_percentile_mean': [821.738396, 105.621134],\n",
        "    'hrv_rr_20th_percentile_mean': [731.996986, 84.6384433],\n",
        "    'hrv_rr_median': [776.111350, 90.3199562],\n",
        "    'hrv_rr_mean': [781.280325, 87.08971004],\n",
        "    'hr_at_rest_mean': [83.199721, 10.66796299],\n",
        "    'skin_temperature_magnitude': [26.393339, 10.98900771],\n",
        "    'skin_temperature_slope': [0.267523, 17.79474941],\n",
        "}\n",
        "\n",
        "FEATURES_TO_INCLUDE = [\n",
        "    'HR',\n",
        "    'eda_level_real',\n",
        "    'leads_contact_counts',\n",
        "    'steps',\n",
        "    'jerk_auto',\n",
        "    'step_count',\n",
        "    'log_energy',\n",
        "    'covariance',\n",
        "    'log_energy_ratio',\n",
        "    'zero_crossing_std',\n",
        "    'zero_crossing_avg',\n",
        "    'axis_mean',\n",
        "    'altim_std',\n",
        "    'kurtosis',\n",
        "    'sleep_coefficient',\n",
        "    'wrist_temperatures',\n",
        "    'hrv_shannon_entropy_rr',\n",
        "    'hrv_shannon_entropy_rrd',\n",
        "    'hrv_percentage_of_nn_30',\n",
        "    'ceda_magnitude_real_micro_siemens',\n",
        "    'ceda_slope_real_micro_siemens',\n",
        "    'rmssd_percentile_0595',\n",
        "    'sdnn_percentile_0595',\n",
        "    'msa_probability',\n",
        "    'hrv_percent_good',\n",
        "    'hrv_rr_80th_percentile_mean',\n",
        "    'hrv_rr_20th_percentile_mean',\n",
        "    'hrv_rr_median',\n",
        "    'hrv_rr_mean',\n",
        "    'hr_at_rest_mean',\n",
        "    'skin_temperature_magnitude',\n",
        "    'skin_temperature_slope'\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6K7i2VQl1S_"
      },
      "outputs": [],
      "source": [
        "path_to_save_lsm_data = (\"/cns/yq-d/home/fitbit-medical-sandboxes/e=1:kid=76381\"\n",
        "                         \":mkey=cns-deid/chr-ards-metabolichealth/deid/exp/\"\n",
        "                         \"mdb=chr-ards-metabolichealth-deid-exp-decrypt/\"\n",
        "                         \"foundational_llm_research/large_sensor_model_data/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrgSKoBvk8P-"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "data.append({'type':  'steps',\n",
        "     'raw_file': 'STEPS_COMPACT_DATA',\n",
        "     'features_to_extract': ['steps'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "data.append({'type':  'momentary_stress_algorithm',\n",
        "     'raw_file': 'MOMENTARY_STRESS_ALGORITHM_DATA',\n",
        "     'features_to_extract': ['hrv_shannon_entropy_rr','hrv_shannon_entropy_rrd','hrv_percentage_of_nn_30','ceda_magnitude_real_micro_siemens','ceda_slope_real_micro_siemens','rmssd_percentile_0595','sdnn_percentile_0595','msa_probability','hrv_percent_good','hrv_rr_80th_percentile_mean','hrv_rr_20th_percentile_mean','hrv_rr_median','hrv_rr_mean','hr_at_rest_mean','skin_temperature_magnitude','skin_temperature_slope'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "data.append({'type':  'ceda',\n",
        "     'raw_file': 'CONTINUOUS_EDA_DATA',\n",
        "     'features_to_extract': ['eda_level_real','leads_contact_counts'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "data.append({'type':  'wrist_temperature',\n",
        "     'raw_file': 'WRIST_TEMPERATURE_DATA',\n",
        "     'features_to_extract': ['wrist_temperatures'],\n",
        "     'timezone_offset_column': 'tz_offset_minutes'})\n",
        "\n",
        "data.append({'type':  'sleep_coefficient',\n",
        "     'raw_file': 'SLEEP_COEFFICIENT_COMPACT_DATA',\n",
        "     'features_to_extract': ['sleep_coefficient','is_on_wrist'],\n",
        "     'timezone_offset_column': 'tz_offset_minutes'})\n",
        "\n",
        "data.append({'type':  'spo2',\n",
        "     'raw_file': 'ABSOLUTE_SPO2_DATA',\n",
        "     'features_to_extract': ['value','confidence','coverage','valid'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "data.append({'type':  'grok',\n",
        "     'raw_file': 'GROK_FEATURE_DATA',\n",
        "     'features_to_extract': ['jerk_auto','step_count','log_energy','covariance',\n",
        "                             'log_energy_ratio','zero_crossing_std',\n",
        "                             'zero_crossing_avg','axis_mean','altim_std','kurtosis'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "data.append({'type':  'heart_rate',\n",
        "     'raw_file': 'HEART_RATE_DATA',\n",
        "     'features_to_extract': ['bpm','confidence'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nujjJzhI4P9V"
      },
      "outputs": [],
      "source": [
        "tf_record_folder = \"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-metabolichealth/deid/exp/aliheydari/metabolic_tfrecords_weekly_v01\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP_N0DKA3LTT"
      },
      "source": [
        "# STEP 0: Move Data from Selected Snapshot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9eV4bhCI4d6"
      },
      "source": [
        "### Vico Surverys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ1xWxrXa16s"
      },
      "outputs": [],
      "source": [
        "gfile.MakeDirs(path_to_save_lsm_data)\n",
        "gfile.SetMode(path_to_save_lsm_data, 0o775)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPzVyh43qNXf"
      },
      "outputs": [],
      "source": [
        "## Import survey and phone data from vico files:\n",
        "vico_folder = pathlib.PurePosixPath('/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-metabolichealth/deid/raw/vico/marmara')\n",
        "vico_files = gfile.ListDir(vico_folder)\n",
        "with gfile.Open(vico_folder / vico_files[-1], 'rb') as f:\n",
        "  members = tarfile.open(fileobj=f, mode='r:gz').getmembers()\n",
        "survey_phone_data = {}\n",
        "with gfile.Open(vico_folder / vico_files[-1], 'rb') as f:\n",
        "  with tarfile.open(fileobj=f, mode='r:gz') as tf:\n",
        "    members = tf.getmembers()\n",
        "    for member in members:\n",
        "      files_excluded = ['battery_status_signals.csv']\n",
        "      if member.name in files_excluded:\n",
        "        continue\n",
        "      survey_phone_data[member.name] = (pd.read_csv(tf.extractfile(member), delimiter='\\t'))\n",
        "\n",
        "      with googlefiles.OpenGoogleFiles():\n",
        "        with open(path_to_save_lsm_data+member.name, 'w') as fs:\n",
        "          survey_phone_data[member.name].to_csv(fs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64IZZmk3I6-n"
      },
      "source": [
        "### Processing Raw Fitbit Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnSHtKg_JJRf"
      },
      "outputs": [],
      "source": [
        "fitbit_folder = pathlib.PurePosixPath('/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-metabolichealth/deid/raw/fitbit/marmara/')\n",
        "fitbit_snapshots = gfile.ListDir(fitbit_folder)\n",
        "fitbit_files = gfile.ListDir(fitbit_folder / fitbit_snapshots[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM_8EJbHJMsv"
      },
      "outputs": [],
      "source": [
        "subset_we_need = ['MSA_WINDOWS_DATA.csv',\n",
        " 'RUN_VO2_MAX_DATA.csv',\n",
        " 'SLEEP_COEFFICIENT_COMPACT_DATA.csv',\n",
        " 'SLEEP_SCORE_DATA.csv',\n",
        " 'SLEEP_STAGE_COMPACT_DATA.csv',\n",
        " 'STEPS_COMPACT_DATA.csv',\n",
        " 'USER_DATA.csv',\n",
        " 'USER_DEVICE_PAIRING_PERIOD_DATA.csv',\n",
        " 'WEIGHT_DATA.csv',\n",
        " 'WRIST_TEMPERATURE_DATA.csv']\n",
        "\n",
        "need_all_fitbit_files = False #@param [True, False] {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvPIMowKZnGV"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "fitbit_folder = pathlib.PurePosixPath('/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-metabolichealth/deid/raw/fitbit/marmara/')\n",
        "fitbit_snapshots = gfile.ListDir(fitbit_folder)\n",
        "fitbit_files = gfile.ListDir(fitbit_folder / fitbit_snapshots[-1])\n",
        "#fitbit_files = fnmatch.filter(fitbit_files, 'DAILY_*')\n",
        "#fitbit_files.append('STEPS_COMPACT_DATA.csv')\n",
        "#fitbit_files.append('SLEEP_SCORE_DATA.csv')\n",
        "fitbit_content = {}\n",
        "if need_all_fitbit_files:\n",
        "  print(\"Looping over a *ALL* Fitbit files\")\n",
        "\n",
        "  fitbit_files_to_loop = fitbit_files\n",
        "else:\n",
        "  print(\"Looping over a *SUBSET* of Fitbit files\")\n",
        "  fitbit_files_to_loop = subset_we_need\n",
        "for data_type in fitbit_files_to_loop:\n",
        "  with gfile.Open(fitbit_folder / fitbit_snapshots[-1] / data_type, 'r') as f:\n",
        "      fitbit_content[data_type] = pd.read_csv(f)\n",
        "\n",
        "      with googlefiles.OpenGoogleFiles():\n",
        "        with open(path_to_save_lsm_data+data_type+'.csv', 'w') as fs:\n",
        "          fitbit_content[data_type].to_csv(fs)\n",
        "\n",
        "print(f\"Total time took {time.time() - start} seconds\")\n",
        "\n",
        "#fitbit_content['STEPS_COMPACT_DATA.csv']['activity_time'] = pd.to_datetime(fitbit_content['STEPS_COMPACT_DATA.csv']['activity_time'])\n",
        "#fitbit_content['STEPS_COMPACT_DATA.csv']['date'] = fitbit_content['STEPS_COMPACT_DATA.csv']['activity_time'].dt.date\n",
        "#print(len(fitbit_content['STEPS_COMPACT_DATA.csv']))\n",
        "#participant_timezone = fitbit_content['STEPS_COMPACT_DATA.csv'].drop_duplicates(subset=['participant_id','date'], keep='first')\n",
        "#participant_timezone\n",
        "#print(len(participant_timezone))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lfcwi0pJoEW"
      },
      "source": [
        "# STEP 1: Prepare Individual Participant Sessions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELzc_gOjCt7r"
      },
      "outputs": [],
      "source": [
        "for d in data:\n",
        "  print(\"BY_SUBJECT_\"+d['type'], len(gfile.ListDir(os.path.join(path_to_save_lsm_data, \"BY_SUBJECT_\"+d['type']))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_PlJMjLmrO1"
      },
      "outputs": [],
      "source": [
        "# @title Listing data available in root path\n",
        "root_folder = path_to_save_lsm_data\n",
        "gfile.ListDir(root_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfO0_lhlRj-z"
      },
      "outputs": [],
      "source": [
        "def get_arrays(row,column,type=float):\n",
        "  list_of_strings = row[column]\n",
        "  list_of_strings = list_of_strings[1:-1]\n",
        "  list_of_integers = list_of_strings.split(',')\n",
        "  series = pd.Series(list_of_integers)\n",
        "  if type == bool:\n",
        "    return series.astype(bool)\n",
        "\n",
        "  try:\n",
        "    return series.astype(float)\n",
        "  except:\n",
        "    print(\"Could not convert millis to float in array conversion.\")\n",
        "    return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYOg06dXQbwX"
      },
      "outputs": [],
      "source": [
        "gfile.SetMode(\"/cns/yq-d/home/fitbit-medical-sandboxes/e=1:kid=76381:mkey=cns-deid/chr-ards-metabolichealth/deid/exp/mdb=chr-ards-metabolichealth-deid-exp-decrypt/foundational_llm_research/large_sensor_model_data/BY_SUBJECT_ceda/\", 0o775)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jN0f1Jt6rEa_"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "\n",
        "# These are already done\n",
        "\n",
        "data.append({'type':  'heart_rate',\n",
        "     'raw_file': 'HEART_RATE_DATA',\n",
        "     'features_to_extract': ['bpm','confidence'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "data.append({'type':  'steps',\n",
        "     'raw_file': 'STEPS_COMPACT_DATA',\n",
        "     'features_to_extract': ['steps'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "data.append({'type':  'wrist_temperature',\n",
        "     'raw_file': 'WRIST_TEMPERATURE_DATA',\n",
        "     'features_to_extract': ['wrist_temperatures'],\n",
        "     'timezone_offset_column': 'tz_offset_minutes'})\n",
        "\n",
        "data.append({'type':  'sleep_coefficient',\n",
        "     'raw_file': 'SLEEP_COEFFICIENT_COMPACT_DATA',\n",
        "     'features_to_extract': ['sleep_coefficient','is_on_wrist'],\n",
        "     'timezone_offset_column': 'tz_offset_minutes'})\n",
        "\n",
        "data.append({'type':  'momentary_stress_algorithm',\n",
        "     'raw_file': 'MOMENTARY_STRESS_ALGORITHM_DATA',\n",
        "     'features_to_extract': ['hrv_shannon_entropy_rr','hrv_shannon_entropy_rrd','hrv_percentage_of_nn_30','ceda_magnitude_real_micro_siemens','ceda_slope_real_micro_siemens','rmssd_percentile_0595','sdnn_percentile_0595','msa_probability','hrv_percent_good','hrv_rr_80th_percentile_mean','hrv_rr_20th_percentile_mean','hrv_rr_median','hrv_rr_mean','hr_at_rest_mean','skin_temperature_magnitude','skin_temperature_slope'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "data.append({'type':  'ceda',\n",
        "     'raw_file': 'CONTINUOUS_EDA_DATA',\n",
        "     'features_to_extract': ['eda_level_real','leads_contact_counts'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "data.append({'type':  'spo2',\n",
        "     'raw_file': 'ABSOLUTE_SPO2_DATA',\n",
        "     'features_to_extract': ['value','confidence','coverage','valid'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "data.append({'type':  'grok',\n",
        "     'raw_file': 'GROK_FEATURE_DATA',\n",
        "     'features_to_extract': ['jerk_auto','step_count','log_energy','covariance',\n",
        "                             'log_energy_ratio','zero_crossing_std',\n",
        "                             'zero_crossing_avg','axis_mean','altim_std','kurtosis'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Wo_Nu_6R0_f"
      },
      "outputs": [],
      "source": [
        "# @title All Files\n",
        "\n",
        "def split_save(L):\n",
        "  df2 = df[df['participant_id']==L]\n",
        "\n",
        "  if gfile.Exists(os.path.join(target_dir,d['type']+\"_\"+str(L)+\".csv\")):\n",
        "    print(L, ' Already exists.')\n",
        "    return\n",
        "\n",
        "  list_arrays = []\n",
        "\n",
        "  ## Loop through raw file and de-nest:\n",
        "  if d['type'] == 'spo2':\n",
        "    df2['activity_time'] = pd.to_datetime(df2['activity_time'], format='mixed')\n",
        "    df2['valid'] = df2['valid'].astype('int')\n",
        "    tmp = df2\n",
        "    tmp['millis_from_start_time'] = 0\n",
        "  else:\n",
        "    for i, row in df2.iterrows():\n",
        "\n",
        "      tmp = pd.DataFrame()\n",
        "\n",
        "      if d['type'] == 'steps' or d['type'] == 'wrist_temperature' or d['type'] == 'sleep_coefficient':\n",
        "        tmp['millis_from_start_time'] = pd.Series(range(0,1440))*1000*60\n",
        "      if d['type'] == 'heart_rate':\n",
        "        tmp['millis_from_start_time'] = pd.Series(range(0,60*60*24))*1000\n",
        "      if d['type'] == 'momentary_stress_algorithm':\n",
        "        tmp['millis_from_start_time'] = get_arrays(row,'offsets')*1000*60\n",
        "      if d['type'] == 'ceda':\n",
        "          tmp['millis_from_start_time'] = get_arrays(row,'millis_from_start_time')\n",
        "      if d['type'] == 'grok':\n",
        "          tmp['millis_from_start_time'] = get_arrays(row,'millis_from_start_of_day')\n",
        "\n",
        "      for feature in d['features_to_extract']:\n",
        "        if feature == 'is_on_wrist':\n",
        "          tmp[feature] = get_arrays(row,feature,bool)\n",
        "        else:\n",
        "          tmp[feature] = get_arrays(row,feature)\n",
        "      tmp['activity_time'] = row['activity_time']\n",
        "      list_arrays.append(tmp)\n",
        "\n",
        "    tmp = pd.concat(list_arrays, ignore_index=True)\n",
        "\n",
        "    tmp['participant_id'] = row['participant_id']\n",
        "    tmp['activity_tm_timezone_offset'] = row[d['timezone_offset_column']]\n",
        "\n",
        "    ## Convert time to LOCAL:\n",
        "    tmp['activity_time'] = pd.to_datetime(tmp['activity_time'])\n",
        "\n",
        "    if d['type'] == 'ceda':\n",
        "      tmp['activity_time_local'] = tmp['activity_time'] + tmp['activity_tm_timezone_offset'].astype('timedelta64[m]') + tmp['activity_tm_timezone_offset'].astype('timedelta64[m]') + tmp['millis_from_start_time'].astype('timedelta64[ms]')\n",
        "    else:\n",
        "      tmp['activity_time_local'] = tmp['activity_time'] + tmp['activity_tm_timezone_offset'].astype('timedelta64[m]') + tmp['millis_from_start_time'].astype('timedelta64[ms]')\n",
        "\n",
        "    ## Rename columns:\n",
        "    tmp.rename(columns={'activity_time_local': 'DT', 'participant_id': 'ID'}, inplace=True)\n",
        "\n",
        "    cols = d['features_to_extract'].copy()\n",
        "    cols.append('ID')\n",
        "    cols.append('DT')\n",
        "    tmp = tmp[cols]\n",
        "\n",
        "  tmp.to_csv(gfile.Open(os.path.join(target_dir,d['type']+\"_\"+str(L)+\".csv\"), 'w'))\n",
        "  print(L, ' successfully saved.')\n",
        "\n",
        "for d in data:\n",
        "\n",
        "  print(d['type'])\n",
        "\n",
        "  ## Load raw file:\n",
        "  with gfile.Open(os.path.join(root_folder, d['raw_file']+'.csv.csv'), 'r') as f:\n",
        "    df = pd.read_csv(f)\n",
        "  df.reset_index(inplace=True)\n",
        "\n",
        "  if len(df) == 0:\n",
        "    continue\n",
        "\n",
        "  ## Make output dir and save per subject files:\n",
        "  target_dir = os.path.join(root_folder,\"BY_SUBJECT_\"+d['type'])\n",
        "  if gfile.Exists(target_dir):\n",
        "    print(target_dir)\n",
        "    #gfile.DeleteRecursively(target_dir)\n",
        "    #gfile.MakeDirs(target_dir)\n",
        "  else:\n",
        "    gfile.MakeDirs(target_dir)\n",
        "    gfile.SetMode(target_dir, 0o775)\n",
        "\n",
        "  #split_save(19395)\n",
        "\n",
        "  WORKER_COUNT = 20\n",
        "  L = pd.unique(df.participant_id)\n",
        "  with multiprocessing.pool.ThreadPool(WORKER_COUNT) as pool:\n",
        "    output = list(pool.map(split_save, L))\n",
        "    pool.close()\n",
        "    pool.join()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYXb5bIQSZIl"
      },
      "outputs": [],
      "source": [
        "root_folder = path_to_save_lsm_data\n",
        "root_folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJLhub57rSSx"
      },
      "outputs": [],
      "source": [
        "for d in data:\n",
        "  print(d['type'], len(gfile.ListDir(os.path.join(root_folder, \"BY_SUBJECT_\"+d['type']))))\n",
        "  # Set permission so that they are globally viewable in ARDS\n",
        "  gfile.SetMode(os.path.join(root_folder, \"BY_SUBJECT_\"+d['type']), 0o775)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWOqq2WpimXj"
      },
      "source": [
        "# STEP 2: Sessionize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvedVd94Yxl-"
      },
      "outputs": [],
      "source": [
        "root_folder = path_to_save_lsm_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NSM9h6ij1rB7"
      },
      "outputs": [],
      "source": [
        "#@title Data session class definition\n",
        "\n",
        "import abc\n",
        "import dataclasses\n",
        "import functools as ft\n",
        "import jaxtyping as jt\n",
        "from scipy.stats import zscore\n",
        "\n",
        "class Sensor(abc.ABC):\n",
        "\n",
        "  def resample(timeseries_data, input_timestamp_units='s', output_timestamp_units='1min'):\n",
        "    \"\"\"Downsamples a pandas dataframe with unknown frequency into a minutely frequency, using the column 't'.\n",
        "\n",
        "    Args:\n",
        "      timeseries_data: A pandas dataframe with a column 't' of timestamps to use\n",
        "      for downsampling.\n",
        "      timestamp_units: The units to use for the timestamps.\n",
        "\n",
        "    Returns:\n",
        "      A pandas dataframe with a minutely frequency.\n",
        "    \"\"\"\n",
        "\n",
        "    timeseries_data['DT'] = pd.to_datetime(\n",
        "        timeseries_data['t'], unit=input_timestamp_units\n",
        "    )\n",
        "    timeseries_data.drop(columns=['t'], inplace=True)\n",
        "    timeseries_data = timeseries_data.resample(output_timestamp_units, on='DT').mean()\n",
        "    return timeseries_data\n",
        "\n",
        "\n",
        "class HeartRate(Sensor):\n",
        "  \"\"\"Heart rate sensor data.\"\"\"\n",
        "\n",
        "  sessions: list\n",
        "\n",
        "  def __init__(self, data, sensor_key,input_timestamp_units='s', output_timestamp_units='1min'):\n",
        "    if sensor_key in data.data.keys():\n",
        "      sessions = data.data[sensor_key]\n",
        "      hr = []\n",
        "      for session in sessions:\n",
        "        times = pd.date_range(datetime.datetime.fromtimestamp(session.activity_tm.seconds, tz=datetime.timezone.utc), periods=60*60*24, freq='1s')\n",
        "        hr_day = pd.DataFrame({'t': times, 'HR': session.bpm})\n",
        "        hr.append(hr_day)\n",
        "      self.hr = Sensor.resample(pd.concat(hr), input_timestamp_units='s', output_timestamp_units='1min')\n",
        "    else:\n",
        "      self.hr = pd.DataFrame(columns=['DT',\n",
        "                                      'HR']).set_index('DT')\n",
        "      print(ValueError(sensor_key + ' not found in data.data.keys()'))\n",
        "\n",
        "\n",
        "\n",
        "class ContinuousEDA(Sensor):\n",
        "  \"\"\"Continuous EDA sensor data.\"\"\"\n",
        "  sessions: list\n",
        "\n",
        "  def __init__(self, data, sensor_key, input_timestamp_units='s', output_timestamp_units='1min'):\n",
        "\n",
        "    if sensor_key in data.data.keys():\n",
        "      sessions = data.data[sensor_key]\n",
        "      continuous_eda = []\n",
        "      for session in sessions:\n",
        "        t = []\n",
        "        for i in session.millis_from_start_time:\n",
        "          t.append(datetime.datetime.fromtimestamp(i/1000 + session.activity_tm_timezone_offset*60 + session.activity_tm.seconds, tz=datetime.timezone.utc))\n",
        "        times = pd.DatetimeIndex(t)\n",
        "        continuous_eda_day = pd.DataFrame({'t': times,\n",
        "                                          'eda_level_real': session.eda_level_real,\n",
        "                                          'eda_level_imaginary': session.eda_level_imaginary,\n",
        "                                          'eda_slope_real': session.eda_slope_real,\n",
        "                                          'eda_slope_imaginary': session.eda_slope_imaginary,\n",
        "                                          'leads_contact_counts': session.leads_contact_counts})\n",
        "        continuous_eda.append(continuous_eda_day)\n",
        "      self.continuous_eda = Sensor.resample(pd.concat(continuous_eda), input_timestamp_units='s', output_timestamp_units='1min')\n",
        "    else:\n",
        "      self.continuous_eda = pd.DataFrame(columns=['DT',\n",
        "                                                  'eda_level_real',\n",
        "                                                  'eda_level_imaginary',\n",
        "                                                  'eda_slope_real',\n",
        "                                                  'eda_slope_imaginary',\n",
        "                                                  'leads_contact_counts']).set_index('DT')\n",
        "      print(ValueError(sensor_key + ' not found in data.data.keys()'))\n",
        "\n",
        "class Steps(Sensor):\n",
        "  \"\"\"Steps sensor data.\"\"\"\n",
        "  sessions: list\n",
        "\n",
        "  def __init__(self, data, sensor_key, input_timestamp_units='s', output_timestamp_units='1min'):\n",
        "    if sensor_key in data.data.keys():\n",
        "      sessions = data.data[sensor_key]\n",
        "      steps = []\n",
        "      for session in sessions:\n",
        "        times = pd.date_range(datetime.datetime.fromtimestamp(session.activity_tm.seconds, tz=datetime.timezone.utc), periods=60*24, freq='1min')\n",
        "        steps_day = pd.DataFrame({'t': times, 'steps': session.steps})\n",
        "        steps.append(steps_day)\n",
        "      self.steps = Sensor.resample(pd.concat(steps), input_timestamp_units='s', output_timestamp_units='1min')\n",
        "    else:\n",
        "      self.steps = pd.DataFrame(columns=['DT',\n",
        "                                         'steps']).set_index('DT')\n",
        "      print(ValueError(sensor_key + ' not found in data.data.keys()'))\n",
        "\n",
        "\n",
        "class Grok(Sensor):\n",
        "  \"\"\"Grok sensor data.\"\"\"\n",
        "  sessions: list\n",
        "\n",
        "  def __init__(self, data, sensor_key, input_timestamp_units='s', output_timestamp_units='1min'):\n",
        "\n",
        "    if sensor_key in data.data.keys():\n",
        "      sessions = data.data[sensor_key]\n",
        "      grok = []\n",
        "      for session in data.data['grok_feature_data_with_dupes']:\n",
        "        t = []\n",
        "        for i in session.activity_tms:\n",
        "          t.append(datetime.datetime.fromtimestamp(i.seconds, tz=datetime.timezone.utc))\n",
        "        times = pd.DatetimeIndex(t)\n",
        "        grok_day = pd.DataFrame({'t': times,\n",
        "                                'jerk_auto': session.jerk_auto,\n",
        "                                'step_count': session.step_count,\n",
        "                                'log_energy': session.log_energy,\n",
        "                                'covariance': session.covariance,\n",
        "                                'log_energy_ratio': session.log_energy_ratio,\n",
        "                                'zero_crossing_std': session.zero_crossing_std,\n",
        "                                'zero_crossing_avg': session.zero_crossing_avg,\n",
        "                                'axis_mean': session.axis_mean,\n",
        "                                'altim_std': session.altim_std,\n",
        "                                'kurtosis': session.kurtosis})\n",
        "        grok.append(grok_day)\n",
        "      self.grok = Sensor.resample(pd.concat(grok), input_timestamp_units='s', output_timestamp_units='1min')\n",
        "    else:\n",
        "      self.grok = pd.DataFrame(columns=['DT',\n",
        "                                        'jerk_auto',\n",
        "                                        'step_count',\n",
        "                                        'log_energy',\n",
        "                                        'covariance',\n",
        "                                        'log_energy_ratio',\n",
        "                                        'zero_crossing_std',\n",
        "                                        'zero_crossing_avg',\n",
        "                                        'axis_mean',\n",
        "                                        'altim_std',\n",
        "                                        'kurtosis']).set_index('DT')\n",
        "      print(ValueError(sensor_key + ' not found in data.data.keys()'))\n",
        "\n",
        "class SleepCoefficient(Sensor):\n",
        "  \"\"\"Sleep coefficient sensor data.\"\"\"\n",
        "  sessions: list\n",
        "\n",
        "  def __init__(self, data, sensor_key, input_timestamp_units='s', output_timestamp_units='1min'):\n",
        "    if sensor_key in data.data.keys():\n",
        "      sessions = data.data[sensor_key]\n",
        "      sleep_coefficient = []\n",
        "      for session in data.data['sleep_coefficient_compact']:\n",
        "        times = pd.date_range(datetime.datetime.fromtimestamp(session.activity_tm.seconds, tz=datetime.timezone.utc), periods=60*24*2, freq='30s')\n",
        "        sleep_coefficient_day = pd.DataFrame({'t': times,\n",
        "                                              'sleep_coefficient': session.sleep_coefficient,\n",
        "                                              'is_on_wrist': session.is_on_wrist})\n",
        "        sleep_coefficient.append(sleep_coefficient_day)\n",
        "      self.sleep_coefficient = Sensor.resample(pd.concat(sleep_coefficient), input_timestamp_units='s', output_timestamp_units='1min')\n",
        "    else:\n",
        "      self.sleep_coefficient = pd.DataFrame(columns=['DT',\n",
        "                                        'sleep_coefficient',\n",
        "                                        'is_on_wrist']).set_index('DT')\n",
        "      print(ValueError(sensor_key + ' not found in data.data.keys()'))\n",
        "\n",
        "class SkinTemp(Sensor):\n",
        "  \"\"\"Skin temperature sensor data.\"\"\"\n",
        "  sessions: list\n",
        "\n",
        "  def __init__(self, data, sensor_key, input_timestamp_units='s', output_timestamp_units='1min'):\n",
        "    if sensor_key in data.data.keys():\n",
        "      sessions = data.data[sensor_key]\n",
        "      skin_temp = []\n",
        "      for session in sessions:\n",
        "        times = pd.date_range(datetime.datetime.fromtimestamp(session.activity_tm.seconds, tz=datetime.timezone.utc), periods=60*24, freq='1min')\n",
        "        skintemp_day = pd.DataFrame({'t': times, 'wrist_temperatures': session.wrist_temperatures})\n",
        "        skin_temp.append(skintemp_day)\n",
        "      self.skin_temp = Sensor.resample(pd.concat(skin_temp), input_timestamp_units='s', output_timestamp_units='1min')\n",
        "    else:\n",
        "      self.skin_temp = pd.DataFrame(columns=['DT',\n",
        "                                        'wrist_temperatures']).set_index('DT')\n",
        "      print(ValueError(sensor_key + ' not found in data.data.keys()'))\n",
        "\n",
        "class MomentaryStressAlgorithm(Sensor):\n",
        "  \"\"\"Momentary stress algorithm sensor data.\"\"\"\n",
        "  sessions: list\n",
        "\n",
        "  def __init__(self, data, sensor_key, input_timestamp_units='s', output_timestamp_units='1min'):\n",
        "\n",
        "    if sensor_key in data.data.keys():\n",
        "      sessions = data.data[sensor_key]\n",
        "      momentary_stress_algorithm = []\n",
        "      for session in sessions:\n",
        "        t = []\n",
        "        for i in session.offsets:\n",
        "          t.append(datetime.datetime.fromtimestamp(i*60 + session.activity_tm.seconds, tz=datetime.timezone.utc))\n",
        "        times = pd.DatetimeIndex(t)\n",
        "        msa_day = pd.DataFrame({'t': times,\n",
        "                                'hrv_shannon_entropy_rr': session.hrv_shannon_entropy_rr,\n",
        "                                'hrv_shannon_entropy_rrd': session.hrv_shannon_entropy_rrd,\n",
        "                                'hrv_percentage_of_nn_30': session.hrv_percentage_of_nn_30,\n",
        "                                'ceda_magnitude_real_micro_siemens': session.ceda_magnitude_real_micro_siemens,\n",
        "                                'ceda_slope_real_micro_siemens': session.ceda_slope_real_micro_siemens,\n",
        "                                'rmssd_percentile_0595': session.rmssd_percentile_0595,\n",
        "                                'sdnn_percentile_0595': session.sdnn_percentile_0595,\n",
        "                                'msa_probability': session.msa_probability,\n",
        "                                'hrv_percent_good': session.hrv_percent_good,\n",
        "                                'hrv_rr_80th_percentile_mean': session.hrv_rr_80th_percentile_mean,\n",
        "                                'hrv_rr_20th_percentile_mean': session.hrv_rr_20th_percentile_mean,\n",
        "                                'hrv_rr_median': session.hrv_rr_median,\n",
        "                                'hrv_rr_mean': session.hrv_rr_mean,\n",
        "                                'hr_at_rest_mean': session.hr_at_rest_mean,\n",
        "                                'skin_temperature_magnitude': session.skin_temperature_magnitude,\n",
        "                                'skin_temperature_slope': session.skin_temperature_slope})\n",
        "        momentary_stress_algorithm.append(msa_day)\n",
        "      self.momentary_stress_algorithm = Sensor.resample(pd.concat(momentary_stress_algorithm), input_timestamp_units='s', output_timestamp_units='1min')\n",
        "    else:\n",
        "      self.momentary_stress_algorithm = pd.DataFrame(columns=['DT',\n",
        "                                                              'hrv_shannon_entropy_rr',\n",
        "                                                              'hrv_shannon_entropy_rrd',\n",
        "                                                              'hrv_percentage_of_nn_30',\n",
        "                                                              'ceda_magnitude_real_micro_siemens',\n",
        "                                                              'ceda_slope_real_micro_siemens',\n",
        "                                                              'rmssd_percentile_0595',\n",
        "                                                              'sdnn_percentile_0595',\n",
        "                                                              'msa_probability',\n",
        "                                                              'hrv_percent_good',\n",
        "                                                              'hrv_rr_80th_percentile_mean',\n",
        "                                                              'hrv_rr_20th_percentile_mean',\n",
        "                                                              'hrv_rr_median',\n",
        "                                                              'hrv_rr_mean',\n",
        "                                                              'hr_at_rest_mean',\n",
        "                                                              'skin_temperature_magnitude',\n",
        "                                                              'skin_temperature_slope']).set_index('DT')\n",
        "\n",
        "    print(ValueError(sensor_key + ' not found in data.data.keys()'))\n",
        "\n",
        "\n",
        "@dataclasses.dataclass(frozen=True)\n",
        "class ProdSession:\n",
        "\n",
        "  # A session specific identifier for a 24hr period of data collection.\n",
        "  session_id: str\n",
        "  # Heart rate table data.\n",
        "  hr: HeartRate\n",
        "  # Continuous heart rate table data.\n",
        "  continuous_eda: ContinuousEDA\n",
        "  # Steps table data.\n",
        "  steps: Steps\n",
        "  # Grok table data.\n",
        "  grok: Grok\n",
        "  # Sleep Coefficient table data.\n",
        "  sleep_coefficient: SleepCoefficient\n",
        "  # Skin Temp table data.\n",
        "  skin_temp: SkinTemp\n",
        "  # MSA table data.\n",
        "  momentary_stress_algorithm: MomentaryStressAlgorithm\n",
        "\n",
        "  def join(self) -\u003e pd.DataFrame:\n",
        "\n",
        "    dfs = [self.hr, self.continuous_eda, self.steps, self.grok, self.sleep_coefficient, self.skin_temp, self.momentary_stress_algorithm]\n",
        "    session = ft.reduce(lambda left, right: pd.merge(left, right, on='DT', how='outer'), dfs)\n",
        "    if 'is_on_wrist' in session.columns:\n",
        "      session.loc[(session.is_on_wrist == 0), :] = np.nan\n",
        "      session = session.apply(lambda col: zscore(col, nan_policy='omit') if col.notna().any() else col)\n",
        "      session = session.clip(-3,3)\n",
        "      return session\n",
        "    else:\n",
        "      return pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdK1aFzJjLyd"
      },
      "source": [
        "# Step 3: Load and Process Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wAdTbwB8a0MB"
      },
      "outputs": [],
      "source": [
        "# @title New: Metabolic Health Labels (Pending Review by Daniel)\n",
        "\n",
        "class MetHealthLabels:\n",
        "  \"\"\"MetHealthLabels sensor data.\"\"\"\n",
        "\n",
        "  def __init__(self, path_to_labels_and_scores:str=(\n",
        "      \"/namespace/fitbit-medical-sandboxes/partner/encrypted/\"\n",
        "      \"chr-ards-metabolichealth/deid/exp/wear_me_study/extracted_features/\"\n",
        "      \"labels_and_scores.pkl\")):\n",
        "    with gfile.Open(path_to_labels_and_scores, 'rb') as f:\n",
        "      self.labels_and_scores_df = pd.read_pickle(f)\n",
        "    self.labels_and_scores_df.dropna(subset=\"participant_id\", inplace=True)\n",
        "    self.labels_and_scores_df['participant_id'] = self.labels_and_scores_df['participant_id'].astype(int)\n",
        "    self.labels_and_scores_df.set_index('participant_id', inplace=True)\n",
        "    # The threshold for HOMA-IR comes from our manuscript\n",
        "    self.labels_and_scores_df['ir_binary'] = (self.labels_and_scores_df['homa_ir'] \u003e 2.9).astype(int)\n",
        "    # Threshold for MSSS comes from https://diabetesjournals.org/care/article/41/11/2421/36563/Use-of-a-Metabolic-Syndrome-Severity-Z-Score-to\n",
        "    self.labels_and_scores_df['msss_binary'] = (self.labels_and_scores_df['msss'] \u003e 0).astype(int)\n",
        "\n",
        "    self.age = self.labels_and_scores_df.loc[:, ['age']].to_dict()['age']\n",
        "    # Unfortunately, we did not ask user's for bilogical sex, so \"sex\" column is\n",
        "    # actually gender\n",
        "    self.gender = self.labels_and_scores_df.loc[:, ['sex']].to_dict()['sex']\n",
        "    # Cardiometabolic scores\n",
        "    self.bmi = self.labels_and_scores_df.loc[:, ['bmi']].to_dict()['bmi']\n",
        "    self.homa_ir = self.labels_and_scores_df.loc[:, ['homa_ir']].to_dict()['homa_ir']\n",
        "    self.homa_ir_binary = self.labels_and_scores_df.loc[:, ['ir_binary']].to_dict()['ir_binary']\n",
        "    self.msss = self.labels_and_scores_df.loc[:, ['msss']].to_dict()['msss']\n",
        "    self.msss_binary = self.labels_and_scores_df.loc[:, ['msss_binary']].to_dict()['msss_binary']\n",
        "    self.framingham_risk_category = self.labels_and_scores_df.loc[:, ['framingham_risk_category']].to_dict()['framingham_risk_category']\n",
        "    # APRI (AST to Platelet Ratio Index) is a score realted to liver health\n",
        "    self.apri = self.labels_and_scores_df.loc[:, ['apri']].to_dict()['apri']\n",
        "    # Since we want to have hypertension as a binary label, we convert all NaNs\n",
        "    # to -1.\n",
        "    self.labels_and_scores_df['hypertension'].fillna(-1, inplace=True)\n",
        "    self.hypertension = self.labels_and_scores_df['hypertension'].astype(int)\n",
        "    self.hyperlipidemia = self.labels_and_scores_df.loc[:, ['hyperlipidemia']].to_dict()['hyperlipidemia']\n",
        "    self.cardiovascular_condition = self.labels_and_scores_df.loc[:, ['CVD']].to_dict()['CVD']\n",
        "    self.diabetes_condition = self.labels_and_scores_df.loc[:, ['diabetes']].to_dict()['diabetes']\n",
        "    self.diabetes_type = self.labels_and_scores_df.loc[:, ['diabetes_type']].to_dict()['diabetes_type']\n",
        "    self.anxiety_condition = self.labels_and_scores_df.loc[:, ['anxiety']].to_dict()['anxiety']\n",
        "    self.respiratory_condition = self.labels_and_scores_df.loc[:, ['respiratory']].to_dict()['respiratory']\n",
        "    self.kidney_condition = self.labels_and_scores_df.loc[:, ['kidney Disease']].to_dict()['kidney Disease']\n",
        "    self.medication = self.labels_and_scores_df.loc[:, ['medications']].to_dict()['medications']\n",
        "    self.regular_mensturation = self.labels_and_scores_df.loc[:, ['regular_periods']].to_dict()['regular_periods']\n",
        "    self.smoking = self.labels_and_scores_df.loc[:, ['smoker']].to_dict()['smoker']\n",
        "    self.alcohol_consumption = self.labels_and_scores_df.loc[:, ['alcohol']].to_dict()['alcohol']\n",
        "\n",
        "  def get_individual_age(self, participant_id:int):\n",
        "    try:\n",
        "      return self.age[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_individual_gender(self, participant_id:int):\n",
        "    try:\n",
        "      return self.gender[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_individual_bmi(self, participant_id:int):\n",
        "    try:\n",
        "      return self.bmi[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_individual_homa_ir(self, participant_id:int):\n",
        "    try:\n",
        "      return self.homa_ir[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_individual_homa_ir_binary(self, participant_id:int):\n",
        "    try:\n",
        "      return self.homa_ir_binary[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_individual_msss(self, participant_id:int):\n",
        "    try:\n",
        "      return self.msss[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_individual_msss_binary(self, participant_id:int):\n",
        "    try:\n",
        "      return self.msss_binary[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_individual_framingham_risk_category(self, participant_id:int):\n",
        "    try:\n",
        "      return self.framingham_risk_category[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_individual_apri(self, participant_id:int):\n",
        "    try:\n",
        "      return self.apri[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_individual_hypertension(self, participant_id:int):\n",
        "    try:\n",
        "      return self.hypertension[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_individual_cardiovascular_condition(self, participant_id:int):\n",
        "    try:\n",
        "      return self.cardiovascular_condition[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_individual_diabetes_condition(self, participant_id:int):\n",
        "    try:\n",
        "      return self.diabetes_condition[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_individual_anxiety_condition(self, participant_id:int):\n",
        "    try:\n",
        "      return self.anxiety_condition[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_individual_respiratory_condition(self, participant_id:int):\n",
        "    try:\n",
        "      return self.respiratory_condition[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_individual_kidney_condition(self, participant_id:int):\n",
        "    try:\n",
        "      return self.kidney_condition[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_individual_medications(self, participant_id:int):\n",
        "    try:\n",
        "      return self.medication[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_individual_regular_mensturation(self, participant_id:int):\n",
        "    try:\n",
        "      return self.regular_mensturation[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_individual_hyperlipidemia(self, participant_id:int):\n",
        "    try:\n",
        "      return self.hyperlipidemia[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_individual_diabetes_type(self, participant_id:int):\n",
        "    try:\n",
        "      return self.diabetes_type[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_individual_smoking(self, participant_id:int):\n",
        "    try:\n",
        "      return self.smoking[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_individual_alcohol_consumption(self, participant_id:int):\n",
        "    try:\n",
        "      return self.alcohol_consumption[participant_id]\n",
        "    except KeyError:\n",
        "      return np.nan\n",
        "\n",
        "  def get_complete_labels_and_scores(self):\n",
        "    return self.labels_and_scores_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "89mJmNdA87nt"
      },
      "outputs": [],
      "source": [
        "# @title Sanity check: Checking how many particpants could have labels\n",
        "\n",
        "files = gfile.ListDir(os.path.join(root_folder,\"BY_SUBJECT_momentary_stress_algorithm\"))\n",
        "ids = [s[-9:] for s in files]\n",
        "\n",
        "obj = MetHealthLabels()\n",
        "df = obj.get_complete_labels_and_scores()\n",
        "df['participant_id'] = df.index\n",
        "\n",
        "total_count = 0\n",
        "num_available = 0\n",
        "\n",
        "for id in ids:\n",
        "  total_count+=1\n",
        "  participant_id = int(id.split(\".\")[0])\n",
        "  current_df = df[df['participant_id']==participant_id]\n",
        "  if len(current_df)\u003e0:\n",
        "    num_available+=1\n",
        "\n",
        "print(f\"Percentage of available potential labels: {(num_available/total_count)*100}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7OqmQ4IyXoSV"
      },
      "outputs": [],
      "source": [
        "# @title Sanity check: Checking how many labels are non-null\n",
        "obj = MetHealthLabels()\n",
        "df = obj.get_complete_labels_and_scores()\n",
        "df['participant_id'] = df.index\n",
        "\n",
        "files = gfile.ListDir(os.path.join(root_folder,\"BY_SUBJECT_momentary_stress_algorithm\"))\n",
        "ids = [s[-9:] for s in files]\n",
        "\n",
        "total_count = 0\n",
        "num_available = 0\n",
        "\n",
        "for id in ids:\n",
        "  total_count+=1\n",
        "  participant_id = int(id.split(\".\")[0])\n",
        "  bmi = obj.get_individual_bmi(participant_id)\n",
        "  if bmi \u003e0:\n",
        "    num_available+=1\n",
        "\n",
        "print(f\"Percentage of non-null labels: {(num_available/total_count)*100}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zRpxzaT0RvUJ"
      },
      "outputs": [],
      "source": [
        "# @title Sanity check: Checking how many labels are non-null\n",
        "obj = MetHealthLabels()\n",
        "df = obj.get_complete_labels_and_scores()\n",
        "df['participant_id'] = df.index\n",
        "\n",
        "files = gfile.ListDir(os.path.join(root_folder,\"BY_SUBJECT_momentary_stress_algorithm\"))\n",
        "ids = [s[-9:] for s in files]\n",
        "\n",
        "total_count = 0\n",
        "num_available = 0\n",
        "\n",
        "for id in ids:\n",
        "  total_count+=1\n",
        "  participant_id = int(id.split(\".\")[0])\n",
        "  homa_ir = obj.get_individual_homa_ir(participant_id)\n",
        "  if homa_ir \u003e0:\n",
        "    num_available+=1\n",
        "\n",
        "print(f\"Percentage of non-null labels: {(num_available/total_count)*100}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUVDrcyqlMIX"
      },
      "outputs": [],
      "source": [
        "root_folder = path_to_save_lsm_data\n",
        "files = gfile.ListDir(os.path.join(root_folder,\"BY_SUBJECT_momentary_stress_algorithm\"))\n",
        "ids = [s[-9:] for s in files]\n",
        "print(ids)\n",
        "\n",
        "all_features = []\n",
        "for d in data:\n",
        "  all_features.extend(d['features_to_extract'])\n",
        "\n",
        "types = list(map(lambda x : x['type'], data))\n",
        "cnt=0\n",
        "\n",
        "def window(ids: list[str], window_length: str, timestamp_units: str):\n",
        "  inputs = []\n",
        "  mask = []\n",
        "\n",
        "  for i in ids:\n",
        "    print('ID: ', i)\n",
        "    d = {}\n",
        "    dfs = []\n",
        "    for table in data:\n",
        "      t = table['type']\n",
        "\n",
        "      try:\n",
        "        d[t] = pd.read_csv(gfile.Open(os.path.join(root_folder,\"BY_SUBJECT_\"+t,t+\"_\"+i), 'r'))\n",
        "        d[t].rename(columns={'DT': 't'}, inplace=True)\n",
        "        d[t]['t'] = pd.to_datetime(d[t]['t'], format='mixed')\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "      cols = table['features_to_extract'].copy()\n",
        "      cols.append('t')\n",
        "      d[t] = d[t][cols]\n",
        "\n",
        "      if t == 'heart_rate':\n",
        "        d[t]['bpm'][d[t]['bpm'] == -1] = np.nan\n",
        "\n",
        "      if t == 'ceda':\n",
        "        d[t].loc[d[t]['eda_level_real'] \u003e 60, \"eda_level_real\"] = 60\n",
        "        d[t].loc[d[t]['eda_level_real'] \u003c 0, \"eda_level_real\"] = 0\n",
        "\n",
        "      if t == 'momentary_stress_algorithm':\n",
        "        d[t].loc[d[t]['ceda_slope_real_micro_siemens'] \u003e 5, \"ceda_slope_real_micro_siemens\"] = 5\n",
        "        d[t].loc[d[t]['ceda_slope_real_micro_siemens'] \u003c -5, \"ceda_slope_real_micro_siemens\"] = -5\n",
        "\n",
        "      if t == 'sleep_coefficient':\n",
        "        d[t].loc[d[t]['sleep_coefficient'] == -1, \"sleep_coefficient\"] = np.nan\n",
        "\n",
        "      if t == 'wrist_temperature':\n",
        "        d[t]['wrist_temperatures'] = d[t]['wrist_temperatures']/20000\n",
        "        d[t].loc[d[t]['wrist_temperatures'] \u003e 41, \"wrist_temperatures\"] = 41\n",
        "        d[t].loc[d[t]['wrist_temperatures'] \u003c 0, \"wrist_temperatures\"] = np.nan\n",
        "\n",
        "      if t == 'grok':\n",
        "        d[t]['altim_std'] = d[t]['altim_std']/255\n",
        "\n",
        "      if len(d[t]) \u003e 0:\n",
        "        d[t] = Sensor.resample(d[t], input_timestamp_units='s', output_timestamp_units=timestamp_units)\n",
        "        dfs.append(d[t])\n",
        "\n",
        "    if len(dfs) \u003e 0:\n",
        "      session = ft.reduce(lambda left, right: pd.merge(left, right, on='DT', how='outer'), dfs)\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "    for feature in FEATURES_TO_INCLUDE:\n",
        "      if feature not in session.columns:\n",
        "        session.loc[:,feature] = np.nan\n",
        "\n",
        "    session = session[FEATURES_TO_INCLUDE]\n",
        "    for feature in FEATURES_TO_INCLUDE:\n",
        "      session.loc[:,feature] = (\n",
        "          session[feature] - NORMALIZATION_PARAMETERS[feature][0]\n",
        "      ) / (NORMALIZATION_PARAMETERS[feature][1])\n",
        "    session = session.clip(-5, 5)\n",
        "\n",
        "    df_grouped = session.groupby(pd.Grouper(freq=window_length))\n",
        "    for name, group in df_grouped:\n",
        "      nan_mask = np.isnan(group.to_numpy())\n",
        "      missingness_ratio = np.sum(nan_mask) / (\n",
        "          nan_mask.shape[0] * nan_mask.shape[1]\n",
        "      )\n",
        "      if group.shape[0] == 168*60 and group.shape[1] == len(FEATURES_TO_INCLUDE):\n",
        "        if missingness_ratio\u003e0.8:\n",
        "          print('.   Too much missingness.')\n",
        "        else:\n",
        "          group = np.nan_to_num(group)\n",
        "          yield name, {\n",
        "              'id': i,\n",
        "              'input': group,#.values,\n",
        "              'mask': nan_mask,\n",
        "          }\n",
        "\n",
        "w = window(ids[:10], '168h', '1min')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0kPTJU2WvbT"
      },
      "outputs": [],
      "source": [
        "df['participant_id'] = df.index\n",
        "df[df['participant_id']== 12612]['homa_ir']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTtuczc5jTRA"
      },
      "source": [
        "# STEP 4: Create TFrecords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JGojH0yjW-a"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import time\n",
        "import enum\n",
        "\n",
        "def _bytes_feature(value):\n",
        "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "  if isinstance(value, type(tf.constant(0))):\n",
        "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _float_feature(value):\n",
        "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "def _int64_feature(value):\n",
        "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "\n",
        "def _string_feature(value):\n",
        "  \"\"\"Returns a bytes_list from a string.\"\"\"\n",
        "  if isinstance(value, type(tf.constant(\"test\"))):\n",
        "    value = value.numpy()  # Ensure it's not a TensorFlow string tensor\n",
        "  return _bytes_feature(value)\n",
        "\n",
        "def numpy_example(array, labels):\n",
        "  feature = {\n",
        "      'bmi': _float_feature(labels[0]),\n",
        "      'homa_ir': _float_feature(labels[1]),\n",
        "      'apri': _float_feature(labels[2]),\n",
        "      'msss': _float_feature(labels[3]),\n",
        "      'hypertension_binary': _int64_feature(labels[4]),\n",
        "      'hyperlipidemia_binary': _int64_feature(labels[5]),\n",
        "      'cardiovascular_binary': _int64_feature(labels[6]),\n",
        "      'diabetes_binary': _int64_feature(labels[7]),\n",
        "      'anxiety_binary': _int64_feature(labels[8]),\n",
        "      'respiratory': _int64_feature(labels[9]),\n",
        "      'kidney_disease': _int64_feature(labels[10]),\n",
        "      'homa_ir_binray': _int64_feature(labels[11]),\n",
        "      'msss_binary': _int64_feature(labels[12]),\n",
        "\n",
        "      'regular_menstruation_str': _string_feature(tf.io.serialize_tensor(labels[13])),\n",
        "      'smoker_str': _string_feature(tf.io.serialize_tensor(labels[14])),\n",
        "      'diabetes_type_str': _string_feature(tf.io.serialize_tensor(labels[15])),\n",
        "      'alcohol_str': _string_feature(tf.io.serialize_tensor(labels[16])),\n",
        "      'medications_str': _string_feature(tf.io.serialize_tensor(labels[17])),\n",
        "\n",
        "      'array_raw': _bytes_feature(tf.io.serialize_tensor(array)),\n",
        "  }\n",
        "  return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "gfile.MakeDirs(tf_record_folder)\n",
        "\n",
        "types = list(map(lambda x : x['type'], data))\n",
        "met_labels_object = MetHealthLabels()\n",
        "\n",
        "# Iterate over the dataset and write each example to the TFRecord file\n",
        "for i in ids[0:10]:\n",
        "  id = i[0:-4]\n",
        "  w = window([i], '168h', '1min')\n",
        "  output_file = 'metabolichealth_' + id +'.tfrecords'\n",
        "  with tf.io.TFRecordWriter(os.path.join(tf_record_folder,output_file)) as writer:\n",
        "    for key, result in w:\n",
        "      metabolic_labels_numerical = {\"bmi\": met_labels_object.get_individual_bmi(int(id)),\n",
        "                    \"homa_ir\": met_labels_object.get_individual_homa_ir(int(id)),\n",
        "                    \"homa_ir_binary\": met_labels_object.get_individual_homa_ir_binary(int(id)),\n",
        "                    \"msss\": met_labels_object.get_individual_msss(int(id)),\n",
        "                    \"msss_binary\": met_labels_object.get_individual_msss_binary(int(id)),\n",
        "                    \"apri\": met_labels_object.get_individual_apri(int(id)),\n",
        "                    \"hypertension\": met_labels_object.get_individual_hypertension(int(id)),\n",
        "                    \"hyperlipidemia\": met_labels_object.get_individual_hyperlipidemia(int(id)),\n",
        "                    \"CVD\": met_labels_object.get_individual_cardiovascular_condition(int(id)),\n",
        "                    \"diabetes\": met_labels_object.get_individual_diabetes_condition(int(id)),\n",
        "                    \"anxiety\": met_labels_object.get_individual_anxiety_condition(int(id)),\n",
        "                    \"respiratory\": met_labels_object.get_individual_respiratory_condition(int(id)),\n",
        "                    \"kidney Disease\": met_labels_object.get_individual_kidney_condition(int(id)),\n",
        "                          }\n",
        "      metabolic_labels_non_numerical = {\n",
        "          \"diabetes_type\": met_labels_object.get_individual_diabetes_type(int(id)),\n",
        "          \"alcohol\": met_labels_object.get_individual_alcohol_consumption(int(id)),\n",
        "          \"medications\": met_labels_object.get_individual_medications(int(id)),\n",
        "          \"regular_periods\": met_labels_object.get_individual_regular_mensturation(int(id)),\n",
        "          \"smoker\": met_labels_object.get_individual_smoking(int(id)),\n",
        "      }\n",
        "      labels = []\n",
        "      print(key)\n",
        "      metabolic_labels_numerical = {k: -999 if np.isnan(v) else v for k, v in metabolic_labels_numerical.items()}\n",
        "      metabolic_labels_string = {k: '-999' if pd.isna(v) else v for k, v in metabolic_labels_non_numerical.items()}\n",
        "\n",
        "      try:\n",
        "        labels.append(float(metabolic_labels_numerical[\"bmi\"]))\n",
        "        labels.append(float(metabolic_labels_numerical[\"homa_ir\"]))\n",
        "        labels.append(float(metabolic_labels_numerical[\"apri\"]))\n",
        "        labels.append(float(metabolic_labels_numerical[\"msss\"]))\n",
        "        labels.append(int(metabolic_labels_numerical[\"hypertension\"]))\n",
        "        labels.append(int(metabolic_labels_numerical[\"hyperlipidemia\"]))\n",
        "        labels.append(int(metabolic_labels_numerical[\"CVD\"]))\n",
        "        labels.append(int(metabolic_labels_numerical[\"diabetes\"]))\n",
        "        labels.append(int(metabolic_labels_numerical[\"anxiety\"]))\n",
        "        labels.append(int(metabolic_labels_numerical[\"respiratory\"]))\n",
        "        labels.append(int(metabolic_labels_numerical[\"kidney Disease\"]))\n",
        "        labels.append(int(metabolic_labels_numerical[\"homa_ir_binary\"]))\n",
        "        labels.append(int(metabolic_labels_numerical[\"msss_binary\"]))\n",
        "\n",
        "        labels.append(str(metabolic_labels_non_numerical[\"regular_periods\"]))\n",
        "        labels.append(str(metabolic_labels_non_numerical[\"smoker\"]))\n",
        "        labels.append(str(metabolic_labels_non_numerical[\"diabetes_type\"]))\n",
        "        labels.append(str(metabolic_labels_non_numerical[\"alcohol\"]))\n",
        "        labels.append(str(metabolic_labels_non_numerical[\"medications\"]))\n",
        "      except:\n",
        "        print('Label missing.')\n",
        "        continue\n",
        "      tf_example = numpy_example(result['input'], labels)\n",
        "      writer.write(tf_example.SerializeToString())\n",
        "      print('.   '+str(key))\n",
        "    print(f'TFRecord file created: {os.path.join(tf_record_folder,output_file)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF0Tmw1oQ_SW"
      },
      "outputs": [],
      "source": [
        "root_folder = path_to_save_lsm_data\n",
        "print(root_folder)\n",
        "files = gfile.ListDir(os.path.join(root_folder,\"BY_SUBJECT_momentary_stress_algorithm\"))\n",
        "ids = [s[-9:] for s in files]\n",
        "print(ids)\n",
        "\n",
        "\n",
        "for i in ids:\n",
        "  id = i[0:-4]\n",
        "  print(id)\n",
        "  print(met_labels_object.get_individual_bmi(int(id)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUz7_W7mOVpR"
      },
      "source": [
        "# Other Notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSzhnTX8li2G"
      },
      "outputs": [],
      "source": [
        "root_folder = path_to_save_lsm_data\n",
        "\n",
        "all_features = []\n",
        "for d in data:\n",
        "  all_features.extend(d['features_to_extract'])\n",
        "\n",
        "print(all_features)\n",
        "\n",
        "files = gfile.ListDir(os.path.join(root_folder,\"BY_SUBJECT_momentary_stress_algorithm\"))\n",
        "ids = [s[-9:] for s in files]\n",
        "\n",
        "types = list(map(lambda x : x['type'], data))\n",
        "met_labels_object = MetHealthLabels()\n",
        "\n",
        "cnt=0\n",
        "for i in ids:\n",
        "  print('ID: ', i)\n",
        "\n",
        "  d = {}\n",
        "  dfs = []\n",
        "  # ----------------Updated January 6th-----------------\n",
        "\n",
        "  participant_id = int(i.split(\".\")[0])\n",
        "  print('.   Loading cardiometabolic and hepatic labels: ')\n",
        "  metabolic_labels = {\"bmi\": met_labels_object.get_individual_bmi(participant_id),\n",
        "                      \"homa_ir\": met_labels_object.get_individual_homa_ir(participant_id),\n",
        "                      \"msss\": met_labels_object.get_individual_msss(participant_id),\n",
        "                      \"apri\": met_labels_object.get_individual_apri(participant_id),\n",
        "                      \"hypertension\": met_labels_object.get_individual_hypertension(participant_id)}\n",
        "\n",
        "  # -------------------------------------------------\n",
        "\n",
        "  for table in data:\n",
        "    t = table['type']\n",
        "\n",
        "    try:\n",
        "      d[t] = pd.read_csv(gfile.Open(os.path.join(root_folder,\"BY_SUBJECT_\"+t,t+\"_\"+i), 'r'))\n",
        "      print('.   Loaded '+t)\n",
        "    except:\n",
        "      print('.   Failed to load '+t)\n",
        "      continue\n",
        "\n",
        "    d[t].rename(columns={'DT': 't'}, inplace=True)\n",
        "    # Ali added the following exception catch here\n",
        "    try:\n",
        "      d[t]['t'] = pd.to_datetime(d[t]['t'])\n",
        "    except ValueError:\n",
        "      print('.   Failed to convert '+t)\n",
        "      d[t]['t'] = pd.to_datetime(d[t]['t'], format='mixed')\n",
        "      continue\n",
        "    cols = table['features_to_extract'].copy()\n",
        "    cols.append('t')\n",
        "    d[t] = d[t][cols]\n",
        "\n",
        "    if t == 'heart_rate':\n",
        "      d[t]['bpm'][d[t]['bpm'] == -1] = np.nan\n",
        "\n",
        "    if t == 'ceda':\n",
        "      d[t]['eda_level_real'][d[t]['eda_level_real'] \u003e 60] = 60\n",
        "      d[t]['eda_level_real'][d[t]['eda_level_real'] \u003c 0] = 0\n",
        "\n",
        "    if t == 'momentary_stress_algorithm':\n",
        "      d[t]['ceda_slope_real_micro_siemens'][d[t]['ceda_slope_real_micro_siemens'] \u003e 5] = 5\n",
        "      d[t]['ceda_slope_real_micro_siemens'][d[t]['ceda_slope_real_micro_siemens'] \u003c -5] = -5\n",
        "\n",
        "    if t == 'sleep_coefficient':\n",
        "      d[t]['sleep_coefficient'][d[t]['sleep_coefficient'] == -1] = np.nan\n",
        "\n",
        "    if t == 'wrist_temperature':\n",
        "      d[t]['wrist_temperatures'] = d[t]['wrist_temperatures']/20000\n",
        "      d[t]['wrist_temperatures'][d[t]['wrist_temperatures'] \u003e 41] = 41\n",
        "      d[t]['wrist_temperatures'][d[t]['wrist_temperatures'] \u003c 0] = np.nan\n",
        "\n",
        "    if t == 'grok':\n",
        "      d[t]['altim_std'] = d[t]['altim_std']/255\n",
        "\n",
        "    if len(d[t]) \u003e 0:\n",
        "      d[t] = Sensor.resample(d[t], input_timestamp_units='s', output_timestamp_units='1min')\n",
        "      dfs.append(d[t])\n",
        "\n",
        "  session = ft.reduce(lambda left, right: pd.merge(left, right, on='DT', how='outer'), dfs)\n",
        "  for f in all_features:\n",
        "    if f not in session.columns:\n",
        "      session[f] = np.nan\n",
        "\n",
        "  session = zscore(session, nan_policy='omit')\n",
        "  sess = session.clip(-3,3)\n",
        "\n",
        "\n",
        "  df_grouped = sess.groupby(pd.Grouper(freq='168h'))\n",
        "  df_grouped.head()\n",
        "  groups = []\n",
        "  for name, group in df_grouped:\n",
        "    # ----------------Updated January 6th-----------------\n",
        "    group['bmi'] = metabolic_labels['bmi']\n",
        "    group['homa_ir'] = metabolic_labels['homa_ir']\n",
        "    group['msss'] = metabolic_labels['msss']\n",
        "    group['apri'] = metabolic_labels['apri']\n",
        "    group['hypertension'] = metabolic_labels['hypertension']\n",
        "    groups.append(group)\n",
        "    # -------------------------------------------------\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(25,5))\n",
        "    x_lims = [group.index[0], group.index[-1]]\n",
        "    y_lims = [0, len(group.columns)]\n",
        "    im = ax.imshow(np.flip(np.flip(group.to_numpy()[0:-1,:].T,axis=0),axis=1), interpolation='nearest', aspect='auto', extent = [x_lims[0], x_lims[1],  y_lims[0], y_lims[1]])\n",
        "    ax.set_yticks(range(0,len(group.columns)))\n",
        "    ax.set_yticklabels(group.columns.to_list())\n",
        "    fig.colorbar(im)\n",
        "    plt.show()\n",
        "\n",
        "  #  ----------------Added December 12th-----------------\n",
        "  df_grouped = pd.concat(groups)\n",
        "  # -------------------------------------------------\n",
        "\n",
        "  cnt += 1\n",
        "  if cnt \u003e10:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EujklwfwkbID"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "class Lsm(tfds.core.GeneratorBasedBuilder):\n",
        "  \"\"\"DatasetBuilder for afib dataset.\"\"\"\n",
        "\n",
        "  VERSION = tfds.core.Version('2.0.0')\n",
        "  RELEASE_NOTES = {\n",
        "      '2.0.0': 'Initial release.',\n",
        "  }\n",
        "\n",
        "  def _info(self) -\u003e tfds.core.DatasetInfo:\n",
        "    \"\"\"Returns the dataset metadata.\"\"\"\n",
        "    return tfds.core.DatasetInfo(\n",
        "        builder=self,\n",
        "        description=_DESCRIPTION,\n",
        "        features=tfds.features.FeaturesDict(_init_feature_dict()),\n",
        "        supervised_keys=None,\n",
        "        citation=_CITATION,\n",
        "    )\n",
        "\n",
        "  def _split_generators(\n",
        "      self, dl_manager: tfds.download.DownloadManager, pipeline: beam.Pipeline\n",
        "  ) -\u003e Any:\n",
        "    \"\"\"Returns SplitGenerators.\"\"\"\n",
        "\n",
        "    # read ids list from csv\n",
        "    root_folder = (\"/cns/yq-d/home/fitbit-medical-sandboxes/e=1:kid=76381\"\n",
        "                         \":mkey=cns-deid/chr-ards-metabolichealth/deid/exp/\"\n",
        "                         \"mdb=chr-ards-metabolichealth-deid-exp-decrypt/\"\n",
        "                         \"foundational_llm_research/large_sensor_model_data/\")\n",
        "\n",
        "    files = gfile.ListDir(os.path.join(root_folder,\"BY_SUBJECT_steps\"))\n",
        "    ids = [s[6:] for s in files]\n",
        "\n",
        "    return {\n",
        "        'train': pipeline | 'GenerateExamples' \u003e\u003e self._generate_examples(ids)\n",
        "    }\n",
        "\n",
        "  def _generate_examples(self, ids: list[str]) -\u003e Any:\n",
        "    \"\"\"Yields examples.\"\"\"\n",
        "    return beam.Create(ids) | 'GetExampleGenerator' \u003e\u003e beam.FlatMap(\n",
        "        get_example_g,\n",
        "        enerator\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3D1jHPckeBy"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "def get_example_generator(user_id: str) -\u003e Any:\n",
        "  \"\"\"Returns an example generator.\"\"\"\n",
        "\n",
        "  logging.info('Retrieving Data for user %s', user_id)\n",
        "  try:\n",
        "    timestamp_millis = int(time.time() * 1000)\n",
        "    random.seed(timestamp_millis)\n",
        "    for i, (_, data) in enumerate(sensor.window([user_id], '1min', 's')):\n",
        "      key = '%d_%d_%s' % (\n",
        "          random.randint(0, 1000000000),\n",
        "          i,\n",
        "          user_id,\n",
        "      )  # get unique key\n",
        "      result = {\n",
        "          constants.TFExampleKey.INPUT.value: _serialize_numpy_array(\n",
        "              data['input']\n",
        "          ),\n",
        "          constants.TFExampleKey.MASK.value: _serialize_numpy_array(\n",
        "              data['mask']\n",
        "          ),\n",
        "      }\n",
        "      yield key, result\n",
        "  except Exception as e:  # pylint: disable=broad-except\n",
        "    logging.warning(\n",
        "        'Failed to retrieve data for user %s, error: %s', user_id, str(e)\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ozT4dyz3t6z"
      },
      "outputs": [],
      "source": [
        "# Demo\n",
        "demo = survey_phone_data['demographic_questionnaire_responses.csv'].copy()\n",
        "\n",
        "try:\n",
        "  demo.set_index([\"#study_participant_id\"], inplace=True)\n",
        "except:\n",
        "  'Index already reset.'\n",
        "\n",
        "joined_df = demo\n",
        "\n",
        "# BFI Responses\n",
        "#contents['intake_survey_bfi_questionnaire.csv'] # Questions\n",
        "bfi = survey_phone_data['intake_survey_bfi_questionnaire_responses.csv']\n",
        "# Code BFI:\n",
        "#Extraversion: 1R, 5 #Agreeableness: 2, 7R #Conscientiousness: 3R, 8 #Neuroticism: 4R, 9 #Openness to Experience: 5R, 10\n",
        "mapping = {'Disagree strongly': 1, 'Disagree a little': 2, 'Neither agree nor disagree': 3, 'Agree a little': 4, 'Agree strongly': 5}\n",
        "for c in range(1,11):\n",
        "  bfi['intake_survey_-_bfi-10_q'+str(c)+'_group_score'] = bfi['intake_survey_-_bfi-10_q'+str(c)+'_group'].map(mapping)\n",
        "\n",
        "bfi['extraversion_score'] = -bfi['intake_survey_-_bfi-10_q1_group_score'] + bfi['intake_survey_-_bfi-10_q5_group_score']\n",
        "bfi['agreeableness_score'] = bfi['intake_survey_-_bfi-10_q2_group_score'] - bfi['intake_survey_-_bfi-10_q7_group_score']\n",
        "bfi['conscientiousness_score'] = -bfi['intake_survey_-_bfi-10_q3_group_score'] + bfi['intake_survey_-_bfi-10_q8_group_score']\n",
        "bfi['neuroticism_score'] = -bfi['intake_survey_-_bfi-10_q4_group_score'] + bfi['intake_survey_-_bfi-10_q9_group_score']\n",
        "bfi['openness_score'] = -bfi['intake_survey_-_bfi-10_q5_group_score'] + bfi['intake_survey_-_bfi-10_q10_group_score']\n",
        "\n",
        "bfi.reset_index(inplace=True)\n",
        "bfi.set_index([\"#study_participant_id\"], inplace=True)\n",
        "#bfi.reset_index(inplace=True)\n",
        "#bfi.set_index([\"#study_participant_id\"], inplace=True)\n",
        "joined_df = joined_df.join(bfi, lsuffix='bfi_')\n",
        "\n",
        "# PHQ Responses\n",
        "#contents['phq_8_complete_questionnaire.csv'] # Questions\n",
        "\n",
        "# Intake:\n",
        "phq = survey_phone_data['phq_8_intake_questionnaire_responses.csv']\n",
        "\n",
        "mapping = {'Not at all': 0, 'Several days': 1, 'More than half the days': 2, 'Nearly every day': 3}\n",
        "columns = ['little_interest','depression','sleep','tired','appetite','failure','trouble_concentrating','restlessness']\n",
        "for c in columns:\n",
        "  phq[c+'_intake_score'] = phq[c].map(mapping)\n",
        "columns = ['little_interest_intake_score','depression_intake_score','sleep_intake_score',\n",
        "           'tired_intake_score','appetite_intake_score','failure_intake_score',\n",
        "           'trouble_concentrating_intake_score','restlessness_intake_score']\n",
        "phq['phq_intake_score'] = phq[columns].mean(axis=1)*8\n",
        "phq = phq[['#study_participant_id',\n",
        "           'little_interest_intake_score','depression_intake_score','sleep_intake_score',\n",
        "           'tired_intake_score','appetite_intake_score','failure_intake_score',\n",
        "           'trouble_concentrating_intake_score','restlessness_intake_score','phq_intake_score']]\n",
        "\n",
        "phq.reset_index(inplace=True)\n",
        "phq.set_index([\"#study_participant_id\"], inplace=True)\n",
        "joined_df = joined_df.join(phq, lsuffix='phq_')\n",
        "\n",
        "# Completion:\n",
        "phq = survey_phone_data['phq_8_complete_questionnaire_responses.csv']\n",
        "\n",
        "mapping = {'Not at all': 0, 'Several days': 1, 'More than half the days': 2, 'Nearly every day': 3}\n",
        "columns = ['little_interest','depression','sleep','tired','appetite','failure','trouble_concentrating','restlessness']\n",
        "for c in columns:\n",
        "  phq[c+'_complete_score'] = phq[c].map(mapping)\n",
        "columns = ['little_interest_complete_score','depression_complete_score','sleep_complete_score',\n",
        "           'tired_complete_score','appetite_complete_score','failure_complete_score',\n",
        "           'trouble_concentrating_complete_score','restlessness_complete_score']\n",
        "phq['phq_complete_score'] = phq[columns].mean(axis=1)*8\n",
        "phq = phq[['#study_participant_id',\n",
        "           'little_interest_complete_score','depression_complete_score','sleep_complete_score',\n",
        "           'tired_complete_score','appetite_complete_score','failure_complete_score',\n",
        "           'trouble_concentrating_complete_score','restlessness_complete_score','phq_complete_score']]\n",
        "\n",
        "phq.reset_index(inplace=True)\n",
        "phq.set_index([\"#study_participant_id\"], inplace=True)\n",
        "phq.drop(columns=['index'], inplace=True)\n",
        "joined_df = joined_df.join(phq, lsuffix='phq_')\n",
        "\n",
        "\n",
        "# GAD Responses\n",
        "#contents['phq_8_complete_questionnaire.csv'] # Questions\n",
        "\n",
        "# Intake:\n",
        "gad = survey_phone_data['gad_7_intake_questionnaire_responses.csv'].copy()\n",
        "\n",
        "mapping = {'Not at all': 0, 'Several days': 1, 'More than half the days': 2,\n",
        "           'Nearly every day': 3}\n",
        "columns = ['anxiety','cannot_stop_worry','too_much_worry','trouble_relaxing','restlessness','irritability','fear']\n",
        "for c in columns:\n",
        "  gad['gad_'+c+'_intake_score'] = gad[c].map(mapping)\n",
        "columns = ['gad_anxiety_intake_score','gad_cannot_stop_worry_intake_score','gad_too_much_worry_intake_score','gad_trouble_relaxing_intake_score','gad_restlessness_intake_score','gad_irritability_intake_score','gad_fear_intake_score']\n",
        "gad['gad_intake_score'] = gad[columns].mean(axis=1)*7\n",
        "gad = gad[['#study_participant_id','gad_anxiety_intake_score','gad_cannot_stop_worry_intake_score','gad_too_much_worry_intake_score','gad_trouble_relaxing_intake_score','gad_restlessness_intake_score','gad_irritability_intake_score','gad_fear_intake_score','gad_intake_score']]\n",
        "\n",
        "gad.reset_index(inplace=True)\n",
        "gad.set_index([\"#study_participant_id\"], inplace=True)\n",
        "gad.drop(columns=['index'], inplace=True)\n",
        "joined_df = joined_df.join(gad, lsuffix='gad_')\n",
        "\n",
        "# Completion:\n",
        "gad = survey_phone_data['gad_7_complete_questionnaire_responses.csv'].copy()\n",
        "\n",
        "mapping = {'Not at all': 0, 'Several days': 1, 'More than half the days': 2,\n",
        "           'Nearly every day': 3}\n",
        "columns = ['anxiety','cannot_stop_worry','too_much_worry','trouble_relaxing','restlessness','irritability','fear']\n",
        "for c in columns:\n",
        "  gad['gad_'+c+'_complete_score'] = gad[c].map(mapping)\n",
        "columns = ['gad_anxiety_complete_score','gad_cannot_stop_worry_complete_score','gad_too_much_worry_complete_score','gad_trouble_relaxing_complete_score','gad_restlessness_complete_score','gad_irritability_complete_score','gad_fear_complete_score']\n",
        "gad['gad_complete_score'] = gad[columns].mean(axis=1)*7\n",
        "gad = gad[['#study_participant_id','gad_anxiety_complete_score','gad_cannot_stop_worry_complete_score','gad_too_much_worry_complete_score','gad_trouble_relaxing_complete_score','gad_restlessness_complete_score','gad_irritability_complete_score','gad_fear_complete_score','gad_complete_score']]\n",
        "\n",
        "gad.reset_index(inplace=True)\n",
        "gad.set_index([\"#study_participant_id\"], inplace=True)\n",
        "gad.drop(columns=['index'], inplace=True)\n",
        "joined_df = joined_df.join(gad, lsuffix='gad_')\n",
        "\n",
        "# Sleep disturbance Responses\n",
        "sleep_disturbance = survey_phone_data['sleep_disturbance_intake_questionnaire_responses.csv']\n",
        "\n",
        "mapping = {'Not at all': 1, 'A little bit': 2, 'Somewhat': 3, 'Quite a bit': 4, 'Very much': 5}\n",
        "columns = ['restless','satisfied', 'refreshing','trouble_falling_asleep']\n",
        "for c in columns:\n",
        "  sleep_disturbance[c+'_score'] = sleep_disturbance[c].map(mapping)\n",
        "\n",
        "mapping = {'Never': 1, 'Rarely': 2, 'Sometimes': 3, 'Often': 4, 'Always': 5}\n",
        "columns = ['trouble_staying_asleep', 'trouble_sleeping',\n",
        "       'enough_sleep']\n",
        "for c in columns:\n",
        "  sleep_disturbance[c+'_score'] = sleep_disturbance[c].map(mapping)\n",
        "\n",
        "mapping = {'Very poor': 1, 'Poor': 2, 'Fair': 3, 'Good': 4, 'Very good': 5}\n",
        "columns = ['quality']\n",
        "for c in columns:\n",
        "  sleep_disturbance[c+'_score'] = sleep_disturbance[c].map(mapping)\n",
        "\n",
        "#cols = fnmatch.filter(sleep_disturbance.columns, '*_score')\n",
        "#sleep_disturbance['sleep_disturbance_score'] = sleep_disturbance[cols].sum(axis=1)\n",
        "sleep_disturbance['sleep_disturbance_score'] = (sleep_disturbance['restless_score'] + (5 - sleep_disturbance['satisfied_score']) + (5 - sleep_disturbance['refreshing_score']) + sleep_disturbance['trouble_falling_asleep_score'] + sleep_disturbance['trouble_staying_asleep_score'] + sleep_disturbance['trouble_sleeping_score'] + (5 - sleep_disturbance['enough_sleep_score']) + (5 - sleep_disturbance['quality_score']) )\n",
        "sleep_disturbance.reset_index(inplace=True)\n",
        "sleep_disturbance.set_index([\"#study_participant_id\"], inplace=True)\n",
        "joined_df = joined_df.join(sleep_disturbance, lsuffix='sleepdisturbance_')\n",
        "\n",
        "\n",
        "# Sleep Impairment Responses\n",
        "sleep_impairment = survey_phone_data['sleep_impairment_intake_questionnaire_responses.csv']\n",
        "\n",
        "mapping = {'Not at all': 1, 'A little bit': 2, 'Somewhat': 3, 'Quite a bit': 4, 'Very much': 5}\n",
        "columns = ['trouble_productivity', 'alertness', 'tiredness',\n",
        "       'problems', 'trouble_concentrating', 'irritability',\n",
        "       'sleepy_during_daytime', 'trouble_staying_awake']\n",
        "for c in columns:\n",
        "  sleep_impairment[c+'_score'] = sleep_impairment[c].map(mapping)\n",
        "\n",
        "#cols = fnmatch.filter(sleep_impairment.columns, '*_score')\n",
        "#sleep_impairment['sleep_impairment_score'] = sleep_impairment[cols].sum(axis=1)\n",
        "sleep_impairment['sleep_impairment_score'] = (sleep_impairment['trouble_productivity_score'] + (5 - sleep_impairment['alertness_score']) + sleep_impairment['tiredness_score'] + sleep_impairment['problems_score'] + sleep_impairment['trouble_concentrating_score'] + sleep_impairment['irritability_score'] + sleep_impairment['sleepy_during_daytime_score'] + sleep_impairment['trouble_staying_awake_score'] )\n",
        "\n",
        "sleep_impairment.reset_index(inplace=True)\n",
        "sleep_impairment.set_index([\"#study_participant_id\"], inplace=True)\n",
        "joined_df = joined_df.join(sleep_impairment, lsuffix='sleepimpairment_')\n",
        "\n",
        "# PSS Responses\n",
        "pss = survey_phone_data['pss_intake_questionnaire_responses.csv']\n",
        "\n",
        "mapping_1 = {'Never': 0, 'Almost Never': 1, 'Sometimes': 2, 'Fairly Often': 3, 'Very Often': 4}\n",
        "mapping_2 = {'Never': 4, 'Almost Never': 3, 'Sometimes': 2, 'Fairly Often': 1, 'Very Often': 0}\n",
        "\n",
        "# 1. In the last month, how often have you been upset because of something that happened unexpectedly?\n",
        "# 2. In the last month, how often have you felt that you were unable to control the important things in your life?\n",
        "# 3. In the last month, how often have you felt nervous and stressed?\n",
        "# 4. In the last month, how often have you felt confident about your ability to handle your personal problems?\n",
        "# 5. In the last month, how often have you felt that things were going your way?\n",
        "# 6. In the last month, how often have you found that you could not cope with all the things that you had to do?\n",
        "# 7. In the last month, how often have you been able to control irritations in your life?\n",
        "# 8. In the last month, how often have you felt that you were on top of things?\n",
        "# 9. In the last month, how often have you been angered because of things that happened that were outside of your control?\n",
        "# 10. In the last month, how often have you felt difficulties were piling up so high that you could not overcome them?\n",
        "\n",
        "columns = ['upset','no_control','stress','handle_personal_problems','things_positive','cannot_cope','control_irritation','on_top_of_things','anger','overwhelm']\n",
        "newcolumns = []\n",
        "for c in columns:\n",
        "  if c in ['handle_personal_problems','things_positive','control_irritation','on_top_of_things']:\n",
        "    pss['pss_'+c+'_score'] = pss[c].map(mapping_2)\n",
        "  else:\n",
        "    pss['pss_'+c+'_score'] = pss[c].map(mapping_1)\n",
        "  newcolumns.append('pss_'+c+'_score')\n",
        "\n",
        "columns = newcolumns\n",
        "pss['pss_score'] = pss[columns].sum(axis=1)\n",
        "columns.append('pss_score')\n",
        "columns.append('#study_participant_id')\n",
        "pss = pss[columns]\n",
        "\n",
        "pss.reset_index(inplace=True)\n",
        "pss.set_index([\"#study_participant_id\"], inplace=True)\n",
        "joined_df = joined_df.join(pss, lsuffix='pss_')\n",
        "\n",
        "plt.figure(figsize=(15, 3))\n",
        "plt.plot(joined_df.isna().sum())\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "joined_df.reset_index(inplace=True)\n",
        "#joined_df.dropna(inplace=True)\n",
        "joined_df.index = joined_df.index.astype(int)\n",
        "demo = joined_df\n",
        "demo['phq_delta'] = demo['phq_complete_score'] - demo['phq_intake_score']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xhg9wJAfppb"
      },
      "source": [
        "# Per Type Processing (Deprecated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4meY-NdtCmE"
      },
      "outputs": [],
      "source": [
        "# @title Steps\n",
        "## Steps:\n",
        "with gfile.Open(os.path.join(root_folder, 'STEPS_COMPACT_DATA.csv.csv'), 'r') as f:\n",
        "  steps = pd.read_csv(f)\n",
        "steps.reset_index(inplace=True)\n",
        "list_arrays = []\n",
        "for i, row in steps.iterrows():\n",
        "  tmp = pd.DataFrame()\n",
        "  tmp['millis_from_start_time'] = pd.Series(range(0,1440))*1000*60\n",
        "  tmp['steps'] = get_arrays(row,'steps')\n",
        "  tmp['participant_id'] = row['participant_id']\n",
        "  tmp['activity_time'] = row['activity_time']\n",
        "  tmp['activity_tm_timezone_offset'] = row['activity_tm_timezone_offset']\n",
        "  list_arrays.append(tmp)\n",
        "\n",
        "steps = pd.concat(list_arrays, ignore_index=True)\n",
        "steps = steps[steps['steps']\u003e=0]\n",
        "steps['activity_time'] = pd.to_datetime(steps['activity_time'])\n",
        "steps['activity_time_local'] = steps['activity_time'] + steps['activity_tm_timezone_offset'].astype('timedelta64[m]') + steps['millis_from_start_time'].astype('timedelta64[ms]')\n",
        "\n",
        "steps.rename(columns={'activity_time_local': 'DT', 'participant_id': 'ID'}, inplace=True)\n",
        "steps = steps[['ID','DT','steps']]\n",
        "steps.to_csv(gfile.Open(os.path.join(root_folder,\"STEPS_COMPACT_DATA_PROCESSED.csv\"), 'w'))\n",
        "\n",
        "WORKER_COUNT = 20\n",
        "\n",
        "def split_save(L):\n",
        "  steps[steps['ID']==L].to_csv(gfile.Open(os.path.join(root_folder,\"steps\",\"steps_\"+str(L)+\".csv\"), 'w'))\n",
        "\n",
        "L = pd.unique(steps.ID)\n",
        "begin = time()\n",
        "with multiprocessing.pool.ThreadPool(WORKER_COUNT) as pool:\n",
        "  output = list(pool.map(split_save, L))\n",
        "  pool.close()\n",
        "  pool.join()\n",
        "end = time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtaUNf5hOoII"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTKBCR7TOMcx"
      },
      "outputs": [],
      "source": [
        "# @title Momentary Stress Algorithm\n",
        "\n",
        "\n",
        "\n",
        "msa = pd.read_csv(gfile.Open(os.path.join(root_folder,\"MOMENTARY_STRESS_ALGORITHM_DATA.csv.csv\", 'r')))\n",
        "\n",
        "list_arrays = []\n",
        "for i, row in msa.iterrows():\n",
        "  msa = pd.DataFrame()\n",
        "\n",
        "  msa['millis_from_start_time'] = get_arrays(row,'offsets')*1000*60\n",
        "  msa['hrv_shannon_entropy_rr'] = get_arrays(row,'hrv_shannon_entropy_rr')\n",
        "  msa['hrv_shannon_entropy_rrd'] = get_arrays(row,'hrv_shannon_entropy_rrd')\n",
        "  msa['hrv_percentage_of_nn_30'] = get_arrays(row,'hrv_percentage_of_nn_30')\n",
        "  msa['ceda_magnitude_real_micro_siemens'] = get_arrays(row,'ceda_magnitude_real_micro_siemens')\n",
        "  msa['ceda_slope_real_micro_siemens'] = get_arrays(row,'ceda_slope_real_micro_siemens')\n",
        "  msa['rmssd_percentile_0595'] = get_arrays(row,'rmssd_percentile_0595')\n",
        "  msa['sdnn_percentile_0595'] = get_arrays(row,'sdnn_percentile_0595')\n",
        "  msa['msa_probability'] = get_arrays(row,'msa_probability')\n",
        "  msa['hrv_percent_good'] = get_arrays(row,'hrv_percent_good')\n",
        "  msa['hrv_rr_80th_percentile_mean'] = get_arrays(row,'hrv_rr_80th_percentile_mean')\n",
        "  msa['hrv_rr_20th_percentile_mean'] = get_arrays(row,'hrv_rr_20th_percentile_mean')\n",
        "  msa['hrv_rr_median'] = get_arrays(row,'hrv_rr_median')\n",
        "  msa['hrv_rr_mean'] = get_arrays(row,'hrv_rr_mean')\n",
        "  msa['hr_at_rest_mean'] = get_arrays(row,'hr_at_rest_mean')\n",
        "  msa['skin_temperature_magnitude'] = get_arrays(row,'skin_temperature_magnitude')\n",
        "  msa['skin_temperature_slope'] = get_arrays(row,'skin_temperature_slope')\n",
        "  msa['participant_id'] = row['participant_id']\n",
        "  msa['activity_time'] = row['activity_time']\n",
        "  msa['activity_tm_timezone_offset'] = row['activity_tm_timezone_offset']\n",
        "  list_arrays.append(msa)\n",
        "  #except:\n",
        "  #  print('Could not process PID: ' + str(row['participant_id']))\n",
        "\n",
        "msa = pd.concat(list_arrays, ignore_index=True)\n",
        "\n",
        "msa['activity_time'] = pd.to_datetime(msa['activity_time'])\n",
        "msa['activity_time_local'] = msa['activity_time']\n",
        "msa['activity_time_true'] = msa['activity_time_local'] + msa['millis_from_start_time'].astype('timedelta64[ms]')\n",
        "msa['minute_today'] = msa['activity_time_true'].dt.hour*60 + msa['activity_time_true'].dt.minute\n",
        "msa['count'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYjx8QeJ7i75"
      },
      "outputs": [],
      "source": [
        "msa.rename(columns={'activity_time_true': 'DT', 'participant_id': 'ID'}, inplace=True)\n",
        "msa[['ID',\n",
        "       'DT',\n",
        "       'hrv_shannon_entropy_rr',\n",
        "       'hrv_shannon_entropy_rrd',\n",
        "       'hrv_percentage_of_nn_30',\n",
        "       'ceda_magnitude_real_micro_siemens',\n",
        "       'ceda_slope_real_micro_siemens',\n",
        "       'rmssd_percentile_0595',\n",
        "       'sdnn_percentile_0595',\n",
        "       'msa_probability',\n",
        "       'hrv_percent_good',\n",
        "       'hrv_rr_80th_percentile_mean',\n",
        "       'hrv_rr_20th_percentile_mean',\n",
        "       'hrv_rr_median',\n",
        "       'hrv_rr_mean',\n",
        "       'hr_at_rest_mean',\n",
        "       'skin_temperature_magnitude',\n",
        "       'skin_temperature_slope',\n",
        "       ]].to_csv(gfile.Open(os.path.join(root_folder,\"MOMENTARY_STRESS_ALGORITHM_DATA_PROCESSED.csv\"), 'w'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pnp0O34s8X59"
      },
      "outputs": [],
      "source": [
        "WORKER_COUNT = 20\n",
        "gfile.MakeDirs(gfile.Open(os.path.join(root_folder,\"momentary_stress_algorithm\")))\n",
        "\n",
        "def split_save(L):\n",
        "  msa[msa['ID']==L].to_csv(gfile.Open(os.path.join(root_folder,\"momentary_stress_algorithm\",\"momentary_stress_algorithm_\"+str(L)+\".csv\"), 'w'))\n",
        "\n",
        "L = pd.unique(msa.ID)\n",
        "begin = time()\n",
        "with multiprocessing.pool.ThreadPool(WORKER_COUNT) as pool:\n",
        "  output = list(pool.map(split_save, L))\n",
        "  pool.close()\n",
        "  pool.join()\n",
        "end = time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVNA6-n9_P2L"
      },
      "outputs": [],
      "source": [
        "# @title EDA\n",
        "eda = pd.read_csv(gfile.Open(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/CONTINUOUS_EDA_DATA.csv.csv\", 'r'))\n",
        "list_arrays = []\n",
        "for i, row in eda.iterrows():\n",
        "  eda = pd.DataFrame()\n",
        "  try:\n",
        "    eda['millis_from_start_time'] = get_arrays(row,'millis_from_start_time')\n",
        "    eda['eda_level_real'] = get_arrays(row,'eda_level_real')\n",
        "    eda['leads_contact_counts'] = get_arrays(row,'leads_contact_counts')\n",
        "    eda['participant_id'] = row['participant_id']\n",
        "    eda['activity_time'] = row['activity_time']\n",
        "    eda['activity_tm_timezone_offset'] = row['activity_tm_timezone_offset']\n",
        "    list_arrays.append(eda)\n",
        "  except:\n",
        "    print('Could not process PID: ' + str(row['participant_id']))\n",
        "\n",
        "eda = pd.concat(list_arrays, ignore_index=True)\n",
        "\n",
        "eda['activity_time'] = pd.to_datetime(eda['activity_time'])\n",
        "eda['activity_time_local'] = eda['activity_time'] + eda['activity_tm_timezone_offset'].astype('timedelta64[m]')\n",
        "eda['activity_time_true'] = eda['activity_time_local'] + eda['millis_from_start_time'].astype('timedelta64[ms]')\n",
        "eda['minute_today'] = eda['activity_time_true'].dt.hour*60 + eda['activity_time_true'].dt.minute\n",
        "eda['count'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZlBNAqx_sMa"
      },
      "outputs": [],
      "source": [
        "eda.rename(columns={'activity_time_true': 'DT', 'participant_id': 'ID'}, inplace=True)\n",
        "eda[['ID',\n",
        "     'DT',\n",
        "     'eda_level_real',\n",
        "     'leads_contact_counts',\n",
        "       ]].to_csv(gfile.Open(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/CONTINUOUS_EDA_DATA_PROCESSED.csv\", 'w'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaW1CPSMAi19"
      },
      "outputs": [],
      "source": [
        "WORKER_COUNT = 20\n",
        "gfile.MakeDirs(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/continuous_eda\")\n",
        "\n",
        "def split_save(L):\n",
        "  eda[eda['ID']==L].to_csv(gfile.Open(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/continuous_eda/continuous_eda_\"+str(L)+\".csv\", 'w'))\n",
        "\n",
        "L = pd.unique(eda.ID)\n",
        "begin = time()\n",
        "with multiprocessing.pool.ThreadPool(WORKER_COUNT) as pool:\n",
        "  output = list(pool.map(split_save, L))\n",
        "  pool.close()\n",
        "  pool.join()\n",
        "end = time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyL6H1emEXVa"
      },
      "outputs": [],
      "source": [
        "# @title Skin Temperature\n",
        "skintemp = pd.read_csv(gfile.Open(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/WRIST_TEMPERATURE_DATA.csv.csv\", 'r'))\n",
        "skintemp.reset_index(inplace=True)\n",
        "list_arrays = []\n",
        "for i, row in skintemp.iterrows():\n",
        "  tmp = pd.DataFrame()\n",
        "  tmp['millis_from_start_time'] = pd.Series(range(0,1440))*1000*60\n",
        "  tmp['wrist_temperatures'] = get_arrays(row,'wrist_temperatures')\n",
        "  tmp['participant_id'] = row['participant_id']\n",
        "  tmp['activity_time'] = row['activity_time']\n",
        "  tmp['activity_tm_timezone_offset'] = row['tz_offset_minutes']\n",
        "  list_arrays.append(tmp)\n",
        "\n",
        "skintemp = pd.concat(list_arrays, ignore_index=True)\n",
        "skintemp = skintemp[skintemp['wrist_temperatures']\u003e=0]\n",
        "\n",
        "skintemp['activity_time'] = pd.to_datetime(skintemp['activity_time'])\n",
        "skintemp['activity_time_local'] = skintemp['activity_time']\n",
        "skintemp['activity_time_true'] = skintemp['activity_time_local'] + skintemp['millis_from_start_time'].astype('timedelta64[ms]')\n",
        "skintemp['minute_today'] = skintemp['activity_time_true'].dt.hour*60 + skintemp['activity_time_true'].dt.minute\n",
        "skintemp['count'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqUHejmeQQto"
      },
      "outputs": [],
      "source": [
        "skintemp.rename(columns={'activity_time_true': 'DT', 'participant_id': 'ID'}, inplace=True)\n",
        "skintemp[['ID',\n",
        "     'DT',\n",
        "     'wrist_temperatures',\n",
        "       ]].to_csv(gfile.Open(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/WRIST_TEMPERATURE_DATA_PROCESSED.csv\", 'w'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaxRnDyhQefY"
      },
      "outputs": [],
      "source": [
        "WORKER_COUNT = 20\n",
        "gfile.MakeDirs(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/wrist_temperatures\")\n",
        "\n",
        "def split_save(L):\n",
        "  skintemp[skintemp['ID']==L].to_csv(gfile.Open(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/wrist_temperatures/wrist_temperatures_\"+str(L)+\".csv\", 'w'))\n",
        "\n",
        "L = pd.unique(skintemp.ID)\n",
        "begin = time()\n",
        "with multiprocessing.pool.ThreadPool(WORKER_COUNT) as pool:\n",
        "  output = list(pool.map(split_save, L))\n",
        "  pool.close()\n",
        "  pool.join()\n",
        "end = time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIWmGlKWQpZL"
      },
      "outputs": [],
      "source": [
        "# @title Sleep Coefficient\n",
        "sleepcoefficient = pd.read_csv(gfile.Open(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/SLEEP_COEFFICIENT_COMPACT_DATA.csv.csv\", 'r'))\n",
        "sleepcoefficient.reset_index(inplace=True)\n",
        "list_arrays = []\n",
        "for i, row in sleepcoefficient.iterrows():\n",
        "  try:\n",
        "    tmp = pd.DataFrame()\n",
        "    tmp['millis_from_start_time'] = pd.Series(range(0,1440))*1000*60\n",
        "    tmp['sleep_coefficient'] = get_arrays(row,'sleep_coefficient')\n",
        "    tmp['is_on_wrist'] = get_arrays(row,'is_on_wrist',bool).astype('int')\n",
        "    tmp['participant_id'] = row['participant_id']\n",
        "    tmp['activity_time'] = row['activity_time']\n",
        "    tmp['activity_tm_timezone_offset'] = row['tz_offset_minutes']\n",
        "    list_arrays.append(tmp)\n",
        "  except:\n",
        "    print('Could not process PID: ' + str(row['participant_id']))\n",
        "\n",
        "sleepcoefficient = pd.concat(list_arrays, ignore_index=True)\n",
        "\n",
        "sleepcoefficient['activity_time'] = pd.to_datetime(sleepcoefficient['activity_time'])\n",
        "sleepcoefficient['activity_time_local'] = sleepcoefficient['activity_time']\n",
        "sleepcoefficient['activity_time_true'] = sleepcoefficient['activity_time_local'] + sleepcoefficient['millis_from_start_time'].astype('timedelta64[ms]')\n",
        "sleepcoefficient['minute_today'] = sleepcoefficient['activity_time_true'].dt.hour*60 + sleepcoefficient['activity_time_true'].dt.minute\n",
        "sleepcoefficient['count'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_F7BVEJa1IR"
      },
      "outputs": [],
      "source": [
        "sleepcoefficient.rename(columns={'activity_time_true': 'DT', 'participant_id': 'ID'}, inplace=True)\n",
        "sleepcoefficient[['ID',\n",
        "     'DT',\n",
        "     'sleep_coefficient',\n",
        "      'is_on_wrist',\n",
        "       ]].to_csv(gfile.Open(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/SLEEP_COEFFICIENT_COMPACT_DATA_PROCESSED.csv\", 'w'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mnNEaqeNYEL"
      },
      "outputs": [],
      "source": [
        "sleepcoefficient = pd.read_csv(gfile.Open(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/SLEEP_COEFFICIENT_COMPACT_DATA_PROCESSED.csv\", 'r'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgoj3Vfza7tk"
      },
      "outputs": [],
      "source": [
        "WORKER_COUNT = 20\n",
        "gfile.MakeDirs(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/sleep_coefficient\")\n",
        "\n",
        "def split_save(L):\n",
        "  sleepcoefficient[sleepcoefficient['ID']==L].to_csv(gfile.Open(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/sleep_coefficient/sleep_coefficient_\"+str(L)+\".csv\", 'w'))\n",
        "\n",
        "L = pd.unique(sleepcoefficient.ID)\n",
        "begin = time()\n",
        "with multiprocessing.pool.ThreadPool(WORKER_COUNT) as pool:\n",
        "  output = list(pool.map(split_save, L))\n",
        "  pool.close()\n",
        "  pool.join()\n",
        "end = time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xd0AEV1-Swel"
      },
      "outputs": [],
      "source": [
        "# @title SpO2\n",
        "spo2 = pd.read_csv(gfile.Open(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/ABSOLUTE_SPO2_DATA.csv.csv\", 'r'))\n",
        "spo2.reset_index(inplace=True)\n",
        "list_arrays = []\n",
        "\n",
        "spo2['activity_time'] = pd.to_datetime(spo2['activity_time'], format='mixed')\n",
        "spo2['activity_time_local'] = spo2['activity_time']\n",
        "spo2['activity_time_true'] = spo2['activity_time_local']\n",
        "spo2['minute_today'] = spo2['activity_time_true'].dt.hour*60 + spo2['activity_time_true'].dt.minute\n",
        "spo2['count'] = 1\n",
        "spo2['valid'] = spo2['valid'].astype('int')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gK7kU3cPZj0X"
      },
      "outputs": [],
      "source": [
        "spo2.rename(columns={'activity_time_true': 'DT', 'participant_id': 'ID'}, inplace=True)\n",
        "spo2[['ID',\n",
        "     'DT',\n",
        "     'value',\n",
        "      'confidence',\n",
        "      'coverage',\n",
        "      'valid',\n",
        "       ]].to_csv(gfile.Open(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/ABSOLUTE_SPO2_DATA_PROCESSED.csv\", 'w'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L04xG4HKZo1R"
      },
      "outputs": [],
      "source": [
        "WORKER_COUNT = 20\n",
        "gfile.MakeDirs(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/spo2\")\n",
        "\n",
        "def split_save(L):\n",
        "  spo2[spo2['ID']==L].to_csv(gfile.Open(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/spo2/spo2_\"+str(L)+\".csv\", 'w'))\n",
        "\n",
        "L = pd.unique(spo2.ID)\n",
        "begin = time()\n",
        "with multiprocessing.pool.ThreadPool(WORKER_COUNT) as pool:\n",
        "  output = list(pool.map(split_save, L))\n",
        "  pool.close()\n",
        "  pool.join()\n",
        "end = time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzQdEvaUSzS2"
      },
      "outputs": [],
      "source": [
        "# @title Grok\n",
        "grok = pd.read_csv(gfile.Open(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/GROK_FEATURE_DATA.csv.csv\", 'r'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFEE-gXnMyPZ"
      },
      "outputs": [],
      "source": [
        "#grok.reset_index(inplace=True)\n",
        "list_arrays = []\n",
        "for i, row in grok.iterrows():\n",
        "  tmp = pd.DataFrame()\n",
        "  tmp['millis_from_start_time'] = get_arrays(row,'millis_from_start_of_day')\n",
        "  tmp['jerk_auto'] = get_arrays(row,'jerk_auto')\n",
        "  tmp['step_count'] = get_arrays(row,'step_count')\n",
        "  tmp['log_energy'] = get_arrays(row,'log_energy')\n",
        "  tmp['covariance'] = get_arrays(row,'covariance')\n",
        "  tmp['log_energy_ratio'] = get_arrays(row,'log_energy_ratio')\n",
        "  tmp['zero_crossing_std'] = get_arrays(row,'zero_crossing_std')\n",
        "  tmp['zero_crossing_avg'] = get_arrays(row,'zero_crossing_avg')\n",
        "  tmp['axis_mean'] = get_arrays(row,'axis_mean')\n",
        "  tmp['altim_std'] = get_arrays(row,'altim_std')\n",
        "  tmp['kurtosis'] = get_arrays(row,'kurtosis')\n",
        "  tmp['participant_id'] = row['participant_id']\n",
        "  tmp['activity_time'] = row['activity_time']\n",
        "  tmp['activity_tm_timezone_offset'] = row['activity_tm_timezone_offset']\n",
        "  list_arrays.append(tmp)\n",
        "\n",
        "grok = pd.concat(list_arrays, ignore_index=True)\n",
        "\n",
        "grok['activity_time'] = pd.to_datetime(grok['activity_time'])\n",
        "grok['activity_time_local'] = grok['activity_time'] + grok['activity_tm_timezone_offset'].astype('timedelta64[m]')\n",
        "grok['activity_time_true'] = grok['activity_time_local'] + grok['millis_from_start_time'].astype('timedelta64[ms]')\n",
        "grok['minute_today'] = grok['activity_time_true'].dt.hour*60 + grok['activity_time_true'].dt.minute\n",
        "grok['count'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpuGWAcpf3UZ"
      },
      "outputs": [],
      "source": [
        "grok.rename(columns={'activity_time_true': 'DT', 'participant_id': 'ID'}, inplace=True)\n",
        "grok[['ID',\n",
        "     'DT',\n",
        "     'jerk_auto',\n",
        "      'step_count',\n",
        "      'log_energy',\n",
        "      'covariance',\n",
        "      'log_energy_ratio',\n",
        "      'zero_crossing_std',\n",
        "      'zero_crossing_avg',\n",
        "      'axis_mean',\n",
        "      'altim_std',\n",
        "      'kurtosis',\n",
        "       ]].to_csv(gfile.Open(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/GROK_FEATURE_DATA_PROCESSED.csv\", 'w'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsh6AI93wQSZ"
      },
      "outputs": [],
      "source": [
        "WORKER_COUNT = 20\n",
        "gfile.MakeDirs(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/grok\")\n",
        "\n",
        "def split_save(L):\n",
        "  grok[grok['ID']==L].to_csv(gfile.Open(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/grok/grok_\"+str(L)+\".csv\", 'w'))\n",
        "\n",
        "L = pd.unique(grok.ID)\n",
        "begin = time()\n",
        "with multiprocessing.pool.ThreadPool(WORKER_COUNT) as pool:\n",
        "  output = list(pool.map(split_save, L))\n",
        "  pool.close()\n",
        "  pool.join()\n",
        "end = time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0ZP4ABVS3aK"
      },
      "outputs": [],
      "source": [
        "# @title Heart Rate\n",
        "heart_rate = pd.read_csv(gfile.Open(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/HEART_RATE_DATA.csv.csv\", 'r'))\n",
        "heart_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEGdNfOpNFbS"
      },
      "outputs": [],
      "source": [
        "heart_rate.reset_index(inplace=True)\n",
        "list_arrays = []\n",
        "for i, row in heart_rate.iterrows():\n",
        "  tmp = pd.DataFrame()\n",
        "  tmp['millis_from_start_time'] = pd.Series(range(0,1440))*1000*60\n",
        "  tmp['hr'] = get_arrays(row,'hr')\n",
        "  tmp['participant_id'] = row['participant_id']\n",
        "  tmp['activity_time'] = row['activity_time']\n",
        "  tmp['activity_tm_timezone_offset'] = row['tz_offset_minutes']\n",
        "  list_arrays.append(tmp)\n",
        "\n",
        "heart_rate = pd.concat(list_arrays, ignore_index=True)\n",
        "\n",
        "heart_rate['activity_time'] = pd.to_datetime(heart_rate['activity_time'])\n",
        "heart_rate['activity_time_local'] = heart_rate['activity_time']\n",
        "heart_rate['activity_time_true'] = heart_rate['activity_time_local'] + heart_rate['millis_from_start_time'].astype('timedelta64[ms]')\n",
        "heart_rate['minute_today'] = heart_rate['activity_time_true'].dt.hour*60 + heart_rate['activity_time_true'].dt.minute\n",
        "heart_rate['count'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wc5mCx8rgiMc"
      },
      "outputs": [],
      "source": [
        "heart_rate.rename(columns={'activity_time_true': 'DT', 'participant_id': 'ID'}, inplace=True)\n",
        "heart_rate[['ID',\n",
        "     'DT',\n",
        "     'hr',\n",
        "       ]].to_csv(gfile.Open(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/HEART_RATE_PROCESSED.csv\", 'w'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsgoyxazgiP-"
      },
      "outputs": [],
      "source": [
        "WORKER_COUNT = 20\n",
        "gfile.MakeDirs(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/heart_rate\")\n",
        "\n",
        "def split_save(L):\n",
        "  heart_rate[heart_rate['ID']==L].to_csv(gfile.Open(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/heart_rate/heart_rate_\"+str(L)+\".csv\", 'w'))\n",
        "\n",
        "L = pd.unique(heart_rate.ID)\n",
        "begin = time()\n",
        "with multiprocessing.pool.ThreadPool(WORKER_COUNT) as pool:\n",
        "  output = list(pool.map(split_save, L))\n",
        "  pool.close()\n",
        "  pool.join()\n",
        "end = time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcpsoCsZS8R7"
      },
      "outputs": [],
      "source": [
        "# @title Heart Rate Variability\n",
        "heart_rate_variability = pd.read_csv(gfile.Open(\"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_data_lsm/HEART_RATE_VARIABILITY_DATA.csv.csv\", 'r'))\n",
        "heart_rate_variability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAr54mTEgi-a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ko4nL3jNgjDj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuoZaKYRgjJ8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "m3MuXBCnCFKm",
        "BP_N0DKA3LTT",
        "g9eV4bhCI4d6",
        "64IZZmk3I6-n",
        "5lfcwi0pJoEW",
        "xUz7_W7mOVpR",
        "-Xhg9wJAfppb"
      ],
      "last_runtime": {
        "build_target": "//fitbit/research/sensing/metabolic_health/colab:rl_colab",
        "kind": "shared"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/downstream_tasks/[LSM]_LSM_Preprocessing_Metabolic_Health.ipynb",
          "timestamp": 1738170188157
        },
        {
          "file_id": "1_dBYxvMkCX-wlmls8gxYwADj65MKrBZw",
          "timestamp": 1735854650095
        },
        {
          "file_id": "1FlWXm9vMR5MaUHL8TsxWKR7c9EcLqfUW",
          "timestamp": 1730308037300
        },
        {
          "file_id": "11UdNvUeAS6o3tfrz49CE4UNNpPX3OygR",
          "timestamp": 1729539561855
        },
        {
          "file_id": "1yZ8pQR8l2O65aCH6xYYYyQVSDnxtp44F",
          "timestamp": 1710526257742
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
