{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNLa2VjaS7At"
      },
      "source": [
        "**To run the experiments for DWB use this AoD grant:**\n",
        "\n",
        "https://grants.corp.google.com/#/grants?request=20h%2Fchr-ards-dwb-deid-eng-policy:r\u0026reason=b%2F264556558%20-%20DWB%20RQ%20and%20Analysis\n",
        "\n",
        "\n",
        "**To run the experiments for Metabolics use this AoD grant:**\n",
        "\n",
        "https://grants.corp.google.com/#/grants?request=20h%2Fchr-ards-metabolichealth-deid-eng-team-sphinx:r\u0026reason=b%2F283774208\n",
        "\n",
        "**To run the experiments for Fitbit Prod use this AoD grant:**\n",
        "\n",
        "https://grants.corp.google.com/#/grants?request=20h%2Fchr-ards-fitbit-prod-research-deid-eng-team:r\u0026reason=%22b%2F285178698%22\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3MuXBCnCFKm"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e8d1tzwqK46"
      },
      "outputs": [],
      "source": [
        "from absl import app\n",
        "from ast import literal_eval\n",
        "from collections.abc import Sequence\n",
        "import csv\n",
        "import datetime\n",
        "import fnmatch\n",
        "from google3.pyglib import gfile  # This is repeated, you might want to remove one\n",
        "from colabtools import googlefiles\n",
        "import json\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.dates as mdates\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib.patches import Patch, Rectangle\n",
        "import matplotlib.pyplot as plt\n",
        "import multiprocessing\n",
        "import multiprocessing.pool\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "import pdb\n",
        "import random  # This is repeated, you might want to remove one\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import shapiro\n",
        "from scipy.stats import zscore\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.graphics.gofplots import qqplot\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from time import sleep, time\n",
        "from google3.pyglib import gfile\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vodaFlKuVNj-"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "\n",
        "data.append({'type':  'steps',\n",
        "     'raw_file': 'STEPS_COMPACT_DATA',\n",
        "     'features_to_extract': ['steps'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "data.append({'type':  'momentary_stress_algorithm',\n",
        "     'raw_file': 'MOMENTARY_STRESS_ALGORITHM_DATA',\n",
        "     'features_to_extract': ['hrv_shannon_entropy_rr','hrv_shannon_entropy_rrd','hrv_percentage_of_nn_30','ceda_magnitude_real_micro_siemens','ceda_slope_real_micro_siemens','rmssd_percentile_0595','sdnn_percentile_0595','msa_probability','hrv_percent_good','hrv_rr_80th_percentile_mean','hrv_rr_20th_percentile_mean','hrv_rr_median','hrv_rr_mean','hr_at_rest_mean','skin_temperature_magnitude','skin_temperature_slope'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "data.append({'type':  'ceda',\n",
        "     'raw_file': 'CONTINUOUS_EDA_DATA',\n",
        "     'features_to_extract': ['eda_level_real','leads_contact_counts'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "data.append({'type':  'wrist_temperature',\n",
        "     'raw_file': 'WRIST_TEMPERATURE_DATA',\n",
        "     'features_to_extract': ['wrist_temperatures'],\n",
        "     'timezone_offset_column': 'tz_offset_minutes'})\n",
        "\n",
        "data.append({'type':  'sleep_coefficient',\n",
        "     'raw_file': 'SLEEP_COEFFICIENT_COMPACT_DATA',\n",
        "     'features_to_extract': ['sleep_coefficient','is_on_wrist'],\n",
        "     'timezone_offset_column': 'tz_offset_minutes'})\n",
        "\n",
        "data.append({'type':  'spo2',\n",
        "     'raw_file': 'ABSOLUTE_SPO2_DATA',\n",
        "     'features_to_extract': ['value','confidence','coverage','valid'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "data.append({'type':  'grok',\n",
        "     'raw_file': 'GROK_FEATURE_DATA',\n",
        "     'features_to_extract': ['jerk_auto','step_count','log_energy','covariance',\n",
        "                             'log_energy_ratio','zero_crossing_std',\n",
        "                             'zero_crossing_avg','axis_mean','altim_std','kurtosis'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "data.append({'type':  'heart_rate',\n",
        "     'raw_file': 'HEART_RATE_DATA',\n",
        "     'features_to_extract': ['bpm','confidence'],\n",
        "     'timezone_offset_column': 'activity_tm_timezone_offset'})\n",
        "\n",
        "\n",
        "FEATURES_TO_INCLUDE = [\n",
        "    'HR',\n",
        "    'eda_level_real',\n",
        "    'leads_contact_counts',\n",
        "    'steps',\n",
        "    'jerk_auto',\n",
        "    'log_energy',\n",
        "    'covariance',\n",
        "    'log_energy_ratio',\n",
        "    'zero_crossing_std',\n",
        "    'zero_crossing_avg',\n",
        "    'axis_mean',\n",
        "    'altim_std',\n",
        "    'kurtosis',\n",
        "    'sleep_coefficient',\n",
        "    'wrist_temperatures',\n",
        "    'hrv_shannon_entropy_rr',\n",
        "    'hrv_shannon_entropy_rrd',\n",
        "    'ceda_slope_real_micro_siemens',\n",
        "    'rmssd_percentile_0595',\n",
        "    'sdnn_percentile_0595',\n",
        "    'hrv_percent_good',\n",
        "    'hrv_rr_80th_percentile_mean',\n",
        "    'hrv_rr_20th_percentile_mean',\n",
        "    'hrv_rr_median',\n",
        "    'hr_at_rest_mean',\n",
        "    'skin_temperature_slope',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bttHwlawCHfG"
      },
      "outputs": [],
      "source": [
        "# Helper Functions\n",
        "def visualize_features(array_feature):\n",
        "\n",
        "  fig = plt.figure(figsize=(20, 7))\n",
        "  ax1 = plt.subplot2grid((1, 12), (0, 0), colspan=12)\n",
        "  group = array_feature.numpy()\n",
        "\n",
        "  ax1 = sns.heatmap(group.T, cmap=\"Reds\", cbar=True, linewidths=0.0,\n",
        "                    linecolor='black', alpha=0.8, ax=ax1, yticklabels=True)\n",
        "\n",
        "  for tick in ax1.get_xticklabels():\n",
        "      tick.set_fontname('Ubuntu')\n",
        "      tick.set_style('italic')\n",
        "  ax1.tick_params(axis='x', labelsize=10.5)\n",
        "\n",
        "  for tick in ax1.get_yticklabels():\n",
        "      tick.set_fontname('Ubuntu')\n",
        "  ax1.tick_params(axis='y', labelsize=10.5)\n",
        "\n",
        "  plt.xticks(rotation=45)  # Rotate labels for better readability\n",
        "  plt.tight_layout()\n",
        "\n",
        "  ax1.set_ylabel(\"Feature\", fontname='Ubuntu', fontsize=14)\n",
        "\n",
        "  ax1.axhline(y=0, color='k',linewidth=1, alpha=1)\n",
        "  ax1.axhline(y=group.shape[1], color='k', alpha=1,linewidth=1)\n",
        "  ax1.axvline(x=0, color='k',linewidth=1, alpha=1)\n",
        "  ax1.axvline(x=group.shape[0], color='k', alpha=1,linewidth=1);\n",
        "\n",
        "  for i in np.arange(0,group.shape[0],60):\n",
        "    ax1.axvline(x=i, color='k', alpha=0.4,linewidth=1);\n",
        "  for i in np.arange(0,group.shape[1],1):\n",
        "    ax1.axhline(y=i, color='k', alpha=0.4,linewidth=1);\n",
        "\n",
        "  fig.savefig(f'example_heatmap.pdf', format='pdf', bbox_inches=\"tight\")\n",
        "  %download_file example_heatmap.pdf\n",
        "  plt.show()\n",
        "\n",
        "def parse_dwb_tfrecord(example):\n",
        "  feature = {\n",
        "    'phq_intake_score': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'gad_intake_score': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'pss_score': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'sleep_disturbance_score': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'sleep_impairment_score': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'extraversion_score': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'smartphoneaddiction_score': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'phq_intake_score_binary': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'gad_intake_score_binary': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'pss_score_binary': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'smartphoneaddiction_score_binary': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'age': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'gender_group': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'bmi': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'array_raw': tf.io.FixedLenFeature([], tf.string),\n",
        "  }\n",
        "  example = tf.io.parse_single_example(example, feature)\n",
        "  array_feature = tf.io.parse_tensor(example['array_raw'], out_type=tf.double)\n",
        "  phq_intake_score = example['phq_intake_score']\n",
        "  gad_intake_score = example['gad_intake_score']\n",
        "  pss_score = example['pss_score']\n",
        "  sleep_disturbance_score = example['sleep_disturbance_score']\n",
        "  sleep_impairment_score = example['sleep_impairment_score']\n",
        "  extraversion_score = example['extraversion_score']\n",
        "  smartphoneaddiction_score = example['smartphoneaddiction_score']\n",
        "  phq_intake_score_binary = example['phq_intake_score_binary']\n",
        "  gad_intake_score_binary = example['gad_intake_score_binary']\n",
        "  pss_score_binary = example['pss_score_binary']\n",
        "  smartphoneaddiction_score_binary = example['smartphoneaddiction_score_binary']\n",
        "\n",
        "  age = example['age']\n",
        "  gender_group = example['gender_group']\n",
        "  bmi = example['bmi']\n",
        "\n",
        "  return phq_intake_score, gad_intake_score, pss_score, sleep_disturbance_score, sleep_impairment_score, extraversion_score, smartphoneaddiction_score, phq_intake_score_binary, gad_intake_score_binary, pss_score_binary, smartphoneaddiction_score_binary, age, gender_group, bmi, array_feature\n",
        "\n",
        "\n",
        "\n",
        "def parse_metabolic_tfrecord(example):\n",
        "  feature = {\n",
        "    'bmi': tf.io.FixedLenFeature([], tf.float32),\n",
        "    'homa_ir': tf.io.FixedLenFeature([], tf.float32),\n",
        "    'apri': tf.io.FixedLenFeature([], tf.float32),\n",
        "    'msss': tf.io.FixedLenFeature([], tf.float32),\n",
        "    'hypertension_binary': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'hyperlipidemia_binary': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'cardiovascular_binary': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'diabetes_binary': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'anxiety_binary': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'respiratory': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'kidney_disease': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'msss_binary': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'framingham': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'age': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'gender': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'regular_menstruation_str': tf.io.FixedLenFeature([], tf.string),\n",
        "    'smoker_str': tf.io.FixedLenFeature([], tf.string),\n",
        "    'diabetes_type_str': tf.io.FixedLenFeature([], tf.string),\n",
        "    'alcohol_str': tf.io.FixedLenFeature([], tf.string),\n",
        "    'medications_str': tf.io.FixedLenFeature([], tf.string),\n",
        "    'array_raw': tf.io.FixedLenFeature([], tf.string),\n",
        "    'array_mask': tf.io.FixedLenFeature([], tf.string),\n",
        "  }\n",
        "  example = tf.io.parse_single_example(example, feature)\n",
        "  array_feature = tf.io.parse_tensor(example['array_raw'], out_type=tf.double)\n",
        "  array_mask = tf.io.parse_tensor(example['array_mask'], out_type=tf.double)\n",
        "  bmi = example['bmi']\n",
        "  homa_ir = example['homa_ir']\n",
        "  apri = example['apri']\n",
        "  msss = example['msss']\n",
        "  hypertension_binary = example['hypertension_binary']\n",
        "  hyperlipidemia_binary = example['hyperlipidemia_binary']\n",
        "  cardiovascular_binary = example['cardiovascular_binary']\n",
        "  diabetes_binary = example['diabetes_binary']\n",
        "  anxiety_binary = example['anxiety_binary']\n",
        "  respiratory = example['respiratory']\n",
        "  kidney_disease = example['kidney_disease']\n",
        "  #homa_ir_binary = example['homa_ir_binary']\n",
        "  msss_binary = example['msss_binary']\n",
        "  framingham = example['framingham']\n",
        "  age = example['age']\n",
        "  gender = example['gender']\n",
        "  regular_menstruation_str = example['regular_menstruation_str']\n",
        "  smoker_str = example['smoker_str']\n",
        "  diabetes_type_str = example['diabetes_type_str']\n",
        "  alcohol_str = example['alcohol_str']\n",
        "  medications_str = example['medications_str']\n",
        "  return  bmi, homa_ir, apri, msss, hypertension_binary, hyperlipidemia_binary, cardiovascular_binary, diabetes_binary, anxiety_binary, respiratory, kidney_disease, msss_binary, framingham, age, gender, regular_menstruation_str, smoker_str, diabetes_type_str, alcohol_str, medications_str, array_feature, array_mask\n",
        "\n",
        "\n",
        "def parse_activity_tfrecord(example):\n",
        "  feature = {\n",
        "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'array_raw': tf.io.FixedLenFeature([], tf.string),\n",
        "    'array_mask': tf.io.FixedLenFeature([], tf.string),\n",
        "  }\n",
        "  example = tf.io.parse_single_example(example, feature)\n",
        "  array = tf.io.parse_tensor(example['array_raw'], out_type=tf.double)\n",
        "  mask = tf.io.parse_tensor(example['array_mask'], out_type=tf.bool)\n",
        "  label = example['label']\n",
        "\n",
        "  return  array, mask, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r7oRyHPFLmm"
      },
      "source": [
        "# STEP 1: Read TFrecords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ger_Rwr9wDC"
      },
      "outputs": [],
      "source": [
        "\n",
        "dwb_record_folder = \"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_tfrecords_24h_missingness_80/\"\n",
        "records = gfile.ListDir(dwb_record_folder)\n",
        "number_records = len(records)\n",
        "print('DWB Records: ', number_records)\n",
        "\n",
        "\n",
        "metabolic_record_folder = \"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-metabolichealth/deid/exp/aliheydari/metabolic_tfrecords_daily_alllabels_v07/\"\n",
        "records = gfile.ListDir(metabolic_record_folder)\n",
        "number_records = len(records)\n",
        "print('Metabolic Records: ', number_records)\n",
        "\n",
        "activity_record_folder = \"/namespace/fitbit-medical-sandboxes/jg/partner/encrypted/chr-ards-fitbit-prod-research/deid/exp/dmcduff/ttl=52w/lsm_v2/activities_tfrecords_24h_missingness_80\"\n",
        "records = gfile.ListDir(activity_record_folder)\n",
        "number_records = len(records)\n",
        "print('Activity Records: ', number_records)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDC23FScUxdq"
      },
      "outputs": [],
      "source": [
        "with gfile.GFile('/namespace/fitbit-medical-sandboxes/jg/partner/encrypted/chr-ards-fitbit-prod-research/deid/exp/dmcduff/ttl=52w/lsm_v2/datasets/raw/activities_all_val.csv', 'r') as f:\n",
        "  d_val = pd.read_csv(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FewWt5ZggPe"
      },
      "outputs": [],
      "source": [
        "activity_classes = ['Walk',\n",
        "                    'Bike',\n",
        "                    'Sport',\n",
        "                    'Run',\n",
        "                    'Aerobics',\n",
        "                    'Elliptical',\n",
        "                    'Treadmill',\n",
        "                    'Spinning',\n",
        "                    'Weightlifting',\n",
        "                    'Swim',\n",
        "                    'Yoga',\n",
        "                    'Circuit Training',\n",
        "                    'Hike',\n",
        "                    'Tennis',\n",
        "                    'CrossFit',\n",
        "                    'Core training',\n",
        "                    'Pilates',\n",
        "                    'Stairclimber',\n",
        "                    'Bootcamp',\n",
        "                    'Dancing',\n",
        "                    'Indoor climbing',\n",
        "                    'Mountain Bike',\n",
        "                    'Golf',\n",
        "                    'Kickboxing',\n",
        "                    'Martial Arts',\n",
        "                    'Skiing',\n",
        "                    'Rollerblading',\n",
        "                    'Snowboarding',\n",
        "                    'Kayaking',\n",
        "                    'Surfing',\n",
        "                    'Paddleboarding']\n",
        "\n",
        "d_val[d_val['activity_name'].isin(activity_classes)].groupby('activity_name').first().head(98)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kapyJESPsti"
      },
      "outputs": [],
      "source": [
        "plotOn = True\n",
        "Xd = []\n",
        "labels = []\n",
        "if plotOn:\n",
        "  for r in records:\n",
        "    dataset = tf.data.TFRecordDataset(os.path.join(activity_record_folder, r))\n",
        "    dataset = dataset.map(parse_activity_tfrecord)\n",
        "    for array, mask, label in dataset:\n",
        "      print(r, label.numpy())\n",
        "      arr = array.numpy()\n",
        "      arr[mask] = np.nan\n",
        "      Xd.append(np.nan_to_num(np.expand_dims(np.nanmean(arr, axis=0),1),0))\n",
        "      labels.append(label.numpy().item())\n",
        "Xd = np.concatenate(Xd, axis=1)\n",
        "      #visualize_features(array)\n",
        "      #plt.figure(figsize=(20,4))\n",
        "      #arr = array.numpy()\n",
        "      #feature = 'eda_level_real'\n",
        "      #plt.plot(arr[:,FEATURES_TO_INCLUDE.index(feature)])\n",
        "      #plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ou3G00tvsNTl"
      },
      "outputs": [],
      "source": [
        "# Fit PCA:\n",
        "pca = PCA()\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca)])\n",
        "plt.figure(figsize=(8,6))\n",
        "Xt = pipe.fit_transform(Xd.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fomtmG8IF4T6"
      },
      "outputs": [],
      "source": [
        "plt.hist(labels,31)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3ETuiTLKAx1"
      },
      "outputs": [],
      "source": [
        "6%5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22wPoztfsUgv"
      },
      "outputs": [],
      "source": [
        "yd = np.array(labels)\n",
        "\n",
        "plt.rcParams['axes.facecolor'] = 'white'\n",
        "plt.rcParams['axes.edgecolor'] = 'black'\n",
        "\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "\n",
        "colors = ['#32cd32', '#ffa500', '#ff2800', '#45cea2', '#9b59b6', '#999999', '#100c08']\n",
        "shapes = ['o','^','*','s','p']\n",
        "for i in np.unique(labels):\n",
        "  plt.scatter(Xt[yd == i, 0], Xt[yd == i, 1], color=colors[int(i/5)], label=activity_classes[i], marker=shapes[i%5], alpha=0.4)\n",
        "\n",
        "plt.xlabel('PCA Dim 1')\n",
        "plt.ylabel('PCA Dim 2')\n",
        "plt.axis('square')\n",
        "plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBtmgl94Cdwf"
      },
      "outputs": [],
      "source": [
        "clf = LDA()\n",
        "clf.fit(Xd.T, yd)\n",
        "#lda = LDA(n_components=None, priors=None, shrinkage=None, solver='svd',store_covariance=False, tol=0.0001)\n",
        "lda = LDA(n_components=None, priors=None, solver='svd',store_covariance=False, tol=0.0001) #  shrinkage='auto' - can help with high dimensions, solver='lsqr' - faster for large datasets\n",
        "X_r2 = lda.fit(Xd.T, yd).transform(Xd.T)\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "\n",
        "colors = ['#32cd32', '#ffa500', '#ff2800', '#45cea2', '#9b59b6', '#999999', '#100c08']\n",
        "shapes = ['o','^','*','s','p']\n",
        "for i in np.unique(labels):\n",
        "  plt.scatter(X_r2[yd == i, 0], X_r2[yd == i, 1], color=colors[int(i/5)], label=activity_classes[i], marker=shapes[i%5], alpha=0.4)\n",
        "\n",
        "plt.xlabel('LDA Dim 1')\n",
        "plt.ylabel('LDA Dim 2')\n",
        "plt.axis('square')\n",
        "plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-YuIa8fPQJt"
      },
      "outputs": [],
      "source": [
        "clf = LDA()\n",
        "clf.fit(Xd.T, yd)\n",
        "#lda = LDA(n_components=None, priors=None, shrinkage=None, solver='svd',store_covariance=False, tol=0.0001)\n",
        "lda = LDA(n_components=None, priors=None, solver='svd',store_covariance=False, tol=0.0001) #  shrinkage='auto' - can help with high dimensions, solver='lsqr' - faster for large datasets\n",
        "X_r2 = lda.fit(Xd.T, yd).transform(Xd.T)\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "\n",
        "colors = ['#32cd32', '#ffa500', '#ff2800', '#45cea2', '#9b59b6', '#999999', '#100c08']\n",
        "shapes = ['o','^','*','s','p']\n",
        "for i in np.unique(labels):\n",
        "  plt.scatter(np.mean(X_r2[yd == i, 0]), np.mean(X_r2[yd == i, 1]), color=colors[int(i/5)], label=activity_classes[i], marker=shapes[i%5], alpha=0.4)\n",
        "\n",
        "plt.xlabel('LDA Dim 1')\n",
        "plt.ylabel('LDA Dim 2')\n",
        "#plt.xlim(-1.2, 1.2)\n",
        "#plt.ylim(-1.2, 1.2)\n",
        "#plt.axis('square')\n",
        "plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtaSKyK3L33n"
      },
      "outputs": [],
      "source": [
        "record_folder = metabolic_record_folder\n",
        "records = gfile.ListDir(record_folder)\n",
        "number_records = len(records)\n",
        "\n",
        "for r in records:\n",
        "  dataset = tf.data.TFRecordDataset(os.path.join(record_folder, r))\n",
        "  dataset = dataset.map(parse_metabolic_tfrecord)\n",
        "  for bmi, homa_ir, apri, msss, hypertension_binary, hyperlipidemia_binary, cardiovascular_binary, diabetes_binary, anxiety_binary, respiratory, kidney_disease, msss_binary, framingham, age, gender, regular_menstruation_str, smoker_str, diabetes_type_str, alcohol_str, medications_str,  array_feature in dataset:\n",
        "    plt.figure(figsize=(20,4))\n",
        "    arr = array_feature.numpy()\n",
        "    feature = 'eda_level_real'\n",
        "    plt.plot(arr[:,FEATURES_TO_INCLUDE.index(feature)])\n",
        "    feature = 'ceda_magnitude_real_micro_siemens'\n",
        "    plt.plot(arr[:,FEATURES_TO_INCLUDE.index(feature)])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8gvsz28xLcm"
      },
      "outputs": [],
      "source": [
        "# Stats:\n",
        "df = pd.DataFrame({'bmi':bmi_vec, 'hypertension':hypertension_vec, 'homa_ir':homa_ir_vec, 'apri':apri_vec})\n",
        "df[df==-999] = np.nan\n",
        "df[df==-1] = np.nan\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(3,20))\n",
        "sns.heatmap(df.isnull(), cbar=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMjGCn7_D1UQ"
      },
      "source": [
        "# DWB Data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOSLVUttHnXz"
      },
      "source": [
        "## Plot Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMvXt7mfzX4q"
      },
      "outputs": [],
      "source": [
        "# Read and process TFrecords:\n",
        "record_folder = \"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_tfrecords_24h_missingness_80/\"\n",
        "records = gfile.ListDir(record_folder)\n",
        "number_records = len(records)\n",
        "\n",
        "Xd = []\n",
        "phq_intake_score_vec = []\n",
        "gad_intake_score_vec = []\n",
        "pss_score_vec = []\n",
        "sleep_disturbance_score_vec = []\n",
        "sleep_impairment_score_vec = []\n",
        "smartphoneaddiction_score_vec = []\n",
        "\n",
        "phq_intake_score_binary_vec = []\n",
        "gad_intake_score_binary_vec = []\n",
        "pss_score_binary_vec = []\n",
        "smartphoneaddiction_score_binary_vec = []\n",
        "\n",
        "age_vec = []\n",
        "gender_group_vec = []\n",
        "bmi_vec = []\n",
        "\n",
        "for r in records:\n",
        "  dataset = tf.data.TFRecordDataset(os.path.join(record_folder, r))\n",
        "  dataset = dataset.map(parse_dwb_tfrecord)\n",
        "\n",
        "  for phq_intake_score, gad_intake_score, pss_score, sleep_disturbance_score, sleep_impairment_score, extraversion_score, smartphoneaddiction_score, phq_intake_score_binary, gad_intake_score_binary, pss_score_binary, smartphoneaddiction_score_binary, age, gender_group, bmi, array_feature in dataset:\n",
        "    arr = array_feature.numpy()\n",
        "    arr = np.nan_to_num(arr)\n",
        "    arr = np.pad(arr,((10080-arr.shape[0],0),(0,0)))\n",
        "    #Xd.append(np.expand_dims(arr.flatten(), axis=0))\n",
        "    Xd.append(np.nan_to_num(np.expand_dims(np.nanmean(arr, axis=0),1),0))\n",
        "    phq_intake_score_vec.append(phq_intake_score.numpy().item())\n",
        "    gad_intake_score_vec.append(gad_intake_score.numpy().item())\n",
        "    pss_score_vec.append(pss_score.numpy().item())\n",
        "    sleep_disturbance_score_vec.append(sleep_disturbance_score.numpy().item())\n",
        "    sleep_impairment_score_vec.append(sleep_impairment_score.numpy().item())\n",
        "    smartphoneaddiction_score_vec.append(smartphoneaddiction_score.numpy().item())\n",
        "\n",
        "    phq_intake_score_binary_vec.append(phq_intake_score_binary.numpy().item())\n",
        "    gad_intake_score_binary_vec.append(gad_intake_score_binary.numpy().item())\n",
        "    pss_score_binary_vec.append(pss_score_binary.numpy().item())\n",
        "    smartphoneaddiction_score_binary_vec.append(smartphoneaddiction_score_binary.numpy().item())\n",
        "\n",
        "    age_vec.append(age.numpy().item())\n",
        "    gender_group_vec.append(gender_group.numpy().item())\n",
        "    bmi_vec.append(bmi.numpy().item())\n",
        "Xd = np.concatenate(Xd, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj0UeyVETEf0"
      },
      "outputs": [],
      "source": [
        "# Plot\n",
        "def visualize_features(array_feature):\n",
        "\n",
        "  fig = plt.figure(figsize=(20, 7))\n",
        "  ax1 = plt.subplot2grid((1, 12), (0, 0), colspan=12)\n",
        "  group = array_feature.numpy()\n",
        "\n",
        "  ax1 = sns.heatmap(group.T, cmap=\"Reds\", cbar=True, linewidths=0.0,\n",
        "                    linecolor='black', alpha=0.8, ax=ax1, yticklabels=True)\n",
        "\n",
        "  for tick in ax1.get_xticklabels():\n",
        "      tick.set_fontname('Ubuntu')\n",
        "      tick.set_style('italic')\n",
        "  ax1.tick_params(axis='x', labelsize=10.5)\n",
        "\n",
        "  for tick in ax1.get_yticklabels():\n",
        "      tick.set_fontname('Ubuntu')\n",
        "  ax1.tick_params(axis='y', labelsize=10.5)\n",
        "\n",
        "  plt.xticks(rotation=45)  # Rotate labels for better readability\n",
        "  plt.tight_layout()\n",
        "\n",
        "  ax1.set_ylabel(\"Feature\", fontname='Ubuntu', fontsize=14)\n",
        "\n",
        "  ax1.axhline(y=0, color='k',linewidth=1, alpha=1)\n",
        "  ax1.axhline(y=group.shape[1], color='k', alpha=1,linewidth=1)\n",
        "  ax1.axvline(x=0, color='k',linewidth=1, alpha=1)\n",
        "  ax1.axvline(x=group.shape[0], color='k', alpha=1,linewidth=1);\n",
        "\n",
        "  for i in np.arange(0,group.shape[0],60):\n",
        "    ax1.axvline(x=i, color='k', alpha=0.4,linewidth=1);\n",
        "  for i in np.arange(0,group.shape[1],1):\n",
        "    ax1.axhline(y=i, color='k', alpha=0.4,linewidth=1);\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "record_folder = \"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-dwb/deid/exp/dmcduff/dwb_tfrecords_24h_missingness_80/\"\n",
        "\n",
        "for r in records:\n",
        "  dataset = tf.data.TFRecordDataset(os.path.join(record_folder, r))\n",
        "  dataset = dataset.map(parse_dwb_tfrecord)\n",
        "  for phq_intake_score, gad_intake_score, pss_score, sleep_disturbance_score, sleep_impairment_score, extraversion_score, smartphoneaddiction_score, phq_intake_score_binary, gad_intake_score_binary, pss_score_binary, smartphoneaddiction_score_binary, age, gender_group, bmi, array_feature in dataset:\n",
        "    visualize_features(array_feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2egTY-7JlCF"
      },
      "source": [
        "## Compute PCA/LDA Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X64gAlF_NK2L"
      },
      "outputs": [],
      "source": [
        "# Fit PCA:\n",
        "pca = PCA()\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca)])\n",
        "plt.figure(figsize=(8,6))\n",
        "Xt = pipe.fit_transform(Xd.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJfPU59WJq1v"
      },
      "outputs": [],
      "source": [
        "labels = ['pss_score','phq_intake_score','sleep_disturbance_score','smartphoneaddiction_score']\n",
        "colors = ['red', 'green', 'blue']\n",
        "\n",
        "plt.rcParams['axes.facecolor'] = 'white'\n",
        "plt.rcParams['axes.edgecolor'] = 'black'\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 5))\n",
        "for idx, label in enumerate(labels):\n",
        "\n",
        "  # Categorize continuous labels:\n",
        "  if label == 'pss_score':\n",
        "      yd = np.array(pss_score_vec)\n",
        "      yd[yd\u003c14] = 0\n",
        "      yd[(yd\u003c22) \u0026 (yd\u003e=14)] = 1\n",
        "      yd[yd\u003e=22] = 2\n",
        "\n",
        "  if label == 'phq_score':\n",
        "      yd = np.array(phq_intake_score_vec)\n",
        "      yd[yd\u003c10] = 0\n",
        "      yd[(yd\u003c20) \u0026 (yd\u003e=10)] = 1\n",
        "      yd[yd\u003e=20] = 2\n",
        "\n",
        "  if label == 'sleep_disturbance_score':\n",
        "      yd = np.array(sleep_disturbance_score_vec)\n",
        "      yd[yd\u003c15] = 0\n",
        "      yd[(yd\u003c25) \u0026 (yd\u003e=15)] = 1\n",
        "      yd[yd\u003e=25] = 2\n",
        "\n",
        "  if label == 'smartphoneaddiction_score':\n",
        "      yd = np.array(smartphoneaddiction_score_binary_vec)\n",
        "\n",
        "  for i in range(3):\n",
        "    axes[idx].scatter(Xt[yd == i, 1], Xt[yd == i, 2], c=colors[i], label=str(i), alpha=0.4)\n",
        "\n",
        "  axes[idx].legend(['low','med','high'])\n",
        "  axes[idx].set_xlabel('PCA Dim 1')\n",
        "  axes[idx].set_ylabel('PCA Dim 2')\n",
        "  axes[idx].axis('square')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SKDCZTfQbYu"
      },
      "outputs": [],
      "source": [
        "# Fit LDA:\n",
        "\n",
        "label = 'phq_score'\n",
        "#yd = np.array(smartphoneaddiction_score_binary_vec)\n",
        "\n",
        "# Categorize continuous labels:\n",
        "if label == 'phq_score':\n",
        "    yd = np.array(phq_intake_score_vec)\n",
        "    plt.hist(yd)\n",
        "    plt.show()\n",
        "    yd[yd\u003c10] = 0\n",
        "    yd[(yd\u003c14) \u0026 (yd\u003e10)] = 1\n",
        "    yd[yd\u003e=14] = 2\n",
        "    plt.hist(yd)\n",
        "    plt.show()\n",
        "\n",
        "print(Xd.shape)\n",
        "print(len(yd))\n",
        "\n",
        "clf = LDA()\n",
        "clf.fit(Xd.T, yd)\n",
        "lda = LDA(n_components=None, priors=None, shrinkage=None, solver='svd',store_covariance=False, tol=0.0001)\n",
        "#lda = LDA(n_components=None, priors=None, shrinkage='auto', solver='lsqr',store_covariance=False, tol=0.0001) #  shrinkage='auto' - can help with high dimensions, solver='lsqr' - faster for large datasets\n",
        "X_r2 = lda.fit(Xd.T, yd).transform(Xd.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHDC-fkyIDIj"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "for i in range(3):\n",
        "    plt.scatter(X_r2[yd == i, 0], X_r2[yd == i, 1], label=str(i), alpha=0.1)\n",
        "plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
        "plt.xlabel('LDA Dim 1')\n",
        "plt.ylabel('LDA Dim 2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDhtxsn8D6iw"
      },
      "source": [
        "# Metabolic Data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ga4nV3YHrx6"
      },
      "source": [
        "## Plot Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXKe6oNM8x_N"
      },
      "outputs": [],
      "source": [
        "record_folder = metabolic_record_folder\n",
        "records = gfile.ListDir(record_folder)\n",
        "number_records = len(records)\n",
        "\n",
        "smoker_vec = []\n",
        "regular_menstruation_vec = []\n",
        "medications_vec = []\n",
        "for r in records:\n",
        "  dataset = tf.data.TFRecordDataset(os.path.join(record_folder, r))\n",
        "  dataset = dataset.map(parse_metabolic_tfrecord)\n",
        "  for bmi, homa_ir, apri, msss, hypertension_binary, hyperlipidemia_binary, cardiovascular_binary, diabetes_binary, anxiety_binary, respiratory, kidney_disease, msss_binary, framingham, age, gender, regular_menstruation_str, smoker_str, diabetes_type_str, alcohol_str, medications_str,  array_feature in dataset:\n",
        "    tmp = []\n",
        "    tmp = str(smoker_str.numpy().decode(encoding=\"utf-8\")).split(\"\\x02\")\n",
        "    if len(tmp)\u003c2:\n",
        "     tmp = str(smoker_str.numpy().decode(encoding=\"utf-8\")).split(\"\\x10\")\n",
        "    if len(tmp)\u003c2:\n",
        "     tmp = str(smoker_str.numpy().decode(encoding=\"utf-8\")).split(\"\\x03\")\n",
        "    if tmp[1] == \"Yes\":\n",
        "      smoker_vec.append(1)\n",
        "    elif tmp[1] == \"No\":\n",
        "      smoker_vec.append(0)\n",
        "    else:\n",
        "      smoker_vec.append(np.nan)\n",
        "\n",
        "    tmp = []\n",
        "    tmp = str(regular_menstruation_str.numpy().decode(encoding=\"utf-8\")).split(\"\\x02\")\n",
        "    if len(tmp)\u003c2:\n",
        "     tmp = str(regular_menstruation_str.numpy().decode(encoding=\"utf-8\")).split(\"\\x10\")\n",
        "    if len(tmp)\u003c2:\n",
        "     tmp = str(regular_menstruation_str.numpy().decode(encoding=\"utf-8\")).split(\"\\x03\")\n",
        "    if len(tmp)\u003c2:\n",
        "     tmp = str(regular_menstruation_str.numpy().decode(encoding=\"utf-8\")).split(\"\\x14\")\n",
        "    if tmp[1] == \"Yes\":\n",
        "      regular_menstruation_vec.append(1)\n",
        "    elif tmp[1] == \"No\":\n",
        "      regular_menstruation_vec.append(0)\n",
        "    else:\n",
        "      regular_menstruation_vec.append(np.nan)\n",
        "\n",
        "    tmp = []\n",
        "    try:\n",
        "      tmp = str(medications_str.numpy().decode(encoding=\"utf-8\")).split(\"\\x1d\")\n",
        "      if len(tmp)\u003c2:\n",
        "        tmp = str(medications_str.numpy().decode(encoding=\"utf-8\")).split(\"\\x10\")\n",
        "        medications_vec.append(1)\n",
        "      else:\n",
        "        medications_vec.append(0)\n",
        "    except:\n",
        "      regular_menstruation_vec.append(np.nan)\n",
        "\n",
        "    #print(str(alcohol_str.numpy().decode(encoding=\"utf-8\")).split(\"\\x02\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFZICwaiIM6l"
      },
      "outputs": [],
      "source": [
        "# Read and process TFrecords:\n",
        "record_folder = metabolic_record_folder\n",
        "records = gfile.ListDir(record_folder)\n",
        "number_records = len(records)\n",
        "\n",
        "Xd = []\n",
        "bmi_vec = []\n",
        "homa_ir_vec = []\n",
        "apri_vec = []\n",
        "hypertension_vec = []\n",
        "hyperlipidemia_vec = []\n",
        "cardiovascular_vec = []\n",
        "diabetes_vec = []\n",
        "anxiety_vec = []\n",
        "respiratory_vec = []\n",
        "kidney_disease_vec = []\n",
        "msss_binary_vec = []\n",
        "framingham_vec = []\n",
        "age_vec = []\n",
        "gender_vec = []\n",
        "regular_menstruation_vec = []\n",
        "smoker_vec = []\n",
        "diabetes_type_vec = []\n",
        "alcohol_vec = []\n",
        "medications_vec = []\n",
        "for r in records:\n",
        "  dataset = tf.data.TFRecordDataset(os.path.join(record_folder, r))\n",
        "  dataset = dataset.map(parse_metabolic_tfrecord)\n",
        "  for bmi, homa_ir, apri, msss, hypertension_binary, hyperlipidemia_binary, cardiovascular_binary, diabetes_binary, anxiety_binary, respiratory, kidney_disease, msss_binary, framingham, age, gender, regular_menstruation_str, smoker_str, diabetes_type_str, alcohol_str, medications_str,  array_feature in dataset:\n",
        "    arr = array_feature.numpy()\n",
        "    arr = np.nan_to_num(arr)\n",
        "    arr = np.pad(arr,((10080-arr.shape[0],0),(0,0)))\n",
        "    arr = np.mean(arr, axis=0) # Collapse to average.\n",
        "    arr = np.append(arr, np.std(arr, axis=0)) # Collapse to average.\n",
        "    Xd.append(np.expand_dims(arr.flatten(), axis=0))\n",
        "    bmi_vec.append(bmi.numpy().item())\n",
        "    homa_ir_vec.append(homa_ir.numpy().item())\n",
        "    apri_vec.append(apri.numpy().item())\n",
        "    hypertension_vec.append(hypertension_binary.numpy().item())\n",
        "    hyperlipidemia_vec.append(hyperlipidemia_binary.numpy().item())\n",
        "    cardiovascular_vec.append(cardiovascular_binary.numpy().item())\n",
        "    diabetes_vec.append(diabetes_binary.numpy().item())\n",
        "    anxiety_vec.append(anxiety_binary.numpy().item())\n",
        "    respiratory_vec.append(respiratory.numpy().item())\n",
        "    kidney_disease_vec.append(kidney_disease.numpy().item())\n",
        "    msss_binary_vec.append(msss_binary.numpy().item())\n",
        "    framingham_vec.append(framingham.numpy().item())\n",
        "    age_vec.append(age.numpy().item())\n",
        "    gender_vec.append(gender.numpy().item())\n",
        "\n",
        "    tmp = []\n",
        "    tmp = str(smoker_str.numpy().decode(encoding=\"utf-8\")).split(\"\\x02\")\n",
        "    if len(tmp)\u003c2:\n",
        "     tmp = str(smoker_str.numpy().decode(encoding=\"utf-8\")).split(\"\\x10\")\n",
        "    if len(tmp)\u003c2:\n",
        "     tmp = str(smoker_str.numpy().decode(encoding=\"utf-8\")).split(\"\\x03\")\n",
        "    if tmp[1] == \"Yes\":\n",
        "      smoker_vec.append(1)\n",
        "    elif tmp[1] == \"No\":\n",
        "      smoker_vec.append(0)\n",
        "    else:\n",
        "      smoker_vec.append(np.nan)\n",
        "\n",
        "    tmp = []\n",
        "    tmp = str(regular_menstruation_str.numpy().decode(encoding=\"utf-8\")).split(\"\\x02\")\n",
        "    if len(tmp)\u003c2:\n",
        "     tmp = str(regular_menstruation_str.numpy().decode(encoding=\"utf-8\")).split(\"\\x10\")\n",
        "    if len(tmp)\u003c2:\n",
        "     tmp = str(regular_menstruation_str.numpy().decode(encoding=\"utf-8\")).split(\"\\x03\")\n",
        "    if len(tmp)\u003c2:\n",
        "     tmp = str(regular_menstruation_str.numpy().decode(encoding=\"utf-8\")).split(\"\\x14\")\n",
        "    if tmp[1] == \"Yes\":\n",
        "      regular_menstruation_vec.append(1)\n",
        "    elif tmp[1] == \"No\":\n",
        "      regular_menstruation_vec.append(0)\n",
        "    else:\n",
        "      regular_menstruation_vec.append(np.nan)\n",
        "\n",
        "    tmp = []\n",
        "    try:\n",
        "      tmp = str(medications_str.numpy().decode(encoding=\"utf-8\")).split(\"\\x1d\")\n",
        "      if len(tmp)\u003c2:\n",
        "        tmp = str(medications_str.numpy().decode(encoding=\"utf-8\")).split(\"\\x10\")\n",
        "        medications_vec.append(1)\n",
        "      else:\n",
        "        medications_vec.append(0)\n",
        "    except:\n",
        "      medications_vec.append(np.nan)\n",
        "\n",
        "\n",
        "Xd = np.concatenate(Xd, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_91M4g7eD8tP"
      },
      "outputs": [],
      "source": [
        "# Plot\n",
        "def visualize_features_nan(array_feature):\n",
        "\n",
        "  fig = plt.figure(figsize=(20, 7))\n",
        "  ax1 = plt.subplot2grid((1, 12), (0, 0), colspan=12)\n",
        "  group = array_feature.numpy()\n",
        "  group[group==0] = np.nan\n",
        "\n",
        "  ax1 = sns.heatmap(group.T, cmap=\"Reds\", cbar=True, linewidths=0.0,\n",
        "                    linecolor='black', alpha=0.8, ax=ax1, yticklabels=True)\n",
        "\n",
        "  for tick in ax1.get_xticklabels():\n",
        "      tick.set_fontname('Ubuntu')\n",
        "      tick.set_style('italic')\n",
        "  ax1.tick_params(axis='x', labelsize=10.5)\n",
        "\n",
        "  ax1.set_yticklabels(FEATURES_TO_INCLUDE)\n",
        "  for tick in ax1.get_yticklabels():\n",
        "      tick.set_fontname('Ubuntu')\n",
        "  ax1.tick_params(axis='y', labelsize=10.5)\n",
        "\n",
        "  plt.xticks(rotation=45)  # Rotate labels for better readability\n",
        "  plt.tight_layout()\n",
        "\n",
        "  ax1.set_ylabel(\"Feature\", fontname='Ubuntu', fontsize=14)\n",
        "\n",
        "  ax1.axhline(y=0, color='k',linewidth=1, alpha=1)\n",
        "  ax1.axhline(y=group.shape[1], color='k', alpha=1,linewidth=1)\n",
        "  ax1.axvline(x=0, color='k',linewidth=1, alpha=1)\n",
        "  ax1.axvline(x=group.shape[0], color='k', alpha=1,linewidth=1);\n",
        "\n",
        "  for i in np.arange(0,group.shape[0],60):\n",
        "    ax1.axvline(x=i, color='k', alpha=0.4,linewidth=1);\n",
        "  for i in np.arange(0,group.shape[1],1):\n",
        "    ax1.axhline(y=i, color='k', alpha=0.4,linewidth=1);\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "# Plot\n",
        "def visualize_features(array_feature):\n",
        "\n",
        "  fig = plt.figure(figsize=(20, 7))\n",
        "  ax1 = plt.subplot2grid((1, 12), (0, 0), colspan=12)\n",
        "  group = array_feature.numpy()\n",
        "\n",
        "  ax1 = sns.heatmap(group.T, cmap=\"Reds\", cbar=True, linewidths=0.0,\n",
        "                    linecolor='black', alpha=0.8, ax=ax1, yticklabels=True)\n",
        "\n",
        "  for tick in ax1.get_xticklabels():\n",
        "      tick.set_fontname('Ubuntu')\n",
        "      tick.set_style('italic')\n",
        "  ax1.tick_params(axis='x', labelsize=10.5)\n",
        "\n",
        "  ax1.set_yticklabels(FEATURES_TO_INCLUDE)\n",
        "  for tick in ax1.get_yticklabels():\n",
        "      tick.set_fontname('Ubuntu')\n",
        "  ax1.tick_params(axis='y', labelsize=10.5)\n",
        "\n",
        "  plt.xticks(rotation=45)  # Rotate labels for better readability\n",
        "  plt.tight_layout()\n",
        "\n",
        "  ax1.set_ylabel(\"Feature\", fontname='Ubuntu', fontsize=14)\n",
        "\n",
        "  ax1.axhline(y=0, color='k',linewidth=1, alpha=1)\n",
        "  ax1.axhline(y=group.shape[1], color='k', alpha=1,linewidth=1)\n",
        "  ax1.axvline(x=0, color='k',linewidth=1, alpha=1)\n",
        "  ax1.axvline(x=group.shape[0], color='k', alpha=1,linewidth=1);\n",
        "\n",
        "  for i in np.arange(0,group.shape[0],60):\n",
        "    ax1.axvline(x=i, color='k', alpha=0.4,linewidth=1);\n",
        "  for i in np.arange(0,group.shape[1],1):\n",
        "    ax1.axhline(y=i, color='k', alpha=0.4,linewidth=1);\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "record_folder = \"/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-metabolichealth/deid/exp/aliheydari/metabolic_tfrecords_weekly_alllabels_v02\"\n",
        "records = gfile.ListDir(record_folder)\n",
        "for r in records:\n",
        "  dataset = tf.data.TFRecordDataset(os.path.join(record_folder, r))\n",
        "  dataset = dataset.map(parse_metabolic_tfrecord)\n",
        "  for bmi, homa_ir, apri, msss, hypertension_binary, hyperlipidemia_binary, cardiovascular_binary, diabetes_binary, anxiety_binary, respiratory, kidney_disease, msss_binary, regular_menstruation_str, smoker_str, diabetes_type_str, alcohol_str, medications_str,  array_feature in dataset:\n",
        "    visualize_features_nan(array_feature)\n",
        "    visualize_features(array_feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hULGdSvRHujy"
      },
      "source": [
        "## Compute PCA/LDA Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UikPSDN1l54X"
      },
      "outputs": [],
      "source": [
        "# Collapse Feature to Average\n",
        "Xd.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rHGtDzOJc3P"
      },
      "outputs": [],
      "source": [
        "# Fit PCA:\n",
        "pca = PCA()\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca)])\n",
        "plt.figure(figsize=(8,6))\n",
        "Xt = pipe.fit_transform(Xd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrDqduDiYclW"
      },
      "outputs": [],
      "source": [
        "Xd.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJushpQKI2H2"
      },
      "outputs": [],
      "source": [
        "labels = ['bmi','hypertension','homa_ir']\n",
        "colors = ['green', 'blue', 'orange', 'red']\n",
        "\n",
        "plt.rcParams['axes.facecolor'] = 'white'\n",
        "plt.rcParams['axes.edgecolor'] = 'black'\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
        "for idx, label in enumerate(labels):\n",
        "\n",
        "  # Categorize continuous labels:\n",
        "  if label == 'bmi':\n",
        "      yd = np.array(bmi_vec)\n",
        "      yd[(yd\u003e0) \u0026 (yd\u003c20)] = 0\n",
        "      yd[(yd\u003e20) \u0026 (yd\u003c=25)] = 1\n",
        "      yd[(yd\u003e20) \u0026 (yd\u003c=30)] = 2\n",
        "      yd[(yd\u003e30) \u0026 (yd\u003c=50)] = 3\n",
        "\n",
        "      for i in range(4):\n",
        "        axes[idx].scatter(Xt[yd == i, 0], Xt[yd == i, 1], c=colors[i], label=str(i), alpha=0.1)\n",
        "\n",
        "      axes[idx].legend(['Under Weight','Normal Weight','Over Weight','Obese'])\n",
        "\n",
        "  if label == 'hypertension':\n",
        "      yd = np.array(hypertension_vec)\n",
        "\n",
        "      for i in range(2):\n",
        "        axes[idx].scatter(Xt[yd == i, 0], Xt[yd == i, 1], c=colors[i], label=str(i), alpha=0.1)\n",
        "\n",
        "      axes[idx].legend(['No Hypertension','Hypertension'])\n",
        "\n",
        "  if label == 'homa_ir':\n",
        "      yd = np.array(homa_ir_vec)\n",
        "      yd[yd\u003c0] = -1\n",
        "      yd[(yd\u003e=0) \u0026 (yd\u003c=2.5)] = 0\n",
        "      yd[(yd\u003e2.5)] = 1\n",
        "\n",
        "      for i in range(2):\n",
        "        axes[idx].scatter(Xt[yd == i, 0], Xt[yd == i, 1], c=colors[i], label=str(i), alpha=0.1)\n",
        "\n",
        "      axes[idx].legend(['Low HOMA-IR','High HOMA-IR'])\n",
        "\n",
        "  axes[idx].set_xlabel('PCA Dim 1')\n",
        "  axes[idx].set_ylabel('PCA Dim 2')\n",
        "  axes[idx].axis('square')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbTkV9k6-3Nx"
      },
      "source": [
        "## Baseline Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU1a6rCBAg9C"
      },
      "outputs": [],
      "source": [
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import matplotlib.gridspec as gridspec\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "age_vec_norm = stats.zscore(age_vec)\n",
        "bmi_vec_norm = stats.zscore(bmi_vec)\n",
        "\n",
        "agg_features = []\n",
        "for sensor_features, age, gender, bmi in zip(Xd,age_vec_norm,gender_vec,bmi_vec_norm):\n",
        "  new_arr = np.append(sensor_features, np.array([age, gender, bmi]))\n",
        "  agg_features.append(new_arr)\n",
        "\n",
        "demo_features = []\n",
        "for age, gender, bmi in zip(age_vec,gender_vec,bmi_vec):\n",
        "  new_arr = np.array([age, gender, bmi])\n",
        "  demo_features.append(new_arr)\n",
        "\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "label_names = ['Smoking', 'Menstration', 'Medications', 'Homa-IR', 'Hypertension', 'Hyperlipidemia', 'Diabetes', 'Anxiety', 'Respiratory Disease'] ##'cardiovascular',\n",
        "labels = [smoker_vec, regular_menstruation_vec, medications_vec, homa_ir_vec, hypertension_vec, hyperlipidemia_vec, diabetes_vec, anxiety_vec, respiratory_vec] #cardiovascular_vec,\n",
        "          #bmi_vec,\n",
        "          #homa_ir_vec,\n",
        "          #apri_vec,\n",
        "          #hypertension_vec,\n",
        "          #hyperlipidemia_vec,\n",
        "          #cardiovascular_vec,\n",
        "          #diabetes_vec,\n",
        "          #anxiety_vec,\n",
        "          #respiratory_vec,\n",
        "          #kidney_disease_vec,\n",
        "          #msss_binary_vec,\n",
        "          #age_vec,\n",
        "          #gender_vec,\n",
        "          #regular_menstruation_vec,\n",
        "          #smoker_vec,\n",
        "          #diabetes_type_vec,\n",
        "          #alcohol_vec,\n",
        "          #medications_vec]\n",
        "\n",
        "number_samples = [10,20,50,100,200,500,1000,3000]\n",
        "\n",
        "methods = ['LR', 'XGBOOST','MLP']\n",
        "colors = ['red', 'blue', 'green', 'orange']\n",
        "\n",
        "plotOn = False\n",
        "\n",
        "for label, label_name in zip(labels, label_names):\n",
        "\n",
        "  scores = []\n",
        "  feature_types = ['Demo','Sensor+Demo']\n",
        "  for feature_type in feature_types:\n",
        "    print(feature_type)\n",
        "\n",
        "    label_mask = np.array(label)\n",
        "    if feature_type == 'Demo':\n",
        "      agg_features_mask = np.array(demo_features)\n",
        "    else:\n",
        "      agg_features_mask = np.array(agg_features)\n",
        "\n",
        "    mask = label_mask!=-999\n",
        "    label_mask = label_mask[mask]\n",
        "    agg_features_mask = agg_features_mask[mask,:]\n",
        "\n",
        "    if label_name == 'Homa-IR':\n",
        "      label_mask = label_mask\u003e2.8\n",
        "      label_mask = label_mask.astype(int)\n",
        "    if label_name == 'APRI':\n",
        "      label_mask = label_mask\u003e0.5\n",
        "      label_mask = label_mask.astype(int)\n",
        "    if label_name == 'Smoking' or 'Menstration' or 'Medications':\n",
        "      agg_features_mask = agg_features_mask[(label_mask == 0) | (label_mask == 1),:]\n",
        "      label_mask = label_mask[(label_mask == 0) | (label_mask == 1)]\n",
        "\n",
        "    label_mask = label_mask.tolist()\n",
        "    agg_features_mask = agg_features_mask.tolist()\n",
        "\n",
        "\n",
        "    for method in methods:\n",
        "      for fwsht in number_samples:\n",
        "        reps=5\n",
        "        for rep in range(reps):\n",
        "\n",
        "\n",
        "          train_agg_features_mask = agg_features_mask[:int(len(agg_features_mask)*0.8)]\n",
        "          train_label_mask = label_mask[:int(len(label_mask)*0.8)]\n",
        "          test_agg_features_mask = agg_features_mask[int(len(agg_features_mask)*0.8) :]\n",
        "          test_label_mask = label_mask[int(len(label_mask)*0.8) :]\n",
        "\n",
        "          X_test = test_agg_features_mask\n",
        "          y_test = test_label_mask\n",
        "          #_, X_test, _, y_test = train_test_split(test_agg_features_mask, test_label_mask, test_size=200, random_state=rep, stratify=test_label_mask)\n",
        "          _, X_train_fewshot, _, y_train_fewshot = train_test_split(train_agg_features_mask, train_label_mask, test_size=fwsht, random_state=rep, stratify=train_label_mask)\n",
        "\n",
        "          if method == 'LR':\n",
        "            # Create a logistic regression model\n",
        "            model = LogisticRegression(solver='liblinear')\n",
        "          if method =='XGBOOST':\n",
        "            model = XGBClassifier(objective='binary:logistic',\n",
        "                                  n_estimators=50,\n",
        "                                  learning_rate=0.1,\n",
        "                                  max_depth=5,\n",
        "                                  random_state=42,\n",
        "                                  eval_metric='error', use_label_encoder=False)\n",
        "          if method =='MLP':\n",
        "            model = MLPClassifier(hidden_layer_sizes=(50, 25),  # Two hidden layers with 50 and 25 neurons respectively\n",
        "                                  activation='relu',            # ReLU activation function\n",
        "                                  solver='adam',                # Adam optimizer\n",
        "                                  alpha=0.001,                  # L2 regularization term\n",
        "                                  batch_size=32,                # Batch size for mini-batch learning\n",
        "                                  learning_rate='adaptive',     # Adaptive learning rate\n",
        "                                  max_iter=300,                 # Maximum number of iterations\n",
        "                                  random_state=42)\n",
        "\n",
        "          # Train the model\n",
        "          model.fit(X_train_fewshot, y_train_fewshot)\n",
        "\n",
        "          # Make predictions on the test set\n",
        "          y_pred = model.predict(X_test)\n",
        "\n",
        "          # Evaluate the model\n",
        "          accuracy = accuracy_score(y_test, y_pred)\n",
        "          #print('TASK: ', label_name)\n",
        "          #print(f\".     Accuracy: {accuracy}\")\n",
        "\n",
        "          if plotOn==True:\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.hist(y_train_fewshot)\n",
        "            plt.title('Train Label Dist.')\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.hist(y_test)\n",
        "            plt.title('Test Label Dist.')\n",
        "\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "            disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "            disp.plot()\n",
        "            plt.show()\n",
        "\n",
        "          row = pd.DataFrame({'method': [method], 'feature_type': feature_type, 'fwsht': [fwsht], 'rep': [rep], 'score': [accuracy], 'f1': [f1_score(y_test, y_pred, average='macro')]})\n",
        "          scores.append(row)\n",
        "\n",
        "  scores = pd.concat(scores)\n",
        "  print('=======================')\n",
        "  fig = plt.figure(figsize=(22, 4))\n",
        "  gs = gridspec.GridSpec(\n",
        "      1,\n",
        "      4,\n",
        "      hspace=0.25,\n",
        "      wspace=0.25,\n",
        "      figure=fig,\n",
        "  )\n",
        "  for method, col in zip(methods,colors):\n",
        "    for feature_type in feature_types:\n",
        "      tmp = scores[(scores['method']==method) \u0026 (scores['feature_type']==feature_type)].groupby(['fwsht']).mean(numeric_only=True)\n",
        "      ax0 = plt.subplot(gs[0, 1])\n",
        "      if feature_type=='Demo':\n",
        "        ax0.plot(tmp.index,tmp.f1, '--', label=method, color=col)\n",
        "      else:\n",
        "        ax0.plot(tmp.index,tmp.f1,label=method, color=col)\n",
        "\n",
        "  ax0.xaxis.set_tick_params(which='major', size=10, width=2, direction='in')\n",
        "  ax0.xaxis.set_tick_params(which='minor', size=7, width=2, direction='in')\n",
        "  ax0.yaxis.set_tick_params(which='major', size=10, width=2, direction='in')\n",
        "  ax0.yaxis.set_tick_params(which='minor', size=7, width=2, direction='in')\n",
        "  ax0.spines['left'].set_linewidth(3)\n",
        "  ax0.spines['bottom'].set_linewidth(3)\n",
        "  ax0.spines['left'].set_edgecolor(\"#000000\")\n",
        "  ax0.spines['bottom'].set_edgecolor(\"#000000\")\n",
        "  ax0.tick_params(bottom=True, left=True, width=2, direction='inout')\n",
        "  ax0.spines['right'].set_visible(False)\n",
        "  ax0.spines['top'].set_visible(False)\n",
        "  ax0.grid(False)\n",
        "  ax0.set_facecolor('xkcd:white')\n",
        "  ax0.set_ylabel('Accuracy (%)')\n",
        "  ax0.set_xlabel('Number of Training Samples')\n",
        "  ax0.set_ylim([0, 1])\n",
        "  ax0.set_title(label_name)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  print('=======================')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WydQl1Eu4Fa8"
      },
      "source": [
        "# Activity Data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7ToUN2S4Ifw"
      },
      "outputs": [],
      "source": [
        "plotOn = True\n",
        "Xd = []\n",
        "labels = []\n",
        "ids = []\n",
        "if plotOn:\n",
        "  for r in records:\n",
        "    dataset = tf.data.TFRecordDataset(os.path.join(activity_record_folder, r))\n",
        "    dataset = dataset.map(parse_activity_tfrecord)\n",
        "    for array, mask, label in dataset:\n",
        "      #print(r, label.numpy())\n",
        "      arr = array.numpy()\n",
        "      arr[mask] = np.nan\n",
        "      Xd.append(np.nan_to_num(np.expand_dims(np.nanmean(arr, axis=0),1),0))\n",
        "      labels.append(label.numpy().item())\n",
        "      ids.append(r[9:-10])\n",
        "Xd = np.concatenate(Xd, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAI_1EqQ4R46"
      },
      "outputs": [],
      "source": [
        "plt.hist(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6J9Y7hkz5CRN"
      },
      "outputs": [],
      "source": [
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import matplotlib.gridspec as gridspec\n",
        "from sklearn.metrics import f1_score, balanced_accuracy_score, average_precision_score\n",
        "\n",
        "label_names = ['homa_ir_binary', 'hypertension', 'hyperlipidemia', 'anxiety', 'diabetes', 'respiratory']\n",
        "feature_types = ['Sensor','Demo','Sensor+Demo']\n",
        "methods = ['LR', 'XGBOOST']#,'MLP']\n",
        "colors = ['red', 'blue', 'green', 'orange']\n",
        "\n",
        "scores = []\n",
        "\n",
        "for label_name in label_names:\n",
        "  for feature_type in feature_types:\n",
        "    print(feature_type)\n",
        "\n",
        "    data_complete = data[data[label_name]\u003e=0]\n",
        "    train = data_complete[data_complete['id']\u003c70467]\n",
        "    test = data_complete[data_complete['id']\u003e=70467]\n",
        "\n",
        "    if feature_type == 'Sensor':\n",
        "      train_features = np.concatenate( train['features'].to_list(), axis=0)\n",
        "      test_features = np.concatenate( test['features'].to_list(), axis=0)\n",
        "    if feature_type == 'Demo':\n",
        "      train_features = train[['age','gender','bmi']].to_numpy()\n",
        "      test_features = test[['age','gender','bmi']].to_numpy()\n",
        "    if feature_type == 'Sensor+Demo':\n",
        "      train_features_lsm = np.concatenate( train['features'].to_list(), axis=0)\n",
        "      train_features_demo = train[['age','gender','bmi']].to_numpy()\n",
        "      train_features = np.concatenate((train_features_lsm, train_features_demo), axis=1)\n",
        "      test_features_lsm = np.concatenate( test['features'].to_list(), axis=0)\n",
        "      test_features_demo = test[['age','gender','bmi']].to_numpy()\n",
        "      test_features = np.concatenate((test_features_lsm, test_features_demo), axis=1)\n",
        "\n",
        "    train_labels = train[label_name].to_numpy()\n",
        "    test_labels = test[label_name].to_numpy()\n",
        "    test_ids = test['id'].to_numpy()\n",
        "\n",
        "    for method in methods:\n",
        "      if method == 'LR':\n",
        "        model = LogisticRegression(solver='liblinear')\n",
        "      if method =='XGBOOST':\n",
        "        model = XGBClassifier(objective='binary:logistic',\n",
        "                              n_estimators=50,\n",
        "                              learning_rate=0.1,\n",
        "                              max_depth=5,\n",
        "                              random_state=42,\n",
        "                              eval_metric='error', use_label_encoder=False)\n",
        "      if method =='MLP':\n",
        "        model = MLPClassifier(hidden_layer_sizes=(50, 25),  # Two hidden layers with 50 and 25 neurons respectively\n",
        "                              activation='relu',            # ReLU activation function\n",
        "                              solver='adam',                # Adam optimizer\n",
        "                              alpha=0.001,                  # L2 regularization term\n",
        "                              batch_size=32,                # Batch size for mini-batch learning\n",
        "                              learning_rate='adaptive',     # Adaptive learning rate\n",
        "                              max_iter=300,                 # Maximum number of iterations\n",
        "                              random_state=42)\n",
        "\n",
        "      # Train the model\n",
        "      model.fit(train_features, train_labels)\n",
        "\n",
        "      # Make predictions on the test set\n",
        "      y_pred = model.predict(test_features)\n",
        "\n",
        "      p = pd.DataFrame({'id': test_ids, 'label': test_labels, 'prediction': y_pred})\n",
        "      p = p.groupby('id').mean().round()\n",
        "      test_labels_p = p['label']\n",
        "      y_pred_p = p['prediction']\n",
        "\n",
        "      # Evaluate the model\n",
        "      plt.subplot(1, 2, 1)\n",
        "      plt.hist(train_labels)\n",
        "      plt.title('Train Label Dist.')\n",
        "      plt.subplot(1, 2, 2)\n",
        "      plt.hist(test_labels)\n",
        "      plt.title('Test Label Dist.')\n",
        "      cm = confusion_matrix(test_labels, y_pred)\n",
        "      disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "      disp.plot()\n",
        "      plt.show()\n",
        "\n",
        "      row = pd.DataFrame({'label': [label_name],\n",
        "                          'feature_type': [feature_type],\n",
        "                          'method': [method],\n",
        "                          'accuracy': [accuracy_score(test_labels, y_pred)],\n",
        "                          'f1': [f1_score(test_labels, y_pred, average='macro')],\n",
        "                          'balanced_accuracy': [balanced_accuracy_score(test_labels, y_pred)],\n",
        "                          'mAP': [average_precision_score(test_labels, y_pred, average='macro')],\n",
        "                          'accuracy_p': [accuracy_score(test_labels_p, y_pred_p)],\n",
        "                          'f1_p': [f1_score(test_labels_p, y_pred_p, average='macro')],\n",
        "                          'balanced_accuracy_p': [balanced_accuracy_score(test_labels_p, y_pred_p)],\n",
        "                          'mAP_p': [average_precision_score(test_labels_p, y_pred_p, average='macro')]})\n",
        "      scores.append(row)\n",
        "    print('=======================')\n",
        "    fig = plt.figure(figsize=(22, 4))\n",
        "    gs = gridspec.GridSpec(\n",
        "        1,\n",
        "        4,\n",
        "        hspace=0.25,\n",
        "        wspace=0.25,\n",
        "        figure=fig,\n",
        "    )\n",
        "scores = pd.concat(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37N-xznMH8gj"
      },
      "source": [
        "# Inference from LSM - Get embeddings from LSM-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLMTjc8eH93R"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Dump embedding data dir\n",
        "train_data_dir = '/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-metabolichealth/deid/exp/aliheydari/metabolic_embedding_dump/metabolic_tfrecords_daily_alllabels_v04_embedding_dump_xid149985992_wid1_20250218153123'\n",
        "valid_data_dir = '/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-metabolichealth/deid/exp/aliheydari/metabolic_embedding_dump/metabolic_tfrecords_daily_alllabels_v04_embedding_dump_xid149985992_wid1_20250218161703'\n",
        "\n",
        "# Glob files\n",
        "ext_pattern = '*.pickle'\n",
        "train_fpattern = os.path.join(train_data_dir, ext_pattern)\n",
        "valid_fpattern = os.path.join(valid_data_dir, ext_pattern)\n",
        "train_flist = gfile.Glob(train_fpattern)\n",
        "valid_flist = gfile.Glob(valid_fpattern)\n",
        "\n",
        "# Example file read\n",
        "path = train_flist[0]\n",
        "with gfile.GFile(path, mode='r') as f:\n",
        "  data = pickle.load(f)\n",
        "\n",
        "print(f'Train num dumped files: {len(train_flist)}')\n",
        "print(f'Valid num dumped files: {len(valid_flist)}\\n')\n",
        "print(f'Data Keys\\n{data.keys()}\\n')\n",
        "print('Embedding Shape:', data['embedding_pre_logits'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DM_OakDNoXOH"
      },
      "outputs": [],
      "source": [
        "embeddings = []\n",
        "bmi_vec = []\n",
        "hypertension_vec = []\n",
        "homa_ir_vec = []\n",
        "batch_mask_vec = []\n",
        "for path in valid_flist:\n",
        "  with gfile.GFile(path, mode='r') as f:\n",
        "    try:\n",
        "      data = pickle.load(f)\n",
        "      flattened_arr = data['embedding_pre_logits'].reshape(data['embedding_pre_logits'].shape[0] * data['embedding_pre_logits'].shape[1], *data['embedding_pre_logits'].shape[2:])\n",
        "      flattened_arr = flattened_arr.reshape(flattened_arr.shape[0],flattened_arr.shape[1] * flattened_arr.shape[2])\n",
        "      embeddings.append(flattened_arr)\n",
        "      bmi_vec.append(data['bmi'].flatten())\n",
        "      hypertension_vec.append(data['hypertension_binary'].flatten())\n",
        "      homa_ir_vec.append(data['homa_ir'].flatten())\n",
        "      batch_mask_vec.append(data['batch_mask'].flatten())\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "embeddings = np.concatenate( embeddings, axis=0 )\n",
        "bmi_vec = np.concatenate( bmi_vec, axis=0 )\n",
        "hypertension_vec = np.concatenate( hypertension_vec, axis=0 )\n",
        "homa_ir_vec = np.concatenate( homa_ir_vec, axis=0 )\n",
        "batch_mask_vec = np.concatenate( batch_mask_vec, axis=0 )\n",
        "print(embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7ENcsb31vYF"
      },
      "outputs": [],
      "source": [
        "n = 5824\n",
        "pca = PCA()\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca)])\n",
        "plt.figure(figsize=(8,6))\n",
        "Xt = pipe.fit_transform(embeddings[:n])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmmPRLSBymB5"
      },
      "outputs": [],
      "source": [
        "labels = ['bmi','hypertension','homa_ir']\n",
        "colors = ['green', 'blue', 'orange', 'red']\n",
        "\n",
        "plt.rcParams['axes.facecolor'] = 'white'\n",
        "plt.rcParams['axes.edgecolor'] = 'black'\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
        "for idx, label in enumerate(labels):\n",
        "\n",
        "  # Categorize continuous labels:\n",
        "  if label == 'bmi':\n",
        "      yd = bmi_vec[:n]\n",
        "      yd[(yd\u003e0) \u0026 (yd\u003c20)] = 0\n",
        "      yd[(yd\u003e20) \u0026 (yd\u003c=25)] = 1\n",
        "      yd[(yd\u003e20) \u0026 (yd\u003c=30)] = 2\n",
        "      yd[(yd\u003e30) \u0026 (yd\u003c=50)] = 3\n",
        "\n",
        "      for i in range(4):\n",
        "        axes[idx].scatter(Xt[yd == i, 0], Xt[yd == i, 1], c=colors[i], label=str(i), alpha=0.1)\n",
        "\n",
        "      axes[idx].legend(['Under Weight','Normal Weight','Over Weight','Obese'])\n",
        "\n",
        "  if label == 'hypertension':\n",
        "      yd = hypertension_vec[:n]\n",
        "      #yd = labels#np.array(hypertension_vec)\n",
        "\n",
        "      for i in range(2):\n",
        "        axes[idx].scatter(Xt[yd == i, 0], Xt[yd == i, 1], c=colors[i], label=str(i), alpha=0.1)\n",
        "\n",
        "      axes[idx].legend(['No Hypertension','Hypertension'])\n",
        "\n",
        "  if label == 'homa_ir':\n",
        "      yd = homa_ir_vec[:n]\n",
        "      yd[yd\u003c0] = -1\n",
        "      yd[(yd\u003e=0) \u0026 (yd\u003c=2.5)] = 0\n",
        "      yd[(yd\u003e2.5)] = 1\n",
        "\n",
        "      for i in range(2):\n",
        "        axes[idx].scatter(Xt[yd == i, 0], Xt[yd == i, 1], c=colors[i], label=str(i), alpha=0.1)\n",
        "\n",
        "      axes[idx].legend(['Low HOMA-IR','High HOMA-IR'])\n",
        "\n",
        "  axes[idx].set_xlabel('PCA Dim 1')\n",
        "  axes[idx].set_ylabel('PCA Dim 2')\n",
        "  axes[idx].axis('square')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d79seMke3-l_"
      },
      "outputs": [],
      "source": [
        "yd"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "m3MuXBCnCFKm",
        "-r7oRyHPFLmm",
        "GMjGCn7_D1UQ",
        "HDhtxsn8D6iw"
      ],
      "last_runtime": {
        "build_target": "//fitbit/research/sensing/fitbit_prod_research/colab_algo:rl_colab",
        "kind": "shared"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1EKl3X1i051DR-99c0LBHfIjjONWGC1v2",
          "timestamp": 1744404269601
        },
        {
          "file_id": "1FlWXm9vMR5MaUHL8TsxWKR7c9EcLqfUW",
          "timestamp": 1733692740543
        },
        {
          "file_id": "11UdNvUeAS6o3tfrz49CE4UNNpPX3OygR",
          "timestamp": 1729539561855
        },
        {
          "file_id": "1yZ8pQR8l2O65aCH6xYYYyQVSDnxtp44F",
          "timestamp": 1710526257742
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
