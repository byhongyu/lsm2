{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mswgwo-WiOod"
      },
      "source": [
        "### CIFAR 10 Masked AutoEncoder Training with Scenic Framework\n",
        "#### Colab Kernel (Brainframe GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3H9M0KUr875D"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "import functools\n",
        "from typing import Any, Callable, Dict, Iterator, Tuple, Optional, Type, Union\n",
        "\n",
        "from absl import logging\n",
        "from clu import metric_writers\n",
        "from clu import periodic_actions\n",
        "from clu import platform\n",
        "import flax\n",
        "from flax import jax_utils\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.profiler\n",
        "import ml_collections\n",
        "import numpy as np\n",
        "import optax\n",
        "from colabtools import adhoc_import\n",
        "import seaborn as sns\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "with adhoc_import.Google3():\n",
        "  from scenic.dataset_lib import dataset_utils\n",
        "  from scenic.google.xm import xm_utils\n",
        "  from scenic.model_lib.base_models import base_model\n",
        "  from scenic.model_lib.layers import nn_ops\n",
        "  # To register the preprocessing ops\n",
        "  from scenic.projects.multimask import data_utils  # pylint: disable=unused-import\n",
        "  from scenic.train_lib import optax as scenic_optax\n",
        "  from scenic.train_lib import pretrain_utils\n",
        "  from scenic.train_lib import train_utils\n",
        "  from scenic.model_lib.base_models import base_model\n",
        "  from scenic.model_lib.base_models import model_utils\n",
        "  from scenic.model_lib.layers import nn_layers\n",
        "  from scenic.projects.baselines import vit\n",
        "  from scenic.train_lib.transfer import fewshot_utils\n",
        "  from scenic.projects.multimask import trainer\n",
        "  from scenic.projects.multimask.models import model_utils as mm_model_utils\n",
        "  from scenic.projects.multimask.models import transformer_encoder\n",
        "  from scenic.projects.multimask.models import vit_encoder\n",
        "  from scenic.projects.multimask.models import vit_mae\n",
        "  from scenic.projects.baselines.configs.google.common import common_fewshot\n",
        "  from scenic.dataset_lib.big_transfer import bit\n",
        "\n",
        "\n",
        "Batch = Dict[str, jnp.ndarray]\n",
        "MetricFn = Callable[\n",
        "    [jnp.ndarray, jnp.ndarray, Dict[str, jnp.ndarray]],\n",
        "    Dict[str, Tuple[float, int]],\n",
        "]\n",
        "LossFn = Callable[\n",
        "    [jnp.ndarray, Batch, Optional[jnp.ndarray], jnp.ndarray], float\n",
        "]\n",
        "LrFns = Dict[str, Callable[[jnp.ndarray], jnp.ndarray]]\n",
        "Patch = Union[Tuple[int, int], Tuple[int, int, int]]\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0lkQK97w29j"
      },
      "source": [
        "## Helpers (Functions and Classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Vuethraui8m_"
      },
      "outputs": [],
      "source": [
        "# @title ViT MAE (adapted from vit_mae.py)\n",
        "\n",
        "\"\"\"ViT encoder-decoder models for mmultimask.\n",
        "\n",
        "Adapted from google3/third_party/py/scenic/projects/multimask/models/vit_mae.py.\n",
        "The below functions/classes are adapted to allow for a dense patch encoder,\n",
        "and to fix a bug in the decoder projection layer.\n",
        "\"\"\"\n",
        "\n",
        "# Mostly copied from ViTMaskedAutoencoder in projects/mfp/vit.py\n",
        "class ViTMAE(nn.Module):\n",
        "  \"\"\"Encoder-decoder Vision Transformer model for masked feature regression.\n",
        "\n",
        "  Copied from google3/third_party/py/scenic/projects/multimask/models/vit_mae.py.\n",
        "\n",
        "  The differences to `ViTMaskedModel` from vit_encoder.py are that:\n",
        "  -- Only non-masked tokens are processed by the encoder\n",
        "  -- The parallel decoder then processes all tokens\n",
        "\n",
        "  Attributes:\n",
        "    num_classes: Number of output classes.\n",
        "    mlp_dim: Dimension of the mlp on top of attention block.\n",
        "    num_layers: Number of layers.\n",
        "    num_heads: Number of self-attention heads.\n",
        "    patches: Configuration of the patches extracted in the stem of the model.\n",
        "    hidden_size: Size of the hidden state of the output of model's stem.\n",
        "    token_mask_probability: Probability of masking out the input tokens (with a\n",
        "      learned mask token) during training.\n",
        "    representation_size: Size of the representation layer in the model's head.\n",
        "      if None, we skip the extra projection + tanh activation at the end.\n",
        "    dropout_rate: Dropout rate.\n",
        "    attention_dropout_rate: Dropout for attention heads.\n",
        "    stochastic_depth: Probability of dropping out a layer during training.\n",
        "    classifier: type of the classifier layer. Options are 'gap', 'gmp', 'gsp',\n",
        "      'token'.\n",
        "    dtype: JAX data type for activations.\n",
        "  \"\"\"\n",
        "\n",
        "  num_classes: int\n",
        "  mlp_dim: int\n",
        "  num_layers: int\n",
        "  num_heads: int\n",
        "  patches: ml_collections.ConfigDict\n",
        "  hidden_size: int\n",
        "  token_mask_probability: str\n",
        "  decoder_config: ml_collections.ConfigDict\n",
        "  representation_size: Optional[int] = None\n",
        "  conv_patch_encoder: bool = True\n",
        "  positional_embedding: str = 'sinusoidal_2d'\n",
        "  positional_embedding_decoder: str = 'sinusoidal_2d'\n",
        "  dropout_rate: float = 0.1\n",
        "  attention_dropout_rate: float = 0.1\n",
        "  stochastic_depth: float = 0.0\n",
        "  classifier: str = 'none'\n",
        "  dtype: jnp.dtype = jnp.float32\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x: jnp.ndarray, *, train: bool, debug: bool = False):\n",
        "    \"\"\"Forward pass of Vision Transformer.\"\"\"\n",
        "\n",
        "    fh, fw = self.patches.size\n",
        "    assert x.shape[1] % fh == 0 and x.shape[2] % fw == 0, (\n",
        "        'Height and width should be divisible by the respective patch sizes,'\n",
        "        f' instead got {x.shape[1:3]} and {(fh, fw)}'\n",
        "    )\n",
        "    # Patch encoder\n",
        "    if self.conv_patch_encoder:\n",
        "      # Extracting patches and then embedding is in fact a single convolution.\n",
        "      x = nn.Conv(\n",
        "          self.hidden_size,\n",
        "          (fh, fw),\n",
        "          strides=(fh, fw),\n",
        "          padding='VALID',\n",
        "          name='embedding',\n",
        "      )(x)\n",
        "      batch, height, width, channels = x.shape\n",
        "      x = jnp.reshape(x, shape=[batch, height * width, channels])\n",
        "    else:\n",
        "    # Traditional dense patch encoder\n",
        "      batch, img_h, img_w, img_c = x.shape\n",
        "      nh, nw = (img_h // fh, img_w // fw)\n",
        "      # Patch image\n",
        "      x = jnp.reshape(x, shape=(batch, nh, fh, nw, fw, img_c))\n",
        "      x = jnp.transpose(x, (0, 1, 3, 2, 4, 5))\n",
        "      x = jnp.reshape(x, shape=(batch, nh*nw, fh*fw*img_c))\n",
        "      # Encode patches\n",
        "      x = nn.Dense(self.hidden_size)(x)\n",
        "      height, width = (nh, nw)\n",
        "      channels = self.hidden_size\n",
        "      x = jnp.reshape(x, shape=[batch, height * width, channels])\n",
        "\n",
        "    # Add positional encodings before removing the masked tokens\n",
        "    x = mm_model_utils.add_positional_embeddings(\n",
        "        x, self.positional_embedding, [batch, height, width, channels]\n",
        "    )\n",
        "\n",
        "    # Remove masked tokens if needed\n",
        "    n_tokens = height * width\n",
        "    if train:\n",
        "      # Generate mask indices.\n",
        "      assert self.token_mask_probability.startswith('constant_'), (\n",
        "          'Only constant token_mask_probability supported in MAE, instad got'\n",
        "          f' {self.token_mask_probability}'\n",
        "      )\n",
        "      token_mask_probability = float(self.token_mask_probability.split('_')[1])\n",
        "      n_masked = int(token_mask_probability * n_tokens)\n",
        "      mask_rng = self.make_rng('dropout')\n",
        "      mask_indices, unmasked_indices, token_mask = (\n",
        "          mm_model_utils.get_mask_indices(\n",
        "              batch, n_tokens, n_masked, mask_rng\n",
        "          )\n",
        "      )\n",
        "      # Process only unmasked tokens with the encoder.\n",
        "      batch_indices = jnp.arange(batch).reshape(batch, 1)\n",
        "      x = x[batch_indices, unmasked_indices]\n",
        "\n",
        "    else:\n",
        "      token_mask = jnp.zeros((batch, n_tokens))\n",
        "      batch_indices = jnp.arange(batch).reshape(batch, 1)\n",
        "      mask_indices = jnp.zeros((batch, n_tokens))\n",
        "      unmasked_indices = jnp.tile(jnp.arange(n_tokens), [batch, 1])\n",
        "\n",
        "    aux = {'token_mask': token_mask}\n",
        "\n",
        "    # If we want to add a class token, add it here.\n",
        "    # Note that in MAE, positional encodings are not added to the CLS token.\n",
        "    if self.classifier == 'token':\n",
        "      cls = self.param('cls', nn.initializers.zeros,\n",
        "                       (1, 1, channels), x.dtype)\n",
        "      cls = jnp.tile(cls, [batch, 1, 1])\n",
        "      x = jnp.concatenate([cls, x], axis=1)\n",
        "\n",
        "    x = vit.Encoder(\n",
        "        mlp_dim=self.mlp_dim,\n",
        "        num_layers=self.num_layers,\n",
        "        num_heads=self.num_heads,\n",
        "        dropout_rate=self.dropout_rate,\n",
        "        attention_dropout_rate=self.attention_dropout_rate,\n",
        "        stochastic_depth=self.stochastic_depth,\n",
        "        dtype=self.dtype,\n",
        "        positional_embedding='none',  # Has already been added.\n",
        "        name='Transformer')(\n",
        "            x, train=train)\n",
        "    aux['pre_logits'] = x\n",
        "\n",
        "    # If not training, skip decoding\n",
        "    if not train:\n",
        "      return x, aux\n",
        "\n",
        "    # Process entire sequence with the decoder.\n",
        "    mask_token = self.param('mask_token',\n",
        "                            nn.initializers.zeros,\n",
        "                            (1, 1, self.decoder_config.hidden_size))\n",
        "\n",
        "    x = nn.Dense(\n",
        "        self.decoder_config.hidden_size,\n",
        "        kernel_init=nn.initializers.xavier_uniform(),\n",
        "        name='decoder_projection')(x)\n",
        "    if self.classifier == 'token':\n",
        "      x = x[:, 1:, :]\n",
        "\n",
        "    # This effectively \"unshuffles\" the tokens. This means that we can simply\n",
        "    # add positional encodings in the decoder without having to worry about\n",
        "    # their ordering.\n",
        "    x_all = jnp.zeros((batch, n_tokens, self.decoder_config.hidden_size))\n",
        "    x_all = x_all.at[batch_indices, unmasked_indices].set(x)\n",
        "    x_all = x_all.at[batch_indices, mask_indices].set(mask_token)\n",
        "    x = x_all\n",
        "    del x_all\n",
        "\n",
        "    # Add positional encodings to the decoder.\n",
        "    x = mm_model_utils.add_positional_embeddings(\n",
        "        x, self.positional_embedding_decoder,\n",
        "        [batch, height, width, self.decoder_config.hidden_size])\n",
        "\n",
        "    # The parallel decoder, which is actually technically an encoder\n",
        "    x = vit.Encoder(\n",
        "        mlp_dim=self.decoder_config.mlp_dim,\n",
        "        num_layers=self.decoder_config.num_layers,\n",
        "        num_heads=self.decoder_config.num_heads,\n",
        "        dropout_rate=self.decoder_config.dropout_rate,\n",
        "        attention_dropout_rate=self.decoder_config.attention_dropout_rate,\n",
        "        stochastic_depth=self.decoder_config.get('stochastic_depth', 0.0),\n",
        "        dtype=self.dtype,\n",
        "        positional_embedding='none',  # Has already been added.\n",
        "        name='Decoder')(x, train=train)\n",
        "\n",
        "    # Predict pixel reconstructions.\n",
        "    if self.representation_size is not None:\n",
        "      x = nn.Dense(self.representation_size, name='pre_logits')(\n",
        "          x)\n",
        "      x = nn.tanh(x)\n",
        "    else:\n",
        "      x = nn_layers.IdentityLayer(name='pre_logits')(x)\n",
        "    aux['pre_logits_decoder'] = x\n",
        "\n",
        "    # NOTE: This is the original (incorrect) implementation from\n",
        "    # google3/third_party/py/scenic/projects/multimask/models/vit_mae.py\n",
        "    # x = nn.Dense(\n",
        "    #     self.num_classes,\n",
        "    #     kernel_init=nn.initializers.zeros,\n",
        "    #     name='output_projection')(x)\n",
        "\n",
        "    # Reshapes to linear embeddeding.\n",
        "    batch, num_patches, embbed_size = x.shape\n",
        "    x = x.reshape(batch, num_patches*embbed_size)\n",
        "    # Apply dense projection layer to embeddeding.\n",
        "    x = nn.Dense(\n",
        "        num_patches * self.num_classes,\n",
        "        kernel_init=nn.initializers.zeros,\n",
        "        name='output_projection')(x)\n",
        "    # Reshape to [batch size, number of patches, patch size / num classes].\n",
        "    x = x.reshape(batch, num_patches, self.num_classes)\n",
        "\n",
        "    # Return output projection and auxilary information\n",
        "    return x, aux\n",
        "\n",
        "\n",
        "# (metric, normalizer, apply_prediction_weights)\n",
        "# Copied from google3/third_party/py/scenic/projects/multimask/models/vit_mae.py.\n",
        "_REGRESSION_METRICS = {\n",
        "    'mean_squared_error_all': (\n",
        "        functools.partial(mm_model_utils.weighted_error, loss_type='squared'),\n",
        "        model_utils.num_examples,\n",
        "        False,\n",
        "    ),\n",
        "    'mean_absolute_error_all': (\n",
        "        functools.partial(mm_model_utils.weighted_error, loss_type='absolute'),\n",
        "        model_utils.num_examples,\n",
        "        False,\n",
        "    ),\n",
        "    'mean_squared_error_masked': (\n",
        "        functools.partial(mm_model_utils.weighted_error, loss_type='squared'),\n",
        "        model_utils.num_examples,\n",
        "        True,\n",
        "    ),\n",
        "    'mean_absolute_error_masked': (\n",
        "        functools.partial(mm_model_utils.weighted_error, loss_type='absolute'),\n",
        "        model_utils.num_examples,\n",
        "        True,\n",
        "    ),\n",
        "}\n",
        "\n",
        "\n",
        "def regression_metrics_function(\n",
        "    predictions: jnp.ndarray,\n",
        "    prediction_masks: jnp.ndarray,\n",
        "    batch: base_model.Batch,\n",
        "    metrics: base_model.MetricNormalizerFnDict,\n",
        "    axis_name: Union[str, Tuple[str, ...]] = 'batch',\n",
        ") -\u003e Dict[str, Tuple[float, int]]:\n",
        "  \"\"\"Calculate metrics for the regression task.\n",
        "\n",
        "  Copied from google3/third_party/py/scenic/projects/multimask/models/vit_mae.py.\n",
        "\n",
        "  Currently we assume each metric_fn has the API:\n",
        "    ```metric_fn(predictions, targets, weights)```\n",
        "  and returns an array of shape [batch,]. We also assume that to compute\n",
        "  the aggregate metric, one should sum across all batches, then divide by the\n",
        "  total samples seen. In this way we currently only support metrics of the 1/N\n",
        "  sum f(inputs, targets). Note, the caller is responsible for dividing by\n",
        "  the normalizer when computing the mean of each metric.\n",
        "\n",
        "  Args:\n",
        "   predictions: Output of model in shape [batch, length, channels].\n",
        "   prediction_masks: Masks used for masked modeling, shape [batch, length]\n",
        "   batch: Batch (dict) with keys 'targets' and optionally 'batch_mask'.\n",
        "   metrics: The regression metrics to evaluate. The key is the name of the\n",
        "     metric, and the value is the metrics function, normalizer, and a bool\n",
        "     indicating whether to apply prediction_masks.\n",
        "   axis_name: List of axes on which we run the pmsum.\n",
        "\n",
        "  Returns:\n",
        "    A dict of metrics, in which keys are metrics name and values are tuples of\n",
        "    (metric, normalizer).\n",
        "  \"\"\"\n",
        "  targets = batch['targets']\n",
        "  batch_weights = batch.get('batch_mask')\n",
        "  weights = jnp.expand_dims(batch_weights, axis=-1) * prediction_masks\n",
        "  evaluated_metrics = {}\n",
        "  for key, val in metrics.items():\n",
        "    curr_weights = weights if val[2] else batch_weights\n",
        "    evaluated_metrics[key] = model_utils.psum_metric_normalizer(\n",
        "        (\n",
        "            val[0](\n",
        "                targets,\n",
        "                predictions,  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
        "                curr_weights,\n",
        "            ),\n",
        "            val[1](\n",
        "                targets,\n",
        "                predictions,  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
        "                batch_weights,\n",
        "            ),\n",
        "        ),\n",
        "        axis_name=axis_name,\n",
        "    )\n",
        "  return evaluated_metrics  # pytype: disable=bad-return-type  # jax-ndarray\n",
        "\n",
        "\n",
        "class ViTMAEModel(base_model.BaseModel):\n",
        "  \"\"\"ViT-based masked modeling.\n",
        "\n",
        "  Copied from google3/third_party/py/scenic/projects/multimask/models/vit_mae.py.\n",
        "  \"\"\"\n",
        "\n",
        "  def build_flax_model(self) -\u003e nn.Module:\n",
        "    model_dtype = getattr(jnp, self.config.get('model_dtype_str', 'float32'))\n",
        "    num_classes = np.prod(tuple(self.config.model.patches.size)) * 3\n",
        "\n",
        "    return ViTMAE(\n",
        "        num_classes=num_classes,\n",
        "        mlp_dim=self.config.model.mlp_dim,\n",
        "        num_layers=self.config.model.num_layers,\n",
        "        num_heads=self.config.model.num_heads,\n",
        "        representation_size=self.config.model.representation_size,\n",
        "        positional_embedding=self.config.model.positional_embedding,\n",
        "        positional_embedding_decoder=self.config.model.positional_embedding_decoder,\n",
        "        patches=self.config.model.patches,\n",
        "        hidden_size=self.config.model.hidden_size,\n",
        "        token_mask_probability=(\n",
        "            self.config.masked_feature_loss.token_mask_probability\n",
        "        ),\n",
        "        classifier='none',\n",
        "        dropout_rate=self.config.model.get('dropout_rate', 0.1),\n",
        "        attention_dropout_rate=self.config.model.get(\n",
        "            'attention_dropout_rate', 0.1\n",
        "        ),\n",
        "        stochastic_depth=self.config.model.get('stochastic_depth', 0.0),\n",
        "        decoder_config=self.config.model.decoder_config,\n",
        "        dtype=model_dtype,\n",
        "    )\n",
        "\n",
        "  def default_flax_model_config(self) -\u003e ml_collections.ConfigDict:\n",
        "    return ml_collections.ConfigDict()\n",
        "\n",
        "  def init_from_train_state(\n",
        "      self,\n",
        "      train_state: Any,\n",
        "      restored_train_state: Any,\n",
        "      restored_model_cfg: ml_collections.ConfigDict,\n",
        "  ) -\u003e Any:\n",
        "    \"\"\"Updates the train_state with data from restored_train_state.\n",
        "\n",
        "    This function is writen to be used for 'fine-tuning' experiments. Here, we\n",
        "    do some surgery to support larger resolutions (longer sequence length) in\n",
        "    the transformer block, with respect to the learned pos-embeddings.\n",
        "\n",
        "    Args:\n",
        "      train_state: A raw TrainState for the model.\n",
        "      restored_train_state: A TrainState that is loaded with parameters/state of\n",
        "        a  pretrained model.\n",
        "      restored_model_cfg: Configuration of the model from which the\n",
        "        restored_train_state come from. Usually used for some asserts.\n",
        "\n",
        "    Returns:\n",
        "      Updated train_state.\n",
        "    \"\"\"\n",
        "    return vit.init_vit_from_train_state(\n",
        "        train_state, restored_train_state, self.config, restored_model_cfg\n",
        "    )\n",
        "\n",
        "  # prediction_masks at the last position to fit the parent class func signature\n",
        "  def loss_function(\n",
        "      self,\n",
        "      predictions: jnp.ndarray,\n",
        "      batch: base_model.Batch,\n",
        "      model_params: Optional[jnp.ndarray] = None,\n",
        "      prediction_masks: Optional[jnp.ndarray] = None,\n",
        "  ) -\u003e float:\n",
        "    \"\"\"Returns the (weighted) mean squared error.\n",
        "\n",
        "    Args:\n",
        "      predictions: Output of model in shape [batch, length, channels].\n",
        "      batch: Batch (dict) with keys 'targets' and optionally 'batch_mask'.\n",
        "      model_params: Parameters of the model, for optionally applying\n",
        "        regularization.\n",
        "      prediction_masks: Masks used for masked modeling, shape [batch, length]\n",
        "\n",
        "    Returns:\n",
        "      The scalar loss, which is the (weighted) absolute error.\n",
        "    \"\"\"\n",
        "    # IIUC, this mask can be provided by the data loader to indicate invalid\n",
        "    # examples, e.g. for incomplete batches during eval\n",
        "    weights = batch['batch_mask']  # shape (batch_size,)\n",
        "\n",
        "    # If requested, compute the loss only on unmasked tokens\n",
        "    if self.config.masked_feature_loss.loss_only_masked_tokens:\n",
        "      weights = jnp.expand_dims(weights, axis=-1) * prediction_masks\n",
        "\n",
        "    targets = batch['targets']\n",
        "    total_loss = mm_model_utils.weighted_error(\n",
        "        predictions,\n",
        "        targets,\n",
        "        weights,\n",
        "        axis=tuple(range(targets.ndim)),  # aggregate over the batch axis too\n",
        "        loss_type=self.config.masked_feature_loss.loss_type,\n",
        "        mean=True,\n",
        "    )\n",
        "\n",
        "    return total_loss  # pytype: disable=bad-return-type  # jax-ndarray\n",
        "\n",
        "  def get_metrics_fn(self, split: Optional[str] = None) -\u003e base_model.MetricFn:\n",
        "    \"\"\"Returns a callable metric function for the model.\n",
        "\n",
        "    By default, we return the same metric for each split.\n",
        "\n",
        "    Args:\n",
        "      split: The split for which we calculate the metrics. It should be one of\n",
        "        the ['train',  'validation', 'test'].\n",
        "    Returns: A metric function with the following API:\n",
        "      ```metrics_fn(predictions, batch)```\n",
        "    \"\"\"\n",
        "\n",
        "    del split  # Same function for all splits.\n",
        "    return functools.partial(\n",
        "        regression_metrics_function, metrics=_REGRESSION_METRICS\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "q5q29AOc-28p"
      },
      "outputs": [],
      "source": [
        "# @title Trainer functions (adapted from trainer.py)\n",
        "\n",
        "\"\"\" Trainer functions.\n",
        "\n",
        "Copied / adapted from google3/third_party/py/scenic/projects/multimask/trainer.py.\n",
        "\n",
        "The below funcctions are adapted to TODO ensure determinitic random behavior\n",
        "in evaluation steps.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_targets(batch: Batch, config: ml_collections.ConfigDict) -\u003e jnp.ndarray:\n",
        "  \"\"\"Copied from google3/third_party/py/scenic/projects/multimask/trainer.py.\n",
        "  \"\"\"\n",
        "  targets_type = config.masked_feature_loss.targets_type\n",
        "  if targets_type == 'rgb':\n",
        "    return get_rgb_targets(batch['inputs'], tuple(config.model.patches.size))\n",
        "  elif targets_type == 'tokens':\n",
        "    return nn.one_hot(batch['inputs'], num_classes=config.model.vocab_size)\n",
        "  else:\n",
        "    raise ValueError(f'Unknown targets_type {targets_type}')\n",
        "\n",
        "\n",
        "def get_rgb_targets(\n",
        "    inputs: jnp.ndarray,\n",
        "    patch_size: Patch,\n",
        "    reconstruct_grayscale: Optional[bool] = False,\n",
        "    standardise_per_patch: Optional[bool] = False,\n",
        ") -\u003e jnp.ndarray:\n",
        "  \"\"\"Get RGB targets to use for feature regression.\n",
        "\n",
        "  Copied from google3/third_party/py/scenic/projects/multimask/trainer.py.\n",
        "\n",
        "  Here, the targets are the raw rgb patches of the image.\n",
        "\n",
        "  Args:\n",
        "    inputs: Tensor of shape [b, h, w, c] or [b, t, h, w, c]. The former are\n",
        "      images, and the later video.\n",
        "    patch_size: The shape of the patch, defined as [ph, pw] for images, and [ph,\n",
        "      pw, pt] for video.\n",
        "    reconstruct_grayscale: If True, the target patch is in grayscale rather than\n",
        "      rgb.\n",
        "    standardise_per_patch: If true, standardise each patch by subtracting the\n",
        "      mean and dividing by the standard deviation of that patch.\n",
        "\n",
        "  Returns:\n",
        "    Patched inputs. For images, shape is [b, gh * gw, ph * pw * c] where\n",
        "      gh = h // ph and gw = w // pw.\n",
        "      For video, shape is [b, gt * gh * gw, pt * ph * pw * c].\n",
        "  \"\"\"\n",
        "  if inputs.ndim != 4:\n",
        "    raise ValueError('Inputs should be 4D (images). Shape {inputs.shape}')\n",
        "\n",
        "  if reconstruct_grayscale:\n",
        "    # Reference for converting between RGB and grayscale.\n",
        "    # https://en.wikipedia.org/wiki/Luma_%28video%29\n",
        "    # Also used in tf.image.rgb_to_grayscale\n",
        "    rgb_weights = jnp.tile(jnp.array([[0.2989, 0.5870, 0.1140]]), (3, 1)).T\n",
        "    inputs = jnp.matmul(inputs, rgb_weights)\n",
        "\n",
        "  assert inputs.ndim == 4, 'the input should shape BxHxWxC'\n",
        "  batch = inputs.shape[0]\n",
        "  # Shape is [batch, ht, wt, hp, wp, c]\n",
        "  patched_image = nn_ops.patch_image(\n",
        "      inputs, inputs_shape=None, patch_size=patch_size\n",
        "  )\n",
        "  num_tokens = patched_image.shape[1] * patched_image.shape[2]\n",
        "  patched_input = jnp.reshape(patched_image, (batch, num_tokens, -1))\n",
        "\n",
        "  if standardise_per_patch:\n",
        "    patched_input = jax.nn.standardize(patched_input, axis=-1, epsilon=1e-6)\n",
        "\n",
        "  return patched_input\n",
        "\n",
        "\n",
        "def eval_step(\n",
        "    train_state: train_utils.TrainState,\n",
        "    batch: Batch,\n",
        "    *,\n",
        "    flax_model: nn.Module,\n",
        "    metrics_fn: MetricFn,\n",
        "    config: ml_collections.ConfigDict,\n",
        "    debug: Optional[bool] = False,\n",
        "    rng: Optional[jax.random.PRNGKey] = None,\n",
        ") -\u003e Tuple[Dict[str, Tuple[float, int]], jnp.ndarray]:\n",
        "  \"\"\"Runs a single step of training.\n",
        "\n",
        "  Adapted from google3/third_party/py/scenic/projects/multimask/trainer.py.\n",
        "\n",
        "  Note that in this code, the buffer of the second argument (batch) is donated\n",
        "  to the computation.\n",
        "\n",
        "  Assumed API of metrics_fn is:\n",
        "  ```metrics = metrics_fn(logits, batch)\n",
        "  where batch is yielded by the batch iterator, and metrics is a dictionary\n",
        "  mapping metric name to a vector of per example measurements. eval_step will\n",
        "  aggregate (by summing) all per example measurements and divide by the\n",
        "  aggregated normalizers. For each given metric we compute:\n",
        "  1/N sum_{b in batch_iter} metric(b), where  N is the sum of normalizer\n",
        "  over all batches.\n",
        "\n",
        "  Args:\n",
        "    train_state: TrainState, the state of training including the current\n",
        "      global_step, model_state, rng, params and optimizer state. The buffer of\n",
        "      this argument can be donated to the computation.\n",
        "    batch: A single batch of data. a metrics function, that given logits and\n",
        "      batch of data, calculates the metrics as well as the loss.\n",
        "    flax_model: A Flax model.\n",
        "    metrics_fn: A metrics function, that given logits and batch of data,\n",
        "      calculates the metrics as well as the loss.\n",
        "    config: Configurations of the experiment.\n",
        "    debug: Whether the debug mode is enabled during evaluation. `debug=True`\n",
        "      enables model specific logging/storing some values using\n",
        "      jax.host_callback.\n",
        "\n",
        "  Returns:\n",
        "    Calculated metrics and logits.\n",
        "  \"\"\"\n",
        "  # Add prediction targets\n",
        "  batch['targets'] = get_targets(batch, config)\n",
        "\n",
        "  if rng is None:\n",
        "    # Always use the same seed, so that eval is as consistent as possible\n",
        "    rng = jax.random.PRNGKey(config.rng_seed)\n",
        "\n",
        "\n",
        "  # Bind the rng to the host/device we are on.\n",
        "  dropout_rng = train_utils.bind_rng_to_host_device(\n",
        "      rng, axis_name='batch', bind_to='device'\n",
        "  )\n",
        "\n",
        "  variables = {'params': train_state.params, **train_state.model_state}\n",
        "  logits, aux = flax_model.apply(\n",
        "      variables,\n",
        "      batch['inputs'],\n",
        "      train=True,  # so that masking is enabled\n",
        "      mutable=False,\n",
        "      rngs={'dropout': dropout_rng},\n",
        "      debug=debug,\n",
        "  )\n",
        "\n",
        "  masked_tokens = aux['token_mask']\n",
        "  metrics = metrics_fn(logits, masked_tokens, batch)\n",
        "  return metrics, logits, masked_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JAovyzQPvui-"
      },
      "outputs": [],
      "source": [
        "# @title Trainer functions (other)\n",
        "\n",
        "def plot_reconstructed_image(\n",
        "    original_img: jnp.ndarray,\n",
        "    reconstructed_img: jnp.ndarray,\n",
        "    masked_tokens: jnp.ndarray,\n",
        "    step: Optional[int] = None,\n",
        "    split_name: Optional[str] = None,\n",
        "):\n",
        "  \"\"\"Plots the original image, image mask,and the reconstructed image.\n",
        "\n",
        "  Args:\n",
        "    original_img: The original image of shape [H, W, C].\n",
        "    reconstructed_img: The reconstructed image of shape\n",
        "      [num patches, patch size (pw*ph*C)].\n",
        "    masked_tokens: masked patches of shape [num patches].\n",
        "    step: The training step (int).\n",
        "    split_name: The data split name (str).\n",
        "  \"\"\"\n",
        "  img_h, img_w, img_c = original_img.shape  # original image shape\n",
        "  ph, pw = config.model.patches.size  # get the patch shape\n",
        "  nh, nw = (img_h // ph, img_w // pw)  # get the number of patches\n",
        "\n",
        "  # Reconstruct predicted img\n",
        "  r1 = jnp.reshape(reconstructed_img, shape=(nh, nw, ph, pw, img_c))\n",
        "  r2 = jnp.transpose(r1, (0, 2, 1, 3, 4))\n",
        "  r3 = jnp.reshape(r2, shape=(nh*ph, nw*pw, img_c))\n",
        "\n",
        "  # Construct patched mask img\n",
        "  p1 = jnp.ones((nh, ph, nw, pw, img_c))\n",
        "  p2 = jnp.transpose(p1, (0, 2, 1, 3, 4))\n",
        "  p3 = jnp.reshape(p2, shape=(nh*nw, ph*pw, img_c))\n",
        "  patched_ones = jnp.reshape(p3, shape=(nh*nw, ph*pw*img_c))\n",
        "  # Apply mask\n",
        "  weights_broadcast = jax.lax.broadcast_in_dim(\n",
        "    masked_tokens,\n",
        "    shape=patched_ones.shape,\n",
        "    broadcast_dimensions=tuple(range(masked_tokens.ndim)),\n",
        "  )\n",
        "  img_mask = model_utils.apply_weights(patched_ones, weights_broadcast)\n",
        "  # Reconstruct mask image\n",
        "  rm1 = jnp.reshape(img_mask, shape=(nh, nw, ph, pw, img_c))\n",
        "  rm2 = jnp.transpose(rm1, (0, 2, 1, 3, 4))\n",
        "  mask_img_negative = jnp.reshape(rm2, shape=(nh*ph, nw*pw, img_c))\n",
        "  mask_img = 1 - mask_img_negative\n",
        "\n",
        "  # Plot\n",
        "  original_title = \"Original\"\n",
        "  reconstructed_title = \"Reconstructed\"\n",
        "  masked_title = \"Mask\"\n",
        "  if split_name is not None:\n",
        "    original_title = original_title + f' ({split_name})'\n",
        "    reconstructed_title = reconstructed_title + f' ({split_name})'\n",
        "  if step is not None:\n",
        "    original_title = original_title + f': {step}'\n",
        "    reconstructed_title = reconstructed_title + f': {step}'\n",
        "\n",
        "  fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(8, 3))\n",
        "  ax[0].imshow(original_img)\n",
        "  ax[0].set_title(original_title)\n",
        "\n",
        "  ax[1].imshow(mask_img)\n",
        "  ax[1].set_title(masked_title)\n",
        "\n",
        "  ax[2].imshow(r3)\n",
        "  ax[2].set_title(reconstructed_title)\n",
        "  plt.show('\\n')\n",
        "\n",
        "\n",
        "def write_note(note):\n",
        "  \"\"\"Forked from google3/third_party/py/scenic/projects/multimask/trainer.py\"\"\"\n",
        "  if lead_host:\n",
        "    platform.work_unit().set_notes(note)\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    train_state: train_utils.TrainState,\n",
        "    step: int,\n",
        "    valid_iter: Iterator[Batch],\n",
        "    num_valid_ex: int,\n",
        "    plot_sample: bool = False,\n",
        "    rng: jax.random.PRNGKey = 42,\n",
        ") -\u003e Dict[str, Any]:\n",
        "  \"\"\"Run evaluation over validation sets.\n",
        "\n",
        "  Forked from google3/third_party/py/scenic/projects/multimask/trainer.py\n",
        "  \"\"\"\n",
        "\n",
        "  eval_summary = {}\n",
        "  if not isinstance(valid_iter, dict):  # Only on validation set.\n",
        "    valid_iter, num_valid_ex = {'valid': valid_iter}, {'valid': num_valid_ex}\n",
        "\n",
        "  # Set up random seed for plotting random sample, and generating random masks.\n",
        "  plot_rng, eval_rng = jax.random.split(rng)\n",
        "\n",
        "  # Iterate over validation sets\n",
        "  for val_name, val_iter in valid_iter.items():\n",
        "    num_ex = num_valid_ex[val_name]\n",
        "    # Ceil rounding such that we include the last incomplete batch.\n",
        "    eval_batch_size = config.get('eval_batch_size', config.batch_size)\n",
        "    total_eval_steps = int(np.ceil(num_ex / eval_batch_size))\n",
        "    steps_per_eval = config.get('steps_per_eval') or total_eval_steps\n",
        "    eval_metrics = []\n",
        "\n",
        "    # Get random plotting indices for each validation set\n",
        "    plot_step_idx = None  # eval step to plot from\n",
        "    plot_batch_idx = None  # sample from batch to plot\n",
        "    if plot_sample:\n",
        "      step_rng, batch_rng = jax.random.split(plot_rng)\n",
        "      plot_step_idx = jax.random.randint(\n",
        "        step_rng, shape=(), minval=0, maxval=steps_per_eval\n",
        "      )\n",
        "      plot_batch_idx = jax.random.randint(\n",
        "        batch_rng, shape=(), minval=0, maxval=eval_batch_size\n",
        "      )\n",
        "\n",
        "    # Evaluate step_per_eval over each validation set\n",
        "    for i in range(steps_per_eval):\n",
        "      eval_batch = next(val_iter)\n",
        "\n",
        "      keys = jax.random.split(eval_rng, jax.process_count() + 1)\n",
        "      eval_rng = keys[0]\n",
        "      eval_step_rng = keys[1:]\n",
        "      e_metrics, e_logits, masked_tokens = eval_step_pmapped(train_state, eval_batch, rng=eval_step_rng)\n",
        "      eval_metrics.append(train_utils.unreplicate_and_get(e_metrics))\n",
        "      eval_summary[val_name] = eval_metrics\n",
        "\n",
        "      # Plot out original vs reconstructed images\n",
        "      if plot_sample and i == plot_step_idx:\n",
        "        img_original = eval_batch['inputs'][0, plot_batch_idx, :, :, :]\n",
        "        img_reconstructed = e_logits[0, plot_batch_idx, :, :]\n",
        "        img_mask = masked_tokens[0, plot_batch_idx]\n",
        "        plot_reconstructed_image(\n",
        "            original_img=img_original,\n",
        "            reconstructed_img=img_reconstructed,\n",
        "            masked_tokens=img_mask,\n",
        "            step=step,\n",
        "            split_name=val_name,\n",
        "        )\n",
        "\n",
        "  return eval_summary\n",
        "\n",
        "\n",
        "def process_valid_summary(eval_summary):\n",
        "  mae_all = []\n",
        "  mae_masked_all = []\n",
        "  mse_all = []\n",
        "  mse_masked_all = []\n",
        "  for batch_eval in eval_summary['valid']:\n",
        "    mae = float(batch_eval['mean_absolute_error_all'][0])\n",
        "    mae_masked = float(batch_eval['mean_absolute_error_masked'][0])\n",
        "    mse = float(batch_eval['mean_squared_error_all'][0])\n",
        "    mse_masked = float(batch_eval['mean_squared_error_masked'][0])\n",
        "    mae_all.append(mae)\n",
        "    mae_masked_all.append(mae_masked)\n",
        "    mse_all.append(mse)\n",
        "    mse_masked_all.append(mse_masked)\n",
        "  return (\n",
        "      sum(mae_all) / len(mae_all),\n",
        "      sum(mae_masked_all) / len(mae_masked_all),\n",
        "      sum(mse_all) / len(mse_all),\n",
        "      sum(mse_masked_all) / len(mse_masked_all),\n",
        "  )\n",
        "\n",
        "\n",
        "def smooth_data(data, smoothing_factor=0.9):\n",
        "  smoothed_data = []\n",
        "  for i, value in enumerate(data):\n",
        "    if i == 0:\n",
        "      smoothed_data.append(value)\n",
        "    else:\n",
        "      smoothed_value = (\n",
        "          smoothing_factor * smoothed_data[-1] + (1 - smoothing_factor) * value\n",
        "      )\n",
        "      smoothed_data.append(smoothed_value)\n",
        "  return smoothed_data\n",
        "\n",
        "\n",
        "def plot_steps(data, mode='train', smoothing_factor=None):\n",
        "  # Create a DataFrame from the dictionary\n",
        "  df = pd.DataFrame.from_dict(\n",
        "      data,\n",
        "      orient='index',\n",
        "      columns=['mae_all', 'mae_masked_all', 'mse_all', 'mse_masked_all'],\n",
        "  )\n",
        "  df.reset_index(inplace=True)\n",
        "  df.rename(columns={'index': 'steps'}, inplace=True)\n",
        "\n",
        "  # Apply smoothing if specified\n",
        "  if smoothing_factor is not None:\n",
        "    df['mae_all'] = smooth_data(df['mae_all'].tolist(), smoothing_factor)\n",
        "    df['mae_masked_all'] = smooth_data(\n",
        "        df['mae_masked_all'].tolist(), smoothing_factor\n",
        "    )\n",
        "    df['mse_all'] = smooth_data(df['mse_all'].tolist(), smoothing_factor)\n",
        "    df['mse_masked_all'] = smooth_data(\n",
        "        df['mse_masked_all'].tolist(), smoothing_factor\n",
        "    )\n",
        "\n",
        "  # Create the plot\n",
        "  fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "  plot_titles = [\n",
        "      f'MAE All ({mode})',\n",
        "      f'MAE Masked All ({mode})',\n",
        "      f'MSE All ({mode})',\n",
        "      f'MSE Masked All ({mode})',\n",
        "  ]\n",
        "\n",
        "  sns.lineplot(ax=axes[0], x='steps', y='mae_all', data=df)\n",
        "  axes[0].set_title(plot_titles[0])\n",
        "  axes[0].set_xlabel('Steps')\n",
        "  axes[0].set_ylabel('MAE All')\n",
        "\n",
        "  sns.lineplot(ax=axes[1], x='steps', y='mae_masked_all', data=df)\n",
        "  axes[1].set_title(plot_titles[1])\n",
        "  axes[1].set_xlabel('Steps')\n",
        "  axes[1].set_ylabel('MAE Masked All')\n",
        "\n",
        "  sns.lineplot(ax=axes[2], x='steps', y='mse_all', data=df)\n",
        "  axes[2].set_title(plot_titles[2])\n",
        "  axes[2].set_xlabel('Steps')\n",
        "  axes[2].set_ylabel('MSE All')\n",
        "\n",
        "  sns.lineplot(ax=axes[3], x='steps', y='mse_masked_all', data=df)\n",
        "  axes[3].set_title(plot_titles[3])\n",
        "  axes[3].set_xlabel('Steps')\n",
        "  axes[3].set_ylabel('MSE Masked All')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CMl7HYfldTnK"
      },
      "outputs": [],
      "source": [
        "# @title Model Class and Trainer Selector Functions (copied from main.py)\n",
        "\n",
        "def get_model_cls(model_name: str):\n",
        "  \"\"\"Get the model class for the Multimask project.\n",
        "\n",
        "  Copied from google3/third_party/py/scenic/projects/multimask/main.py\n",
        "  \"\"\"\n",
        "  if model_name == 'vit_masked_encoder':\n",
        "    return vit_encoder.VitMaskedEncoderModel\n",
        "  elif model_name == 'vit_mae':\n",
        "    return ViTMAEModel\n",
        "  elif model_name == 'transformer_masked_encoder':\n",
        "    return transformer_encoder.TransformerMaskedEncoderModel\n",
        "  else:\n",
        "    raise ValueError(f'Unrecognized model: {model_name}.')\n",
        "\n",
        "\n",
        "def get_train_fn(trainer_name):\n",
        "  \"\"\" Copied from google3/third_party/py/scenic/projects/multimask/main.py\n",
        "  \"\"\"\n",
        "  if trainer_name == 'multimask_trainer':\n",
        "    return trainer.train\n",
        "  else:\n",
        "    raise ValueError(f'Unrecognized trainer: {trainer_name}.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWeI-XKRw-wF"
      },
      "source": [
        "## Training and Evaluation pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2RPnTd7FR9x"
      },
      "outputs": [],
      "source": [
        "# @title Config\n",
        "\n",
        "# To set values.\n",
        "NUM_TRAIN_STEPS = 20000  # steps to train for\n",
        "VARIANT = 'TiShallow/8'  # Model variant\n",
        "LRS = [1e-3]  # Learnign rate\n",
        "TOKEN_MASK_PROB = 'constant_0.8'  # Percentage of image to mask\n",
        "IMAGE_H = 64  # image height\n",
        "IMAGE_W = 64  # image width\n",
        "IMAGE_RESIZE = 80  # resize image before crop\n",
        "LOSS_ONLY_MASKED_TOKENS = False  # loss from only masked tokens?\n",
        "PLOT_EVAL_RECONSTRUCTIONS = True  # plot sample eval reconstructions\n",
        "\n",
        "# Model configurations.\n",
        "HIDDEN_SIZES = {\n",
        "    'Deb': 16,\n",
        "    'Ti': 192,\n",
        "    'TiShallow': 192,\n",
        "    'S': 384,\n",
        "    'SShallow': 384,\n",
        "    'M': 512,\n",
        "    'B': 768,\n",
        "    'L': 1024,\n",
        "    'H': 1280,\n",
        "    'g': 1408,\n",
        "    'G': 1664,\n",
        "    'e': 1792,\n",
        "}\n",
        "MLP_DIMS = {\n",
        "    'Deb': 32,\n",
        "    'Ti': 768,\n",
        "    'TiShallow': 768,\n",
        "    'S': 1536,\n",
        "    'SShallow': 1536,\n",
        "    'M': 2048,\n",
        "    'B': 3072,\n",
        "    'L': 4096,\n",
        "    'H': 5120,\n",
        "    'g': 6144,\n",
        "    'G': 8192,\n",
        "    'e': 15360,\n",
        "}\n",
        "NUM_HEADS = {\n",
        "    'Deb': 2,\n",
        "    'Ti': 3,\n",
        "    'TiShallow': 3,\n",
        "    'S': 6,\n",
        "    'SShallow': 6,\n",
        "    'M': 8,\n",
        "    'B': 12,\n",
        "    'L': 16,\n",
        "    'H': 16,\n",
        "    'g': 16,\n",
        "    'G': 16,\n",
        "    'e': 16,\n",
        "}\n",
        "NUM_LAYERS = {\n",
        "    'Deb': 2,\n",
        "    'Ti': 12,\n",
        "    'TiShallow': 4,\n",
        "    'S': 12,\n",
        "    'SShallow': 4,\n",
        "    'M': 12,\n",
        "    'B': 12,\n",
        "    'L': 24,\n",
        "    'H': 32,\n",
        "    'g': 40,\n",
        "    'G': 48,\n",
        "    'e': 56,\n",
        "}\n",
        "\n",
        "DECODER_HIDDEN_SIZES = {\n",
        "    'Deb': 16,\n",
        "    'Ti': 128,\n",
        "    'TiShallow': 128,\n",
        "    'S': 256,\n",
        "    'B': 512,\n",
        "    'L': 512,\n",
        "    'H': 512\n",
        "}\n",
        "DECODER_MLP_DIMS = {\n",
        "    'Deb': 32,\n",
        "    'Ti': 512,\n",
        "    'TiShallow': 512,\n",
        "    'S': 1024,\n",
        "    'B': 2048,\n",
        "    'L': 2048,\n",
        "    'H': 2048\n",
        "}\n",
        "DECODER_NUM_LAYERS = {\n",
        "    'Deb': 2,\n",
        "    'Ti': 2,\n",
        "    'TiShallow': 2,\n",
        "    'S': 4,\n",
        "    'B': 8,\n",
        "    'L': 8,\n",
        "    'H': 8\n",
        "}\n",
        "DECODER_NUM_HEADS = {\n",
        "    'Deb': 2,\n",
        "    'Ti': 4,\n",
        "    'TiShallow': 4,\n",
        "    'S': 8,\n",
        "    'B': 16,\n",
        "    'L': 16,\n",
        "    'H': 16\n",
        "}\n",
        "\n",
        "\n",
        "def get_config(runlocal=''):\n",
        "  \"\"\"Returns the ViT experiment configuration.\"\"\"\n",
        "\n",
        "  runlocal = bool(runlocal)\n",
        "\n",
        "  config = ml_collections.ConfigDict()\n",
        "  config.experiment_name = 'cifar10-mae-vit-tiny'\n",
        "  # Dataset.\n",
        "  config.dataset_name = 'bit'\n",
        "  config.data_dtype_str = 'float32'\n",
        "  config.dataset_configs = ml_collections.ConfigDict()\n",
        "  config.dataset_configs.dataset = 'cifar10'\n",
        "  # config.dataset_configs.num_classes = NUM_CLASSES\n",
        "  config.dataset_configs.train_split = 'train'\n",
        "  config.dataset_configs.val_split = 'test'\n",
        "  config.dataset_configs.pp_train = (\n",
        "      'decode'\n",
        "      f'|resize_small({IMAGE_RESIZE})|random_crop({IMAGE_H})|flip_lr'\n",
        "      '|value_range(-1, 1)'\n",
        "      '|copy(\"label\", \"labels\")'\n",
        "  )\n",
        "  config.dataset_configs.pp_eval = (\n",
        "      'decode'\n",
        "      f'|resize_small({IMAGE_RESIZE})|central_crop({IMAGE_H})'\n",
        "      '|value_range(-1, 1)'\n",
        "      '|copy(\"label\", \"labels\")'\n",
        "  )\n",
        "\n",
        "  # Prefetch and shuffle.\n",
        "  config.dataset_configs.prefetch_to_device = 2\n",
        "  # Shuffle_buffer_size is per host, so small-ish is ok.\n",
        "  config.dataset_configs.shuffle_buffer_size = 250_000\n",
        "\n",
        "  # Model.\n",
        "  version, patch = VARIANT.split('/')\n",
        "  patch_h = int(patch)\n",
        "  patch_w = int(patch)\n",
        "  version = 'Deb' if runlocal else version\n",
        "  config.model_name = 'vit_mae'\n",
        "  config.model = ml_collections.ConfigDict()\n",
        "  # encoder\n",
        "  config.model.hidden_size = HIDDEN_SIZES[version]\n",
        "  config.model.patches = ml_collections.ConfigDict()\n",
        "  config.model.patches.size = [patch_h, patch_w]\n",
        "  config.model.num_heads = NUM_HEADS[version]\n",
        "  config.model.mlp_dim = MLP_DIMS[version]\n",
        "  config.model.num_layers = NUM_LAYERS[version]\n",
        "  config.model.dropout_rate = 0.\n",
        "  config.model.classifier = 'none'  # Has to be \"none\" for the autoencoder\n",
        "  config.model.representation_size = None\n",
        "  config.model.positional_embedding = 'sinusoidal_2d'\n",
        "  config.model.positional_embedding_decoder = 'sinusoidal_2d'\n",
        "  # decoder\n",
        "  config.model.decoder_config = ml_collections.ConfigDict()\n",
        "  config.model.decoder_config.hidden_size = DECODER_HIDDEN_SIZES[version]\n",
        "  config.model.decoder_config.mlp_dim = DECODER_MLP_DIMS[version]\n",
        "  config.model.decoder_config.num_layers = DECODER_NUM_LAYERS[version]\n",
        "  config.model.decoder_config.num_heads = DECODER_NUM_HEADS[version]\n",
        "  config.model.decoder_config.dropout_rate = 0.\n",
        "  config.model.decoder_config.attention_dropout_rate = 0.\n",
        "\n",
        "  config.masked_feature_loss = ml_collections.ConfigDict()\n",
        "  config.masked_feature_loss.targets_type = 'rgb'\n",
        "  config.masked_feature_loss.token_mask_probability = TOKEN_MASK_PROB\n",
        "  config.masked_feature_loss.loss_only_masked_tokens = LOSS_ONLY_MASKED_TOKENS\n",
        "  config.masked_feature_loss.loss_type = 'squared'  # 'squared' or 'absolute'\n",
        "\n",
        "  # Training.\n",
        "  config.trainer_name = 'multimask_trainer'\n",
        "  config.batch_size = 8 if runlocal else 1024\n",
        "  config.num_training_steps = NUM_TRAIN_STEPS\n",
        "  config.log_eval_steps = 100\n",
        "  config.log_summary_steps = 100\n",
        "  config.rng_seed = 42\n",
        "  sched = ml_collections.ConfigDict()\n",
        "  sched.re = '(.*)'\n",
        "  sched.lr_configs = ml_collections.ConfigDict()\n",
        "  sched.lr_configs.learning_rate_schedule = 'compound'\n",
        "  sched.lr_configs.factors = 'constant * cosine_decay * linear_warmup'\n",
        "  sched.lr_configs.total_steps = NUM_TRAIN_STEPS\n",
        "  sched.lr_configs.steps_per_cycle = sched.lr_configs.total_steps\n",
        "  sched.lr_configs.warmup_steps = 2000\n",
        "  sched.lr_configs.base_learning_rate = LRS[0]\n",
        "  config.schedule = ml_collections.ConfigDict({'all': sched})\n",
        "\n",
        "  # *Single* optimizer.\n",
        "  optim = ml_collections.ConfigDict()\n",
        "  optim.optax_name = 'scale_by_adam'\n",
        "  # optim.optax = dict(mu_dtype='bfloat16')\n",
        "  optim.optax_configs = ml_collections.ConfigDict(\n",
        "      {  # Optimizer settings.\n",
        "          'b1': 0.9,\n",
        "          'b2': 0.999,\n",
        "      })\n",
        "  config.optax = dict(mu_dtype='bfloat16')\n",
        "  optim.max_grad_norm = 1.0\n",
        "\n",
        "  optim.weight_decay = 1e-4\n",
        "  optim.weight_decay_decouple = True\n",
        "  config.optimizer = optim\n",
        "\n",
        "  # Fewshot.\n",
        "  config.fewshot = common_fewshot.get_config(\n",
        "      batch_size=config.batch_size, target_resolution=64, resize_resolution=80\n",
        "  )\n",
        "  config.fewshot.datasets = {\n",
        "      'cifar100': ('cifar100', 'train', 'test'),\n",
        "      'cifar10': ('cifar10', 'train', 'test'),\n",
        "  }\n",
        "  config.fewshot.walk_first = ('cifar10', 10)\n",
        "  config.fewshot.representation_layer = 'pre_logits'\n",
        "  config.fewshot.log_eval_steps = 1000\n",
        "\n",
        "  # Logging.\n",
        "  config.write_summary = True\n",
        "  config.xprof = True  # Profile using xprof.\n",
        "  config.checkpoint = True  # Do checkpointing.\n",
        "  config.checkpoint_steps = 1000\n",
        "  config.debug_train = False  # Debug mode during training.\n",
        "  config.debug_eval = False  # Debug mode during eval.\n",
        "  # Plot a random image reconstruction during each eval.\n",
        "  config.plot_reconstructions = PLOT_EVAL_RECONSTRUCTIONS\n",
        "\n",
        "  # BEGIN GOOGLE-INTERNAL\n",
        "  if runlocal:\n",
        "    # Current implementation fails with UPTC.\n",
        "    config.count_flops = False\n",
        "  # END GOOGLE-INTERNAL\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "# BEGIN GOOGLE-INTERNAL\n",
        "def get_hyper(hyper):\n",
        "  \"\"\"Defines the hyper-parameters sweeps for doing grid search.\"\"\"\n",
        "  return hyper.product([\n",
        "      hyper.sweep('config.schedule.all.lr_configs.base_learning_rate', LRS),\n",
        "  ])\n",
        "config = get_config(True)\n",
        "config.trainer_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LP39TogyGGEl"
      },
      "outputs": [],
      "source": [
        "#@title Init Training States\n",
        "\n",
        "model_cls = get_model_cls(config.model_name)\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = train_utils.get_dataset(\n",
        "    config, data_rng\n",
        ")\n",
        "\n",
        "lead_host = jax.process_index() == 0\n",
        "# Build the loss_fn, metrics, and flax_model.\n",
        "model = model_cls(config, dataset.meta_data)\n",
        "\n",
        "# Initialize model.\n",
        "rng, params_init_rng, dropout_init_rng = jax.random.split(rng, num=3)\n",
        "init_rngs = {'params': params_init_rng, 'dropout': dropout_init_rng}\n",
        "init_batch = next(dataset.train_iter)\n",
        "(params, model_state, num_trainable_params, gflops) = (\n",
        "    train_utils.initialize_model(\n",
        "        model_def=model.flax_model,\n",
        "        input_spec=[\n",
        "            (init_batch['inputs'].shape[1:], init_batch['inputs'].dtype)\n",
        "        ],\n",
        "        config=config,\n",
        "        rngs=init_rngs,\n",
        "        train=True,  # so that masking and decoding in MAE are initialized\n",
        "    )\n",
        ")\n",
        "\n",
        "# Create LR schedules and optimizer.\n",
        "schedule_fns = scenic_optax.make_schedule(config.get('schedule'))\n",
        "tx, _ = scenic_optax.make(config.optimizer, schedule_fns, params)\n",
        "opt_state = tx.init(params)\n",
        "\n",
        "rng, train_rng = jax.random.split(rng)\n",
        "\n",
        "# Create chrono class to track and store training statistics and metadata:\n",
        "chrono = train_utils.Chrono()\n",
        "\n",
        "train_state = train_utils.TrainState(\n",
        "    global_step=0,\n",
        "    opt_state=opt_state,\n",
        "    tx=tx,\n",
        "    params=params,\n",
        "    model_state=model_state,\n",
        "    rng=train_rng,\n",
        "    metadata={'chrono': chrono.save()},\n",
        ")\n",
        "start_step = train_state.global_step\n",
        "chrono.load(train_state.metadata['chrono'])\n",
        "train_state = train_state.replace(metadata={})\n",
        "\n",
        "# Replicate the optimzier, state, and rng.\n",
        "train_state = jax_utils.replicate(train_state)\n",
        "del params  # Do not keep a copy of the initial params.\n",
        "\n",
        "# Calculate the total number of training steps.\n",
        "# TODO(adosovitskiy): get rid of epochs?\n",
        "total_steps, steps_per_epoch = train_utils.get_num_training_steps(\n",
        "    config, dataset.meta_data\n",
        ")\n",
        "\n",
        "train_step_pmapped = jax.pmap(\n",
        "    functools.partial(\n",
        "        trainer.train_step,\n",
        "        flax_model=model.flax_model,\n",
        "        loss_fn=model.loss_function,\n",
        "        lr_fns={name: lr_fn for _, name, (lr_fn, _) in schedule_fns},\n",
        "        metrics_fn=model.get_metrics_fn('train'),\n",
        "        config=config,\n",
        "        debug=config.debug_train,\n",
        "    ),\n",
        "    axis_name='batch',\n",
        "    # We can donate both buffers of train_state and train_batch.\n",
        "    donate_argnums=(0, 1),\n",
        ")\n",
        "eval_step_pmapped = jax.pmap(\n",
        "    functools.partial(\n",
        "        eval_step,\n",
        "        flax_model=model.flax_model,\n",
        "        metrics_fn=model.get_metrics_fn('validation'),\n",
        "        config=config,\n",
        "        debug=config.debug_eval,\n",
        "    ),\n",
        "    axis_name='batch',\n",
        "    # We can donate the eval_batch's buffer.\n",
        "    donate_argnums=(1,),\n",
        ")\n",
        "\n",
        "if 'fewshot' in config:\n",
        "  representation_fn_partial = functools.partial(\n",
        "      trainer.representation_fn,\n",
        "      flax_model=model.flax_model,\n",
        "      representation_layer=config.fewshot.representation_layer,\n",
        "  )\n",
        "\n",
        "  fewshotter = fewshot_utils.FewShotEvaluator(\n",
        "      representation_fn_partial, config.fewshot\n",
        "  )\n",
        "\n",
        "log_eval_steps = config.get('log_eval_steps')\n",
        "if not log_eval_steps:\n",
        "  raise ValueError(\"'log_eval_steps' should be specified in the config.\")\n",
        "checkpoint_steps = config.get('checkpoint_steps') or log_eval_steps\n",
        "log_summary_steps = config.get('log_summary_steps') or log_eval_steps\n",
        "\n",
        "train_metrics, extra_training_logs = [], []\n",
        "train_summary, eval_summary = None, None\n",
        "\n",
        "chrono.inform(start_step, total_steps, config.batch_size, steps_per_epoch)\n",
        "logging.info('Starting training loop at step %d.', start_step + 1)\n",
        "\n",
        "hooks = []\n",
        "\n",
        "# Print information about available compute\n",
        "print(\"Available devices:\", jax.devices())\n",
        "print(\"Default device:\", jax.default_backend())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WCpq05NX3t1a"
      },
      "outputs": [],
      "source": [
        "# @title Sample Image Patching and Reconstruction\n",
        "\n",
        "def plot_patched_img(img_patches, title, nh, nw, ph, pw, img_c, ax, axis_on=False):\n",
        "    inner_gs = gridspec.GridSpecFromSubplotSpec(nh, nw, subplot_spec=ax, wspace=0.1, hspace=0.1)\n",
        "    patch_idx = 0\n",
        "    for _ in range(nh):\n",
        "        for _ in range(nw):\n",
        "            ax_sub = plt.subplot(inner_gs[patch_idx]);\n",
        "            p_img = img_patches[patch_idx]\n",
        "            p_img = np.reshape(p_img, newshape=(ph, pw, img_c))\n",
        "            ax_sub.imshow(p_img)\n",
        "            if not axis_on:\n",
        "              ax_sub.axis('off')\n",
        "            else:\n",
        "              ax_sub.set_xticks([])\n",
        "              ax_sub.set_yticks([])\n",
        "            patch_idx += 1\n",
        "    plt.text(0.5, 1.05, title, ha='center', va='bottom', transform=ax.transAxes, fontsize=12)\n",
        "\n",
        "\n",
        "def plot_sample_img(img, title, ax):\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(title)\n",
        "    ax.axis('off')\n",
        "\n",
        "\n",
        "# Original image and meta data\n",
        "img = init_batch['inputs'][0, [0, 1], :, :, :]\n",
        "b, img_h, img_w, img_c = img.shape  # img shape\n",
        "ph, pw = config.model.patches.size  # patch size\n",
        "nh, nw = (img_h // ph, img_w // pw)  # number of patches\n",
        "\n",
        "# Patch image\n",
        "p1 = jnp.reshape(img, shape=(b, nh, ph, nw, pw, img_c))\n",
        "p2 = jnp.transpose(p1, (0, 1, 3, 2, 4, 5))\n",
        "p3 = jnp.reshape(p2, shape=(b, nh*nw, ph*pw, img_c))\n",
        "p4 = jnp.reshape(p3, shape=(b, nh*nw, ph*pw*img_c))\n",
        "\n",
        "# Set up masking weight matrices\n",
        "num_patches = nh * nw\n",
        "mask_percentage = 0.5\n",
        "masked_patches = int(mask_percentage * num_patches)\n",
        "masked_indices_1 = np.random.choice(num_patches, masked_patches, replace=False)\n",
        "masked_indices_2 = np.random.choice(num_patches, masked_patches, replace=False)\n",
        "\n",
        "# masked_indices_1 = [5, 6, 32, 45]\n",
        "# masked_indices_2 = [0, 1, 2, 3, 63]\n",
        "# test masking weights (used for plotting)\n",
        "weights_array = np.ones_like(p3)\n",
        "weights_array[0, masked_indices_1, :] = 0\n",
        "weights_array[1, masked_indices_2, :] = 0\n",
        "weights_array = jnp.array(weights_array)\n",
        "# masking weights\n",
        "weights = np.ones((p4.shape[:-1]))\n",
        "weights[0, masked_indices_1] = 0  # img 1 mask\n",
        "weights[1, masked_indices_2] = 0  # img 2 mask\n",
        "weights = jnp.array(weights)\n",
        "\n",
        "# Reconstruct image\n",
        "r1 = jnp.reshape(p4, shape=(b, nh, nw, ph, pw, img_c))\n",
        "r2 = jnp.transpose(r1, (0, 1, 3, 2, 4, 5))\n",
        "r3 = jnp.reshape(r2, shape=(b, nh*ph, nw*pw, img_c))\n",
        "\n",
        "# Mask image\n",
        "weights_broadcast = jax.lax.broadcast_in_dim(\n",
        "    weights,\n",
        "    shape=p4.shape,\n",
        "    broadcast_dimensions=tuple(range(weights.ndim)),\n",
        ")\n",
        "masked_img = model_utils.apply_weights(p4, weights_broadcast)\n",
        "\n",
        "# Reconstructed masked images\n",
        "rm1 = jnp.reshape(masked_img, shape=(b, nh, nw, ph, pw, img_c))\n",
        "rm2 = jnp.transpose(rm1, (0, 1, 3, 2, 4, 5))\n",
        "rm3 = jnp.reshape(rm2, shape=(b, nh*ph, nw*pw, img_c))\n",
        "\n",
        "# Plot\n",
        "for img_idx in range(b):\n",
        "  plt.figure(figsize=(20, 3.5))\n",
        "  main_gs = gridspec.GridSpec(1, 5)\n",
        "\n",
        "  # Original image\n",
        "  original = plt.subplot(main_gs[0, 0])\n",
        "  plot_sample_img(img[img_idx], 'Original', original)\n",
        "\n",
        "  # Patched image\n",
        "  patched = plt.subplot(main_gs[0, 1])\n",
        "  plot_patched_img(p3[img_idx], 'Patched', nh, nw, ph, pw, img_c, patched)\n",
        "\n",
        "  # Reconstructed image\n",
        "  reconstruct = plt.subplot(main_gs[0, 2])\n",
        "  plot_sample_img(r3[img_idx], 'Reconstruction', reconstruct)\n",
        "\n",
        "  # Mask image\n",
        "  mask = plt.subplot(main_gs[0, 3])\n",
        "  plot_patched_img(\n",
        "      weights_array[img_idx], f'Mask ({int(100*mask_percentage)}%)',\n",
        "      nh, nw, ph, pw, img_c, mask, axis_on=True\n",
        "  )\n",
        "\n",
        "  # Masked image\n",
        "  masked_img = plt.subplot(main_gs[0, 4])\n",
        "  plot_sample_img(rm3[img_idx], 'Masked Reconstruction', masked_img)\n",
        "\n",
        "  plt.show();\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OQK_9W0VWoLf"
      },
      "outputs": [],
      "source": [
        "#@title Run Training\n",
        "\n",
        "# Adapted from google3/third_party/py/scenic/projects/multimask/trainer.py.\n",
        "\n",
        "train_losses = {}\n",
        "validation_losses = {}\n",
        "\n",
        "flax.config.update('flax_use_orbax_checkpointing', False)\n",
        "for step in range(start_step + 1, total_steps + 1):\n",
        "  with jax.profiler.StepTraceAnnotation('train', step_num=step):\n",
        "    train_batch = next(dataset.train_iter)\n",
        "    train_state, t_metrics, t_logs = train_step_pmapped(\n",
        "        train_state, train_batch\n",
        "    )\n",
        "\n",
        "    # This will accumulate metrics in TPU memory up to the point that we log\n",
        "    # them. This is no problem for small metrics but may be a problem for\n",
        "    # large (e.g. segmentation) metrics. An alternative is to set\n",
        "    # `log_summary_steps` to a small number, or to use\n",
        "    # `train_utils.unreplicate_and_get` here instead of right before writing\n",
        "    # summaries, but that means in each step, we have data transfer between\n",
        "    # tpu and host, which might slow down the training.\n",
        "    train_metrics.append(t_metrics)\n",
        "    # Additional training logs: learning rate:\n",
        "    t_logs = jax.tree_util.tree_map(jax_utils.unreplicate, t_logs)\n",
        "    extra_training_logs.append(t_logs)\n",
        "    mae_all = round(float(t_metrics['mean_absolute_error_all'][0][0]), 2)\n",
        "    mae_masked = round(float(t_metrics['mean_absolute_error_masked'][0][0]), 2)\n",
        "    mse_all = round(float(t_metrics['mean_squared_error_all'][0][0]), 2)\n",
        "    mse_masked = round(float(t_metrics['mean_squared_error_masked'][0][0]), 2)\n",
        "    if step % 100 == 0:\n",
        "      print(\n",
        "          'step',\n",
        "          step,\n",
        "          'mae_all:',\n",
        "          mae_all,\n",
        "          'mae_masked:',\n",
        "          mae_masked,\n",
        "          'mse_all:',\n",
        "          mse_all,\n",
        "          'mse_masked:',\n",
        "          mse_masked,\n",
        "          sep=' ',\n",
        "      )\n",
        "    train_losses[step] = (mae_all, mae_masked, mse_all, mse_masked)\n",
        "  # Quick indication that training is happening.\n",
        "  logging.log_first_n(logging.INFO, 'Finished training step %d.', 5, step)\n",
        "  for h in hooks:\n",
        "    h(step)\n",
        "  ################### EVALUATION #######################\n",
        "  if (step % log_eval_steps == 0) or (step == total_steps):\n",
        "    # chrono.pause(wait_for=(train_state.params))\n",
        "    train_state = train_utils.sync_model_state_across_replicas(train_state)\n",
        "    rng, eval_rng = jax.random.split(rng)\n",
        "    eval_summary = evaluate(\n",
        "          train_state,\n",
        "          step,\n",
        "          dataset.valid_iter,\n",
        "          dataset.meta_data['num_eval_examples'],\n",
        "          plot_sample=config.plot_reconstructions,\n",
        "          rng=eval_rng,\n",
        "      )\n",
        "    mae_all, mae_masked_all, mse_all, mse_masked_all = process_valid_summary(eval_summary)\n",
        "    validation_losses[step] = (mae_all, mae_masked_all, mse_all, mse_masked_all)\n",
        "\n",
        "plot_steps(validation_losses, 'validation')\n",
        "plot_steps(train_losses, 'train', smoothing_factor=0.9)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1N87hdhFaNHRRGsYZoAAiivaDbudNzAhE",
          "timestamp": 1716438287052
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
