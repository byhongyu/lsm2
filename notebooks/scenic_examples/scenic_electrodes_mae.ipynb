{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mswgwo-WiOod"
      },
      "source": [
        "# Electrode Masked Auto Encoder Training with Scenic Framework\n",
        "\n",
        "1.   Implements a methods to read in a custom (electrodes) datasets, compiant with the Scenic framework.\n",
        "2.   Adapts the ViT MAE structure to allow for single channel image inputs\n",
        "3.   Adds custom processing to allow for non-square patches   \n",
        "\n",
        "\n",
        "#### Colab Kernel (Electrodes Kernel)\n",
        "Grants command for Access on Demand (AoD):\n",
        "\n",
        "https://grants.corp.google.com/#/grants?request=20h%2Fchr-ards-electrodes-deid-colab-jobs\u0026reason=b%2F314799341\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3H9M0KUr875D"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "import functools\n",
        "from typing import Any, Callable, Dict, Iterator, Tuple, Optional, Type, Union\n",
        "\n",
        "from absl import logging\n",
        "from clu import metric_writers\n",
        "from clu import periodic_actions\n",
        "from clu import platform\n",
        "import flax\n",
        "from flax import jax_utils\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.profiler\n",
        "import ml_collections\n",
        "import numpy as np\n",
        "import optax\n",
        "from colabtools import adhoc_import\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import time\n",
        "import tqdm\n",
        "\n",
        "with adhoc_import.Google3():\n",
        "  from scenic.dataset_lib import dataset_utils\n",
        "  from scenic.google.xm import xm_utils\n",
        "  from scenic.model_lib.base_models import base_model\n",
        "  from scenic.model_lib.base_models import model_utils\n",
        "  from scenic.projects.multimask.models import model_utils as mm_model_utils\n",
        "  from scenic.model_lib.layers import nn_layers\n",
        "  from scenic.model_lib.layers import nn_ops\n",
        "  # To register the preprocessing ops\n",
        "  from scenic.projects.multimask import data_utils  # pylint: disable=unused-import\n",
        "\n",
        "  from scenic.train_lib import optax as scenic_optax\n",
        "  from scenic.train_lib import pretrain_utils\n",
        "  from scenic.train_lib import train_utils\n",
        "  from scenic.train_lib.transfer import fewshot_utils\n",
        "  from scenic.projects.multimask import trainer\n",
        "  from scenic.projects.multimask.models import transformer_encoder\n",
        "  from scenic.projects.baselines import vit\n",
        "  from scenic.projects.multimask.models import vit_encoder\n",
        "  from scenic.projects.multimask.models import vit_mae\n",
        "  from scenic.projects.baselines.configs.google.common import common_fewshot\n",
        "\n",
        "# Aliases for custom types:\n",
        "Batch = Dict[str, jnp.ndarray]\n",
        "MetricFn = Callable[\n",
        "    [jnp.ndarray, jnp.ndarray, Dict[str, jnp.ndarray]],\n",
        "    Dict[str, Tuple[float, int]],\n",
        "]\n",
        "LossFn = Callable[\n",
        "    [jnp.ndarray, Batch, Optional[jnp.ndarray], jnp.ndarray], float\n",
        "]\n",
        "LrFns = Dict[str, Callable[[jnp.ndarray], jnp.ndarray]]\n",
        "Patch = Union[Tuple[int, int], Tuple[int, int, int]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkKKAmEbf6hJ"
      },
      "source": [
        "## Sample dataset visualization and metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "su7PJmuCjkjL"
      },
      "outputs": [],
      "source": [
        "# @title Electrodes dataset meta data\n",
        "dataset_name = 'lsm_prod/lsm_300min_100K_unimpute'\n",
        "data_dir = '/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-electrodes/deid/raw/datasets/msa_1_5/lsm_tfds_datasets'\n",
        "featurenames_csv = '/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-electrodes/deid/raw/datasets/msa_1_5/lsm_tfds_datasets/lsm_prod/lsm_300min_100K_unimpute/1.0.0/Dataset_FeatureNames.csv'\n",
        "\n",
        "print('TF Dataset Information')\n",
        "tfds.builder(dataset_name, data_dir=data_dir, try_gcs=True).info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4J4oifrNf5Ox"
      },
      "outputs": [],
      "source": [
        "# @title Load dataset\n",
        "ds = tfds.load(dataset_name, data_dir=data_dir, split='train', shuffle_files=True)\n",
        "assert isinstance(ds, tf.data.Dataset)\n",
        "print(f'Dataset:\\n{ds}\\n')\n",
        "\n",
        "with gfile.Open(featurenames_csv, 'r') as f:\n",
        "  df = pd.read_csv(f)\n",
        "\n",
        "features = df.columns\n",
        "print(f'Features:\\n{features}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "L9USCf8Zgik_"
      },
      "outputs": [],
      "source": [
        "# @title Plot sample feature image\n",
        "\n",
        "ds = ds.take(1)  # Only take a single example\n",
        "\n",
        "for example in ds:  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
        "  inputs = example[\"input_signal\"]\n",
        "  label = example[\"label\"]\n",
        "  print('Example meta data:')\n",
        "  print('Example Keys:', list(example.keys()))\n",
        "  print('Inputs shape:', inputs.shape)\n",
        "  print('Labels shape:', label)\n",
        "\n",
        "  plt.figure(figsize=(15,10))\n",
        "  imgplot = plt.imshow(np.swapaxes(inputs,0,1))\n",
        "  plt.grid(None)\n",
        "  plt.xlabel('Time (minutes)')\n",
        "  plt.ylabel('Feature #')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zPSeZBSfglln"
      },
      "outputs": [],
      "source": [
        "# @title Plot individual features\n",
        "\n",
        "print('Printing feature rows separately...')\n",
        "for example in ds:  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
        "  inputs = example[\"input_signal\"]\n",
        "  label = example[\"label\"]\n",
        "\n",
        "  fig, axs = plt.subplots(25, 1, figsize=(10,35))#, layout='constrained')\n",
        "  for i, ax in enumerate(axs):\n",
        "    ax.plot(inputs[:,i])\n",
        "    ax.set_title(features[i])\n",
        "    if i \u003c len(axs) - 1:\n",
        "      ax.get_xaxis().set_ticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EOwq7pigKIo"
      },
      "source": [
        "## Helpers (Functions and Classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "V7x6RPjS3RlB"
      },
      "outputs": [],
      "source": [
        "# @title Trainer functions (adapted from trainer.py)\n",
        "\n",
        "\"\"\" Trainer functions.\n",
        "\n",
        "Adapted from google3/third_party/py/scenic/projects/multimask/trainer.py.\n",
        "\n",
        "The below funcctions are adapted to allow for different input field names.\n",
        "The original, expected field name was 'inputs', and it has here\n",
        "been modified to 'input_signals'.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_targets(batch: Batch, config: ml_collections.ConfigDict) -\u003e jnp.ndarray:\n",
        "  \"\"\"Adapted from: google3/third_party/py/scenic/projects/multimask/trainer.py\n",
        "  \"\"\"\n",
        "\n",
        "  targets_type = config.masked_feature_loss.targets_type\n",
        "  if targets_type == 'rgb':\n",
        "    return get_rgb_targets(batch['input_signal'], tuple(config.model.patches.size))\n",
        "  elif targets_type == 'tokens':\n",
        "    return nn.one_hot(batch['input_signal'], num_classes=config.model.vocab_size)\n",
        "  else:\n",
        "    raise ValueError(f'Unknown targets_type {targets_type}')\n",
        "\n",
        "\n",
        "def get_rgb_targets(\n",
        "    inputs: jnp.ndarray,\n",
        "    patch_size: Patch,\n",
        "    reconstruct_grayscale: Optional[bool] = False,\n",
        "    standardise_per_patch: Optional[bool] = False,\n",
        ") -\u003e jnp.ndarray:\n",
        "  \"\"\"Get RGB targets to use for feature regression.\n",
        "\n",
        "  Copied from: google3/third_party/py/scenic/projects/multimask/trainer.py\n",
        "\n",
        "  Here, the targets are the raw rgb patches of the image.\n",
        "\n",
        "  Args:\n",
        "    inputs: Tensor of shape [b, h, w, c] or [b, t, h, w, c]. The former are\n",
        "      images, and the later video.\n",
        "    patch_size: The shape of the patch, defined as [ph, pw] for images, and [ph,\n",
        "      pw, pt] for video.\n",
        "    reconstruct_grayscale: If True, the target patch is in grayscale rather than\n",
        "      rgb.\n",
        "    standardise_per_patch: If true, standardise each patch by subtracting the\n",
        "      mean and dividing by the standard deviation of that patch.\n",
        "\n",
        "  Returns:\n",
        "    Patched inputs. For images, shape is [b, gh * gw, ph * pw * c] where\n",
        "      gh = h // ph and gw = w // pw.\n",
        "      For video, shape is [b, gt * gh * gw, pt * ph * pw * c].\n",
        "  \"\"\"\n",
        "  if inputs.ndim != 4:\n",
        "    raise ValueError('Inputs should be 4D (images). Shape {inputs.shape}')\n",
        "\n",
        "  if reconstruct_grayscale:\n",
        "    # Reference for converting between RGB and grayscale.\n",
        "    # https://en.wikipedia.org/wiki/Luma_%28video%29\n",
        "    # Also used in tf.image.rgb_to_grayscale\n",
        "    rgb_weights = jnp.tile(jnp.array([[0.2989, 0.5870, 0.1140]]), (3, 1)).T\n",
        "    inputs = jnp.matmul(inputs, rgb_weights)\n",
        "\n",
        "  assert inputs.ndim == 4, 'the input should shape BxHxWxC'\n",
        "  batch = inputs.shape[0]\n",
        "  # Shape is [batch, ht, wt, hp, wp, c]\n",
        "  patched_image = nn_ops.patch_image(\n",
        "      inputs, inputs_shape=None, patch_size=patch_size\n",
        "  )\n",
        "  num_tokens = patched_image.shape[1] * patched_image.shape[2]\n",
        "  patched_input = jnp.reshape(patched_image, (batch, num_tokens, -1))\n",
        "\n",
        "  if standardise_per_patch:\n",
        "    patched_input = jax.nn.standardize(patched_input, axis=-1, epsilon=1e-6)\n",
        "\n",
        "  return patched_input\n",
        "\n",
        "\n",
        "# Forked from projects/mfp/trainer.py\n",
        "def representation_fn(\n",
        "    train_state: train_utils.TrainState,\n",
        "    batch: Batch,\n",
        "    *,\n",
        "    flax_model: nn.Module,\n",
        "    representation_layer: str,\n",
        "    gather_to_host: bool = True\n",
        ") -\u003e Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
        "  \"\"\"Feeds the inputs to the model and returns their representations.\n",
        "\n",
        "  Adapted from: google3/third_party/py/scenic/projects/multimask/trainer.py\n",
        "\n",
        "  Args:\n",
        "    train_state: TrainState, the state of training including the current\n",
        "      global_step, model_state, rng, and optimizer. The buffer of this argument\n",
        "      can be donated to the computation.\n",
        "    batch: A single batch of data from the dataset.\n",
        "    flax_model: A Flax model.\n",
        "    representation_layer: The name of the layer to use as the representation.\n",
        "    gather_to_host: Whether to gather results from all devices to the host,\n",
        "      rather than leaving them distributed.\n",
        "\n",
        "  Returns:\n",
        "    Representation learned by the model for the given inputs and the labels and\n",
        "    masks. If `gather_to_host` is True, these are collected from all hosts.\n",
        "  \"\"\"\n",
        "  variables = {'params': train_state.params, **train_state.model_state}\n",
        "\n",
        "  _, aux = flax_model.apply(\n",
        "      variables,\n",
        "      batch['input_signal'],\n",
        "      train=False,\n",
        "      debug=False\n",
        "  )\n",
        "  representation = aux[representation_layer]\n",
        "\n",
        "  if representation.ndim == 3:\n",
        "    # Feature regression models return [batch, num_tokens, channels]\n",
        "    logging.info('Representation shape before pooling tokens: %s',\n",
        "                 representation.shape)\n",
        "    representation = jnp.mean(representation, axis=1)\n",
        "  logging.info('Representation shape: %s', representation.shape)\n",
        "\n",
        "  if gather_to_host:\n",
        "    representation = jax.lax.all_gather(representation, 'batch')\n",
        "    batch = jax.lax.all_gather(batch, 'batch')\n",
        "  return representation, batch['label'], batch['batch_mask']\n",
        "\n",
        "\n",
        "# Forked from projects/baselines/plainvit/trainer.py\n",
        "def train_step(\n",
        "    train_state: train_utils.TrainState,\n",
        "    batch: Batch,\n",
        "    *,\n",
        "    flax_model: nn.Module,\n",
        "    loss_fn: LossFn,\n",
        "    lr_fns: LrFns,\n",
        "    metrics_fn: MetricFn,\n",
        "    config: ml_collections.ConfigDict,\n",
        "    debug: Optional[bool] = False,\n",
        ") -\u003e Tuple[\n",
        "    train_utils.TrainState, Dict[str, Tuple[float, int]], Dict[str, Any]\n",
        "]:\n",
        "  \"\"\"Runs a single step of training.\n",
        "\n",
        "  Adapted from: google3/third_party/py/scenic/projects/multimask/trainer.py\n",
        "\n",
        "  Given the state of the training and a batch of data, computes\n",
        "  the loss and updates the parameters of the model.\n",
        "\n",
        "  Note that in this code, the buffers of the first (train_state) and second\n",
        "  (batch) arguments are donated to the computation.\n",
        "\n",
        "  Args:\n",
        "    train_state: The state of training including the current global_step,\n",
        "      model_state, rng, params, and optimizer. The buffer of this argument can\n",
        "      be donated to the computation.\n",
        "    batch: A single batch of data. The buffer of this argument can be donated to\n",
        "      the computation.\n",
        "    flax_model: A Flax model.\n",
        "    loss_fn: A loss function that given logits, a batch, and parameters of the\n",
        "      model calculates the loss.\n",
        "    lr_fns: The learning rate fns used for the optimizer in train_state.\n",
        "    metrics_fn: A metrics function that given logits and batch of data,\n",
        "      calculates the metrics as well as the loss.\n",
        "    config: Configurations of the experiment.\n",
        "    debug: Whether the debug mode is enabled during training. `debug=True`\n",
        "      enables model specific logging/storing some values using\n",
        "      jax.host_callback.\n",
        "\n",
        "  Returns:\n",
        "    Updated state of training and computed metrics and some training logs.\n",
        "  \"\"\"\n",
        "  training_logs = {}\n",
        "  new_rng, rng = jax.random.split(train_state.rng)\n",
        "\n",
        "  # Bind the rng to the host/device we are on.\n",
        "  dropout_rng = train_utils.bind_rng_to_host_device(\n",
        "      rng, axis_name='batch', bind_to='device'\n",
        "  )\n",
        "\n",
        "  # Add prediction targets\n",
        "  batch['targets'] = get_targets(batch, config)\n",
        "\n",
        "  def training_loss_fn(params):\n",
        "    variables = {'params': params, **train_state.model_state}\n",
        "    (logits, aux), new_model_state = flax_model.apply(\n",
        "        variables,\n",
        "        batch['input_signal'],\n",
        "        mutable=['batch_stats'],\n",
        "        train=True,\n",
        "        rngs={'dropout': dropout_rng},\n",
        "        debug=debug,\n",
        "    )\n",
        "    loss = loss_fn(logits, batch, variables['params'], aux['token_mask'])\n",
        "    return loss, (new_model_state, logits, aux['token_mask'])\n",
        "\n",
        "  compute_gradient_fn = jax.value_and_grad(training_loss_fn, has_aux=True)\n",
        "  (train_cost, (new_model_state, logits, masks)), grad = compute_gradient_fn(\n",
        "      train_state.params\n",
        "  )\n",
        "\n",
        "  del train_cost\n",
        "  # Re-use same axis_name as in the call to `pmap(...train_step...)` below.\n",
        "  grad = jax.lax.pmean(grad, axis_name='batch')\n",
        "\n",
        "  updates, new_opt_state = train_state.tx.update(\n",
        "      grad, train_state.opt_state, train_state.params\n",
        "  )\n",
        "  new_params = optax.apply_updates(train_state.params, updates)\n",
        "\n",
        "  training_logs['l2_grads'] = jnp.sqrt(\n",
        "      sum([jnp.vdot(g, g) for g in jax.tree_util.tree_leaves(grad)])\n",
        "  )\n",
        "  ps = jax.tree_util.tree_leaves(new_params)\n",
        "  training_logs['l2_params'] = jnp.sqrt(sum([jnp.vdot(p, p) for p in ps]))\n",
        "  us = jax.tree_util.tree_leaves(updates)\n",
        "  training_logs['l2_updates'] = jnp.sqrt(sum([jnp.vdot(u, u) for u in us]))\n",
        "  for name, lr_fn in lr_fns.items():\n",
        "    lr_name = 'learning_rate' if name == 'all' else f'learning_rate_{name}'\n",
        "    training_logs[lr_name] = lr_fn(train_state.global_step)\n",
        "\n",
        "  metrics = metrics_fn(logits, masks, batch)\n",
        "\n",
        "  new_train_state = train_state.replace(  # pytype: disable=attribute-error\n",
        "      global_step=train_state.global_step + 1,\n",
        "      opt_state=new_opt_state,\n",
        "      params=new_params,\n",
        "      model_state=new_model_state,\n",
        "      rng=new_rng,\n",
        "  )\n",
        "\n",
        "  return new_train_state, metrics, training_logs\n",
        "\n",
        "\n",
        "def eval_step(\n",
        "    train_state: train_utils.TrainState,\n",
        "    batch: Batch,\n",
        "    *,\n",
        "    flax_model: nn.Module,\n",
        "    metrics_fn: MetricFn,\n",
        "    config: ml_collections.ConfigDict,\n",
        "    debug: Optional[bool] = False,\n",
        ") -\u003e Tuple[Dict[str, Tuple[float, int]], jnp.ndarray]:\n",
        "  \"\"\"Runs a single step of training.\n",
        "\n",
        "  Adapted from: google3/third_party/py/scenic/projects/multimask/trainer.py\n",
        "\n",
        "  Note that in this code, the buffer of the second argument (batch) is donated\n",
        "  to the computation.\n",
        "\n",
        "  Assumed API of metrics_fn is:\n",
        "  ```metrics = metrics_fn(logits, batch)\n",
        "  where batch is yielded by the batch iterator, and metrics is a dictionary\n",
        "  mapping metric name to a vector of per example measurements. eval_step will\n",
        "  aggregate (by summing) all per example measurements and divide by the\n",
        "  aggregated normalizers. For each given metric we compute:\n",
        "  1/N sum_{b in batch_iter} metric(b), where  N is the sum of normalizer\n",
        "  over all batches.\n",
        "\n",
        "  Args:\n",
        "    train_state: TrainState, the state of training including the current\n",
        "      global_step, model_state, rng, params and optimizer state. The buffer of\n",
        "      this argument can be donated to the computation.\n",
        "    batch: A single batch of data. a metrics function, that given logits and\n",
        "      batch of data, calculates the metrics as well as the loss.\n",
        "    flax_model: A Flax model.\n",
        "    metrics_fn: A metrics function, that given logits and batch of data,\n",
        "      calculates the metrics as well as the loss.\n",
        "    config: Configurations of the experiment.\n",
        "    debug: Whether the debug mode is enabled during evaluation. `debug=True`\n",
        "      enables model specific logging/storing some values using\n",
        "      jax.host_callback.\n",
        "\n",
        "  Returns:\n",
        "    Calculated metrics and logits.\n",
        "  \"\"\"\n",
        "  # Add prediction targets\n",
        "  batch['targets'] = get_targets(batch, config)\n",
        "\n",
        "  # Always use the same seed, so that eval is as consistent as possible\n",
        "  rng = jax.random.PRNGKey(config.rng_seed)\n",
        "\n",
        "  # Bind the rng to the host/device we are on.\n",
        "  dropout_rng = train_utils.bind_rng_to_host_device(\n",
        "      rng, axis_name='batch', bind_to='device'\n",
        "  )\n",
        "\n",
        "  variables = {'params': train_state.params, **train_state.model_state}\n",
        "  logits, aux = flax_model.apply(\n",
        "      variables,\n",
        "      batch['input_signal'],\n",
        "      train=True,  # so that masking is enabled\n",
        "      mutable=False,\n",
        "      rngs={'dropout': dropout_rng},\n",
        "      debug=debug,\n",
        "  )\n",
        "  metrics = metrics_fn(logits, aux['token_mask'], batch)\n",
        "  return metrics, logits\n",
        "\n",
        "\n",
        "# Overwrite package functions with functions that allow for an input of key name\n",
        "# 'input_signal'\n",
        "trainer.get_targets = get_targets\n",
        "trainer.representation_fn = representation_fn\n",
        "trainer.train_step = train_step\n",
        "trainer.eval_step = eval_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KgB_MTHg5a_h"
      },
      "outputs": [],
      "source": [
        "# @title Single channel ViT MAE (adapted from vit_mae.py)\n",
        "\n",
        "\"\"\"ViT encoder-decoder models for mmultimask.\n",
        "\n",
        "Adapted from google3/third_party/py/scenic/projects/multimask/models/vit_mae.py.\n",
        "The below functions/classes are adapted to allow for single channel input\n",
        "images, where as the original implementation was for 3-channel RGB images.\n",
        "\"\"\"\n",
        "\n",
        "# Mostly copied from ViTMaskedAutoencoder in projects/mfp/vit.py\n",
        "class ViTMAE(nn.Module):\n",
        "  \"\"\"Encoder-decoder Vision Transformer model for masked feature regression.\n",
        "\n",
        "  Copied from google3/third_party/py/scenic/projects/multimask/models/vit_mae.py.\n",
        "\n",
        "  The differences to `ViTMaskedModel` from vit_encoder.py are that:\n",
        "  -- Only non-masked tokens are processed by the encoder\n",
        "  -- The parallel decoder then processes all tokens\n",
        "\n",
        "  Attributes:\n",
        "    num_classes: Number of output classes.\n",
        "    mlp_dim: Dimension of the mlp on top of attention block.\n",
        "    num_layers: Number of layers.\n",
        "    num_heads: Number of self-attention heads.\n",
        "    patches: Configuration of the patches extracted in the stem of the model.\n",
        "    hidden_size: Size of the hidden state of the output of model's stem.\n",
        "    token_mask_probability: Probability of masking out the input tokens (with a\n",
        "      learned mask token) during training.\n",
        "    representation_size: Size of the representation layer in the model's head.\n",
        "      if None, we skip the extra projection + tanh activation at the end.\n",
        "    dropout_rate: Dropout rate.\n",
        "    attention_dropout_rate: Dropout for attention heads.\n",
        "    stochastic_depth: Probability of dropping out a layer during training.\n",
        "    classifier: type of the classifier layer. Options are 'gap', 'gmp', 'gsp',\n",
        "      'token'.\n",
        "    dtype: JAX data type for activations.\n",
        "  \"\"\"\n",
        "\n",
        "  num_classes: int\n",
        "  mlp_dim: int\n",
        "  num_layers: int\n",
        "  num_heads: int\n",
        "  patches: ml_collections.ConfigDict\n",
        "  hidden_size: int\n",
        "  token_mask_probability: str\n",
        "  decoder_config: ml_collections.ConfigDict\n",
        "  representation_size: Optional[int] = None\n",
        "  positional_embedding: str = 'sinusoidal_2d'\n",
        "  positional_embedding_decoder: str = 'sinusoidal_2d'\n",
        "  dropout_rate: float = 0.1\n",
        "  attention_dropout_rate: float = 0.1\n",
        "  stochastic_depth: float = 0.0\n",
        "  classifier: str = 'none'\n",
        "  dtype: jnp.dtype = jnp.float32\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x: jnp.ndarray, *, train: bool, debug: bool = False):\n",
        "    \"\"\"Forward pass of Vision Transformer.\"\"\"\n",
        "\n",
        "    fh, fw = self.patches.size\n",
        "    assert x.shape[1] % fh == 0 and x.shape[2] % fw == 0, (\n",
        "        'Height and width should be divisible by the respective patch sizes,'\n",
        "        f' instead got {x.shape[1:3]} and {(fh, fw)}'\n",
        "    )\n",
        "    # Extracting patches and then embedding is in fact a single convolution.\n",
        "    x = nn.Conv(\n",
        "        self.hidden_size,\n",
        "        (fh, fw),\n",
        "        strides=(fh, fw),\n",
        "        padding='VALID',\n",
        "        name='embedding',\n",
        "    )(x)\n",
        "    batch, height, width, channels = x.shape\n",
        "    x = jnp.reshape(x, [batch, height * width, channels])\n",
        "\n",
        "    # Add positional encodings before removing the masked tokens\n",
        "    x = mm_model_utils.add_positional_embeddings(\n",
        "        x, self.positional_embedding, [batch, height, width, channels]\n",
        "    )\n",
        "\n",
        "    # Remove masked tokens if needed\n",
        "    n_tokens = height * width\n",
        "    if train:\n",
        "      # Generate mask indices.\n",
        "      assert self.token_mask_probability.startswith('constant_'), (\n",
        "          'Only constant token_mask_probability supported in MAE, instad got'\n",
        "          f' {self.token_mask_probability}'\n",
        "      )\n",
        "      token_mask_probability = float(self.token_mask_probability.split('_')[1])\n",
        "      n_masked = int(token_mask_probability * n_tokens)\n",
        "      mask_indices, unmasked_indices, token_mask = (\n",
        "          mm_model_utils.get_mask_indices(\n",
        "              batch, n_tokens, n_masked, self.make_rng('dropout')\n",
        "          )\n",
        "      )\n",
        "\n",
        "      # Process only unmasked tokens with the encoder.\n",
        "      batch_indices = jnp.arange(batch).reshape(batch, 1)\n",
        "      x = x[batch_indices, unmasked_indices]\n",
        "    else:\n",
        "      token_mask = jnp.zeros((batch, n_tokens))\n",
        "\n",
        "    aux = {'token_mask': token_mask}\n",
        "\n",
        "    # If we want to add a class token, add it here.\n",
        "    # Note that in MAE, positional encodings are not added to the CLS token.\n",
        "    if self.classifier == 'token':\n",
        "      cls = self.param('cls', nn.initializers.zeros,\n",
        "                       (1, 1, channels), x.dtype)\n",
        "      cls = jnp.tile(cls, [batch, 1, 1])\n",
        "      x = jnp.concatenate([cls, x], axis=1)\n",
        "\n",
        "    x = vit.Encoder(\n",
        "        mlp_dim=self.mlp_dim,\n",
        "        num_layers=self.num_layers,\n",
        "        num_heads=self.num_heads,\n",
        "        dropout_rate=self.dropout_rate,\n",
        "        attention_dropout_rate=self.attention_dropout_rate,\n",
        "        stochastic_depth=self.stochastic_depth,\n",
        "        dtype=self.dtype,\n",
        "        positional_embedding='none',  # Has already been added.\n",
        "        name='Transformer')(\n",
        "            x, train=train)\n",
        "    aux['pre_logits'] = x\n",
        "\n",
        "    # If not training, skip decoding\n",
        "    if not train:\n",
        "      return x, aux\n",
        "\n",
        "    # Process entire sequence with the decoder.\n",
        "    mask_token = self.param('mask_token',\n",
        "                            nn.initializers.zeros,\n",
        "                            (1, 1, self.decoder_config.hidden_size))\n",
        "\n",
        "    x = nn.Dense(\n",
        "        self.decoder_config.hidden_size,\n",
        "        kernel_init=nn.initializers.xavier_uniform(),\n",
        "        name='decoder_projection')(x)\n",
        "    if self.classifier == 'token':\n",
        "      x = x[:, 1:, :]\n",
        "\n",
        "    # This effectively \"unshuffles\" the tokens. This means that we can simply\n",
        "    # add positional encodings in the decoder without having to worry about\n",
        "    # their ordering.\n",
        "    x_all = jnp.zeros((batch, n_tokens, self.decoder_config.hidden_size))\n",
        "    x_all = x_all.at[batch_indices, unmasked_indices].set(x)\n",
        "    x_all = x_all.at[batch_indices, mask_indices].set(mask_token)\n",
        "    x = x_all\n",
        "    del x_all\n",
        "\n",
        "    # Add positional encodings to the decoder.\n",
        "    x = mm_model_utils.add_positional_embeddings(\n",
        "        x, self.positional_embedding_decoder,\n",
        "        [batch, height, width, self.decoder_config.hidden_size])\n",
        "\n",
        "    # The parallel decoder, which is actually technically an encoder\n",
        "    x = vit.Encoder(\n",
        "        mlp_dim=self.decoder_config.mlp_dim,\n",
        "        num_layers=self.decoder_config.num_layers,\n",
        "        num_heads=self.decoder_config.num_heads,\n",
        "        dropout_rate=self.decoder_config.dropout_rate,\n",
        "        attention_dropout_rate=self.decoder_config.attention_dropout_rate,\n",
        "        stochastic_depth=self.decoder_config.get('stochastic_depth', 0.0),\n",
        "        dtype=self.dtype,\n",
        "        positional_embedding='none',  # Has already been added.\n",
        "        name='Decoder')(x, train=train)\n",
        "\n",
        "    # Predict pixel reconstructions.\n",
        "    if self.representation_size is not None:\n",
        "      x = nn.Dense(self.representation_size, name='pre_logits')(\n",
        "          x)\n",
        "      x = nn.tanh(x)\n",
        "    else:\n",
        "      x = nn_layers.IdentityLayer(name='pre_logits')(x)\n",
        "    aux['pre_logits_decoder'] = x\n",
        "    x = nn.Dense(\n",
        "        self.num_classes,\n",
        "        kernel_init=nn.initializers.zeros,\n",
        "        name='output_projection')(x)\n",
        "\n",
        "    return x, aux\n",
        "\n",
        "\n",
        "# (metric, normalizer, apply_prediction_weights)\n",
        "# Copied from google3/third_party/py/scenic/projects/multimask/models/vit_mae.py.\n",
        "_REGRESSION_METRICS = {\n",
        "    'mean_squared_error_all': (\n",
        "        functools.partial(mm_model_utils.weighted_error, loss_type='squared'),\n",
        "        model_utils.num_examples,\n",
        "        False,\n",
        "    ),\n",
        "    'mean_absolute_error_all': (\n",
        "        functools.partial(mm_model_utils.weighted_error, loss_type='absolute'),\n",
        "        model_utils.num_examples,\n",
        "        False,\n",
        "    ),\n",
        "    'mean_squared_error_masked': (\n",
        "        functools.partial(mm_model_utils.weighted_error, loss_type='squared'),\n",
        "        model_utils.num_examples,\n",
        "        True,\n",
        "    ),\n",
        "    'mean_absolute_error_masked': (\n",
        "        functools.partial(mm_model_utils.weighted_error, loss_type='absolute'),\n",
        "        model_utils.num_examples,\n",
        "        True,\n",
        "    ),\n",
        "}\n",
        "\n",
        "\n",
        "def regression_metrics_function(\n",
        "    predictions: jnp.ndarray,\n",
        "    prediction_masks: jnp.ndarray,\n",
        "    batch: base_model.Batch,\n",
        "    metrics: base_model.MetricNormalizerFnDict,\n",
        "    axis_name: Union[str, Tuple[str, ...]] = 'batch',\n",
        ") -\u003e Dict[str, Tuple[float, int]]:\n",
        "  \"\"\"Calculate metrics for the regression task.\n",
        "\n",
        "  Copied from google3/third_party/py/scenic/projects/multimask/models/vit_mae.py.\n",
        "\n",
        "  Currently we assume each metric_fn has the API:\n",
        "    ```metric_fn(predictions, targets, weights)```\n",
        "  and returns an array of shape [batch,]. We also assume that to compute\n",
        "  the aggregate metric, one should sum across all batches, then divide by the\n",
        "  total samples seen. In this way we currently only support metrics of the 1/N\n",
        "  sum f(inputs, targets). Note, the caller is responsible for dividing by\n",
        "  the normalizer when computing the mean of each metric.\n",
        "\n",
        "  Args:\n",
        "   predictions: Output of model in shape [batch, length, channels].\n",
        "   prediction_masks: Masks used for masked modeling, shape [batch, length]\n",
        "   batch: Batch (dict) with keys 'targets' and optionally 'batch_mask'.\n",
        "   metrics: The regression metrics to evaluate. The key is the name of the\n",
        "     metric, and the value is the metrics function, normalizer, and a bool\n",
        "     indicating whether to apply prediction_masks.\n",
        "   axis_name: List of axes on which we run the pmsum.\n",
        "\n",
        "  Returns:\n",
        "    A dict of metrics, in which keys are metrics name and values are tuples of\n",
        "    (metric, normalizer).\n",
        "  \"\"\"\n",
        "  targets = batch['targets']\n",
        "  batch_weights = batch.get('batch_mask')\n",
        "  weights = jnp.expand_dims(batch_weights, axis=-1) * prediction_masks\n",
        "  evaluated_metrics = {}\n",
        "  for key, val in metrics.items():\n",
        "    curr_weights = weights if val[2] else batch_weights\n",
        "    evaluated_metrics[key] = model_utils.psum_metric_normalizer(\n",
        "        (\n",
        "            val[0](\n",
        "                targets,\n",
        "                predictions,  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
        "                curr_weights,\n",
        "            ),\n",
        "            val[1](\n",
        "                targets,\n",
        "                predictions,  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
        "                batch_weights,\n",
        "            ),\n",
        "        ),\n",
        "        axis_name=axis_name,\n",
        "    )\n",
        "  return evaluated_metrics  # pytype: disable=bad-return-type  # jax-ndarray\n",
        "\n",
        "\n",
        "class ViTMAESingleChannelModel(base_model.BaseModel):\n",
        "  \"\"\"ViT-based masked modeling.\n",
        "\n",
        "  Adapted from google3/third_party/py/scenic/projects/multimask/models/vit_mae.py.\n",
        "  \"\"\"\n",
        "\n",
        "  def build_flax_model(self) -\u003e nn.Module:\n",
        "    model_dtype = getattr(jnp, self.config.get('model_dtype_str', 'float32'))\n",
        "    num_classes = np.prod(tuple(self.config.model.patches.size)) * 1\n",
        "\n",
        "    return ViTMAE(\n",
        "        num_classes=num_classes,\n",
        "        mlp_dim=self.config.model.mlp_dim,\n",
        "        num_layers=self.config.model.num_layers,\n",
        "        num_heads=self.config.model.num_heads,\n",
        "        representation_size=self.config.model.representation_size,\n",
        "        positional_embedding=self.config.model.positional_embedding,\n",
        "        positional_embedding_decoder=self.config.model.positional_embedding_decoder,\n",
        "        patches=self.config.model.patches,\n",
        "        hidden_size=self.config.model.hidden_size,\n",
        "        token_mask_probability=(\n",
        "            self.config.masked_feature_loss.token_mask_probability\n",
        "        ),\n",
        "        classifier='none',\n",
        "        dropout_rate=self.config.model.get('dropout_rate', 0.1),\n",
        "        attention_dropout_rate=self.config.model.get(\n",
        "            'attention_dropout_rate', 0.1\n",
        "        ),\n",
        "        stochastic_depth=self.config.model.get('stochastic_depth', 0.0),\n",
        "        decoder_config=self.config.model.decoder_config,\n",
        "        dtype=model_dtype,\n",
        "    )\n",
        "\n",
        "  def default_flax_model_config(self) -\u003e ml_collections.ConfigDict:\n",
        "    return ml_collections.ConfigDict()\n",
        "\n",
        "  def init_from_train_state(\n",
        "      self,\n",
        "      train_state: Any,\n",
        "      restored_train_state: Any,\n",
        "      restored_model_cfg: ml_collections.ConfigDict,\n",
        "  ) -\u003e Any:\n",
        "    \"\"\"Updates the train_state with data from restored_train_state.\n",
        "\n",
        "    This function is writen to be used for 'fine-tuning' experiments. Here, we\n",
        "    do some surgery to support larger resolutions (longer sequence length) in\n",
        "    the transformer block, with respect to the learned pos-embeddings.\n",
        "\n",
        "    Args:\n",
        "      train_state: A raw TrainState for the model.\n",
        "      restored_train_state: A TrainState that is loaded with parameters/state of\n",
        "        a  pretrained model.\n",
        "      restored_model_cfg: Configuration of the model from which the\n",
        "        restored_train_state come from. Usually used for some asserts.\n",
        "\n",
        "    Returns:\n",
        "      Updated train_state.\n",
        "    \"\"\"\n",
        "    return vit.init_vit_from_train_state(\n",
        "        train_state, restored_train_state, self.config, restored_model_cfg\n",
        "    )\n",
        "\n",
        "  # prediction_masks at the last position to fit the parent class func signature\n",
        "  def loss_function(\n",
        "      self,\n",
        "      predictions: jnp.ndarray,\n",
        "      batch: base_model.Batch,\n",
        "      model_params: Optional[jnp.ndarray] = None,\n",
        "      prediction_masks: Optional[jnp.ndarray] = None,\n",
        "  ) -\u003e float:\n",
        "    \"\"\"Returns the (weighted) mean squared error.\n",
        "\n",
        "    Args:\n",
        "      predictions: Output of model in shape [batch, length, channels].\n",
        "      batch: Batch (dict) with keys 'targets' and optionally 'batch_mask'.\n",
        "      model_params: Parameters of the model, for optionally applying\n",
        "        regularization.\n",
        "      prediction_masks: Masks used for masked modeling, shape [batch, length]\n",
        "\n",
        "    Returns:\n",
        "      The scalar loss, which is the (weighted) absolute error.\n",
        "    \"\"\"\n",
        "    # IIUC, this mask can be provided by the data loader to indicate invalid\n",
        "    # examples, e.g. for incomplete batches during eval\n",
        "    weights = batch['batch_mask']  # shape (batch_size,)\n",
        "\n",
        "    # If requested, compute the loss only on unmasked tokens\n",
        "    if self.config.masked_feature_loss.loss_only_masked_tokens:\n",
        "      weights = jnp.expand_dims(weights, axis=-1) * prediction_masks\n",
        "\n",
        "    targets = batch['targets']\n",
        "\n",
        "    total_loss = mm_model_utils.weighted_error(\n",
        "        predictions,\n",
        "        targets,\n",
        "        weights,\n",
        "        axis=tuple(range(targets.ndim)),  # aggregate over the batch axis too\n",
        "        loss_type=self.config.masked_feature_loss.loss_type,\n",
        "        mean=True,\n",
        "    )\n",
        "    return total_loss  # pytype: disable=bad-return-type  # jax-ndarray\n",
        "\n",
        "  def get_metrics_fn(self, split: Optional[str] = None) -\u003e base_model.MetricFn:\n",
        "    \"\"\"Returns a callable metric function for the model.\n",
        "\n",
        "    By default, we return the same metric for each split.\n",
        "\n",
        "    Args:\n",
        "      split: The split for which we calculate the metrics. It should be one of\n",
        "        the ['train',  'validation', 'test'].\n",
        "    Returns: A metric function with the following API:\n",
        "      ```metrics_fn(predictions, batch)```\n",
        "    \"\"\"\n",
        "\n",
        "    del split  # Same function for all splits.\n",
        "    return functools.partial(\n",
        "        regression_metrics_function, metrics=_REGRESSION_METRICS\n",
        "    )\n",
        "\n",
        "\n",
        "vit_mae.ViTMAESingleChannelModel = ViTMAESingleChannelModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "q5q29AOc-28p"
      },
      "outputs": [],
      "source": [
        "# @title Model Class and Trainer Selector Functions\n",
        "\n",
        "\"\"\" Adapted from google3/third_party/py/scenic/projects/multimask/main.py\n",
        "\"\"\"\n",
        "\n",
        "def get_model_cls(model_name: str):\n",
        "  \"\"\"Get the model class for the Multimask project.\"\"\"\n",
        "  if model_name == 'vit_masked_encoder':\n",
        "    return vit_encoder.VitMaskedEncoderModel\n",
        "  elif model_name == 'vit_mae':\n",
        "    return vit_mae.ViTMAEModel\n",
        "  elif model_name == 'vit_mae_single_channel':\n",
        "    print('Model: ViTMAESingleChannelModel')\n",
        "    return vit_mae.ViTMAESingleChannelModel\n",
        "  elif model_name == 'transformer_masked_encoder':\n",
        "    return transformer_encoder.TransformerMaskedEncoderModel\n",
        "  else:\n",
        "    raise ValueError(f'Unrecognized model: {model_name}.')\n",
        "\n",
        "def get_train_fn(trainer_name):\n",
        "  if trainer_name == 'multimask_trainer':\n",
        "    return trainer.train\n",
        "  else:\n",
        "    raise ValueError(f'Unrecognized trainer: {trainer_name}.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bNoPSBRW69ph"
      },
      "outputs": [],
      "source": [
        "# @title Electrode Model Dataloader\n",
        "\n",
        "\"\"\" Adapted from a combination of the following files:\n",
        "google3/third_party/py/scenic/dataset_lib/cifar10_dataset.py\n",
        "google3/third_party/py/scenic/dataset_lib/dataset_utils.py\n",
        "\"\"\"\n",
        "\n",
        "def preprocess_example(example, patch_size=None, dtype=tf.float32):\n",
        "  \"\"\"Preprocesses the given example.\n",
        "\n",
        "  Adapted from google3/third_party/py/scenic/dataset_lib/cifar10_dataset.py\n",
        "\n",
        "  Args:\n",
        "    example: dict; Example that has an 'image' and a 'label'.\n",
        "    dtype: Tensorflow data type; Data type of the image.\n",
        "\n",
        "  Returns:\n",
        "    A preprocessed example.\n",
        "\n",
        "  NOTE: This assumes that the image is in the shape [H, W, C],\n",
        "    where H is the Time axis, and W is the feature axis.\n",
        "  \"\"\"\n",
        "  image = tf.cast(example['input_signal'], dtype=dtype)\n",
        "  H, W, C = image.shape\n",
        "\n",
        "  # Crop and pad image to allow for square patches\n",
        "  if patch_size is not None:\n",
        "    patch_h, patch_w = patch_size\n",
        "\n",
        "    # Crop H to make even patches\n",
        "    num_patches_h = H // patch_h\n",
        "    crop_h = H - int(num_patches_h * patch_h)\n",
        "    image = image[crop_h:, :, :]\n",
        "\n",
        "    # Pad W to make even patches\n",
        "    remainder_w = W % patch_w\n",
        "    if remainder_w != 0:\n",
        "      pad_total = patch_w - remainder_w\n",
        "      pad1 = pad_total // 2\n",
        "      pad2 = pad_total - pad1\n",
        "      paddings = [[0, 0], [pad1, pad2], [0, 0]]\n",
        "      image = tf.pad(image, paddings, mode='CONSTANT', constant_values=0)\n",
        "\n",
        "  # Return preprocessed feature\n",
        "  return {'input_signal': image}\n",
        "\n",
        "\n",
        "# TODO(girishvn): define augmentation function\n",
        "# def augment_example(example, dtype=tf.float32, data_augmentations=None):\n",
        "#   \"\"\"Apply data augmentation on the given training example.\n",
        "\n",
        "\n",
        "def get_electrodes_dataset(\n",
        "    *,\n",
        "    config,\n",
        "    num_shards,\n",
        "    batch_size,\n",
        "    eval_batch_size=None,\n",
        "    dtype_str='float32',\n",
        "    shuffle_seed=0,\n",
        "    rng=None,\n",
        "    shuffle_buffer_size=None,\n",
        "    dataset_configs=None,\n",
        "    dataset_service_address: Optional[str] = None,\n",
        "    cache = False,\n",
        "    dataset_name='lsm_prod/lsm_300min_100K_unimpute',\n",
        "    data_dir='/namespace/fitbit-medical-sandboxes/partner/encrypted/chr-ards-electrodes/deid/raw/datasets/msa_1_5/lsm_tfds_datasets'\n",
        "):\n",
        "  \"\"\" Gets and formats the Electrodes dataset.\n",
        "\n",
        "  Adapted from:\n",
        "  google3/third_party/py/scenic/dataset_lib/cifar10_dataset.py and\n",
        "  google3/third_party/py/scenic/dataset_lib/dataset_utils.py.\n",
        "  \"\"\"\n",
        "\n",
        "  del rng\n",
        "  dtype = getattr(tf, dtype_str)  # data dtype\n",
        "  p_idx = jax.process_index()  # current process index\n",
        "  p_cnt = jax.process_count()  # process count (number of processes)\n",
        "  if eval_batch_size is None: eval_batch_size = batch_size  # set eval batch\n",
        "\n",
        "  # Setup split preprocessing functions.\n",
        "  train_preprocess_fn = functools.partial(\n",
        "      preprocess_example, patch_size=config.model.patches.size, dtype=dtype)\n",
        "  eval_preprocess_fn = functools.partial(\n",
        "      preprocess_example, patch_size=config.model.patches.size, dtype=dtype)\n",
        "\n",
        "  # Setup augmentation functions.\n",
        "  # TODO(girishvn): Set up data augmentations here.\n",
        "\n",
        "  # Create dataset splits (even splits per worker).\n",
        "  train_split_range = tfds.even_splits(split='train', n=p_cnt)[p_idx]\n",
        "  eval_split_range = tfds.even_splits(split='test', n=p_cnt)[p_idx]\n",
        "\n",
        "  # Load tf dataset.\n",
        "  train_ds = tfds.load(\n",
        "      dataset_name, data_dir=data_dir, split=train_split_range,\n",
        "      shuffle_files=False\n",
        "  )\n",
        "  eval_ds = tfds.load(\n",
        "      dataset_name, data_dir=data_dir, split=eval_split_range,\n",
        "      shuffle_files=False\n",
        "  )\n",
        "  logging.info(\n",
        "      f'Loaded train and eval split {p_idx}/{p_cnt} from {dataset_name}.'\n",
        "  )\n",
        "\n",
        "  # Enable multi threaded workers.\n",
        "  train_options = tf.data.Options()\n",
        "  train_options.threading.private_threadpool_size = 48\n",
        "  train_ds = train_ds.with_options(train_options)\n",
        "\n",
        "  eval_options = tf.data.Options()\n",
        "  eval_options.threading.private_threadpool_size = 48\n",
        "  eval_ds = eval_ds.with_options(eval_options)\n",
        "\n",
        "  # Applying preprocessing before `ds.cache()` to re-use it.\n",
        "  train_ds = train_ds.map(\n",
        "      train_preprocess_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "  eval_ds = train_ds.map(\n",
        "      eval_preprocess_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "\n",
        "  # Cache datasets.\n",
        "  if cache:\n",
        "    train_ds = train_ds.cache()\n",
        "    eval_ds = eval_ds.cache()\n",
        "\n",
        "  train_ds = train_ds.repeat()  # repeat\n",
        "  # TODO(girishvn): add augmentations\n",
        "  shuffle_buffer_size = shuffle_buffer_size  or (8 * batch_size)\n",
        "  train_ds = train_ds.shuffle(shuffle_buffer_size, seed=shuffle_seed)  # shuffle\n",
        "  train_ds = train_ds.batch(batch_size, drop_remainder=True)  # batch\n",
        "  train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)  # prefetch\n",
        "\n",
        "  eval_ds = eval_ds.batch(batch_size, drop_remainder=True)  # batch\n",
        "  eval_ds = eval_ds.repeat()  # repeat\n",
        "  eval_ds = eval_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  if dataset_service_address:\n",
        "    if shuffle_seed is not None:\n",
        "      raise ValueError('Using dataset service with a random seed causes each '\n",
        "                       'worker to produce exactly the same data. Add '\n",
        "                       'config.shuffle_seed = None to your config if you '\n",
        "                       'want to run with dataset service.')\n",
        "    train_ds = dataset_utils.distribute(train_ds, dataset_service_address)\n",
        "    logging.info('Using the tf.data service at %s', dataset_service_address)\n",
        "\n",
        "  # Other mappings\n",
        "  maybe_pad_batches_train = functools.partial(\n",
        "      dataset_utils.maybe_pad_batch, train=True,\n",
        "      batch_size=batch_size, inputs_key='input_signal')\n",
        "  maybe_pad_batches_eval = functools.partial(\n",
        "      dataset_utils.maybe_pad_batch, train=False,\n",
        "      batch_size=eval_batch_size, inputs_key='input_signal')\n",
        "  shard_batches = functools.partial(dataset_utils.shard, n_devices=num_shards)\n",
        "\n",
        "  # Iter dataset\n",
        "  train_iter = iter(train_ds)\n",
        "  train_iter = map(dataset_utils.tf_to_numpy, train_iter)\n",
        "  train_iter = map(maybe_pad_batches_train, train_iter)\n",
        "  train_iter = map(shard_batches, train_iter)\n",
        "\n",
        "  eval_iter = iter(eval_ds)\n",
        "  eval_iter = map(dataset_utils.tf_to_numpy, eval_iter)\n",
        "  eval_iter = map(maybe_pad_batches_eval, eval_iter)\n",
        "  eval_iter = map(shard_batches, eval_iter)\n",
        "\n",
        "  # Save meta data\n",
        "  info = tfds.builder(dataset_name, data_dir=data_dir, try_gcs=True).info\n",
        "  input_shape = tuple([-1] + list(info.features['input_signal'].shape))\n",
        "  meta_data = {\n",
        "      'input_shape': input_shape,\n",
        "      'num_train_examples': dataset_utils.get_num_examples(\n",
        "          dataset=dataset_name, split='train', data_dir=data_dir),\n",
        "      'num_eval_examples': dataset_utils.get_num_examples(\n",
        "          dataset=dataset_name, split='test', data_dir=data_dir),\n",
        "      'input_dtype': getattr(jnp, dtype_str),\n",
        "      'target_is_onehot': False,\n",
        "      'num_classes': None,\n",
        "  }\n",
        "\n",
        "  # Return dataset structure.\n",
        "  return dataset_utils.Dataset(train_iter, eval_iter, None, meta_data)\n",
        "\n",
        "\n",
        "def get_dataset(\n",
        "    config: ml_collections.ConfigDict,\n",
        "    data_rng: jnp.ndarray,\n",
        "    *,\n",
        "    num_local_shards: Optional[int] = None,\n",
        "    dataset_service_address: Optional[str] = None,\n",
        "    dataset_name: Optional[str] = None,\n",
        "    dataset_configs: Optional[ml_collections.ConfigDict] = None,\n",
        "    **kwargs: Any,\n",
        ") -\u003e dataset_utils.Dataset:\n",
        "\n",
        "  \"\"\" Adapted from: google3/third_party/py/scenic/train_lib/train_utils.py\n",
        "  \"\"\"\n",
        "\n",
        "  # Get device count\n",
        "  device_count = jax.device_count()\n",
        "  logging.info('device_count: %d', device_count)\n",
        "  logging.info('num_hosts : %d', jax.process_count())\n",
        "  logging.info('host_id : %d', jax.process_index())\n",
        "\n",
        "  # Set the dataset builder functions\n",
        "  dataset_name = dataset_name or config.dataset_name\n",
        "  if dataset_name == 'lsm_prod/lsm_300min_100K_unimpute':\n",
        "    dataset_builder = get_electrodes_dataset\n",
        "  else:\n",
        "    raise ValueError(f'Dataset {dataset_name} is not supported.')\n",
        "\n",
        "  # Get batch size\n",
        "  batch_size = config.batch_size\n",
        "  if batch_size % device_count \u003e 0:\n",
        "    raise ValueError(\n",
        "        f'Batch size ({batch_size}) must be divisible by the '\n",
        "        f'number of devices ({device_count})'\n",
        "    )\n",
        "\n",
        "  local_batch_size = batch_size // jax.process_count()\n",
        "  device_batch_size = batch_size // device_count\n",
        "  logging.info('local_batch_size : %d', local_batch_size)\n",
        "  logging.info('device_batch_size : %d', device_batch_size)\n",
        "\n",
        "  # Get shuffle seed - ensure it exists\n",
        "  shuffle_seed = config.get('shuffle_seed', None)\n",
        "  if dataset_service_address and shuffle_seed is not None:\n",
        "    raise ValueError(\n",
        "        'Using dataset service with a random seed causes each '\n",
        "        'worker to produce exactly the same data. Add '\n",
        "        'config.shuffle_seed = None to your config if you want '\n",
        "        'to run with dataset service.'\n",
        "    )\n",
        "\n",
        "  dataset_configs = dataset_configs or config.get('dataset_configs', {})\n",
        "  num_local_shards = num_local_shards or jax.local_device_count()\n",
        "\n",
        "  # Build the dataset\n",
        "  ds = dataset_builder(\n",
        "      config=config,\n",
        "      num_shards=num_local_shards,\n",
        "      batch_size=local_batch_size,\n",
        "      dtype_str=config.data_dtype_str,\n",
        "      shuffle_seed=shuffle_seed,\n",
        "      rng=data_rng,\n",
        "      dataset_configs=dataset_configs,\n",
        "      dataset_service_address=dataset_service_address,\n",
        "      **kwargs,\n",
        "  )\n",
        "\n",
        "  return ds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6aHZ0bdgVn-"
      },
      "source": [
        "## Training and eval pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "q2RPnTd7FR9x"
      },
      "outputs": [],
      "source": [
        "#@title Config\n",
        "\n",
        "NUM_TRAIN_STEPS = 5000\n",
        "\n",
        "# Model variant / patch H (time steps) / patch W (features)\n",
        "VARIANT = 'TiShallow/10/5'\n",
        "\n",
        "LRS = [1e-3]\n",
        "TOKEN_MASK_PROB = 'constant_0.8'\n",
        "\n",
        "HIDDEN_SIZES = {\n",
        "    'Deb': 16,\n",
        "    'Ti': 192,\n",
        "    'TiShallow': 192,\n",
        "    'S': 384,\n",
        "    'SShallow': 384,\n",
        "    'M': 512,\n",
        "    'B': 768,\n",
        "    'L': 1024,\n",
        "    'H': 1280,\n",
        "    'g': 1408,\n",
        "    'G': 1664,\n",
        "    'e': 1792,\n",
        "}\n",
        "MLP_DIMS = {\n",
        "    'Deb': 32,\n",
        "    'Ti': 768,\n",
        "    'TiShallow': 768,\n",
        "    'S': 1536,\n",
        "    'SShallow': 1536,\n",
        "    'M': 2048,\n",
        "    'B': 3072,\n",
        "    'L': 4096,\n",
        "    'H': 5120,\n",
        "    'g': 6144,\n",
        "    'G': 8192,\n",
        "    'e': 15360,\n",
        "}\n",
        "NUM_HEADS = {\n",
        "    'Deb': 2,\n",
        "    'Ti': 3,\n",
        "    'TiShallow': 3,\n",
        "    'S': 6,\n",
        "    'SShallow': 6,\n",
        "    'M': 8,\n",
        "    'B': 12,\n",
        "    'L': 16,\n",
        "    'H': 16,\n",
        "    'g': 16,\n",
        "    'G': 16,\n",
        "    'e': 16,\n",
        "}\n",
        "NUM_LAYERS = {\n",
        "    'Deb': 2,\n",
        "    'Ti': 12,\n",
        "    'TiShallow': 4,\n",
        "    'S': 12,\n",
        "    'SShallow': 4,\n",
        "    'M': 12,\n",
        "    'B': 12,\n",
        "    'L': 24,\n",
        "    'H': 32,\n",
        "    'g': 40,\n",
        "    'G': 48,\n",
        "    'e': 56,\n",
        "}\n",
        "\n",
        "\n",
        "DECODER_HIDDEN_SIZES = {\n",
        "    'Deb': 16,\n",
        "    'Ti': 128,\n",
        "    'TiShallow': 128,\n",
        "    'S': 256,\n",
        "    'B': 512,\n",
        "    'L': 512,\n",
        "    'H': 512\n",
        "}\n",
        "DECODER_MLP_DIMS = {\n",
        "    'Deb': 32,\n",
        "    'Ti': 512,\n",
        "    'TiShallow': 512,\n",
        "    'S': 1024,\n",
        "    'B': 2048,\n",
        "    'L': 2048,\n",
        "    'H': 2048\n",
        "}\n",
        "DECODER_NUM_LAYERS = {\n",
        "    'Deb': 2,\n",
        "    'Ti': 2,\n",
        "    'TiShallow': 2,\n",
        "    'S': 4,\n",
        "    'B': 8,\n",
        "    'L': 8,\n",
        "    'H': 8\n",
        "}\n",
        "DECODER_NUM_HEADS = {\n",
        "    'Deb': 2,\n",
        "    'Ti': 4,\n",
        "    'TiShallow': 4,\n",
        "    'S': 8,\n",
        "    'B': 16,\n",
        "    'L': 16,\n",
        "    'H': 16\n",
        "}\n",
        "\n",
        "\n",
        "def get_config(runlocal=''):\n",
        "  \"\"\"Returns the ViT experiment configuration.\"\"\"\n",
        "\n",
        "  runlocal = bool(runlocal)\n",
        "\n",
        "  config = ml_collections.ConfigDict()\n",
        "  config.experiment_name = 'electrodes-mae-vit-tiny'\n",
        "  # Dataset.\n",
        "  config.dataset_name = 'lsm_prod/lsm_300min_100K_unimpute'\n",
        "  config.data_dtype_str = 'float32'\n",
        "  config.dataset_configs = ml_collections.ConfigDict()\n",
        "  config.dataset_configs.dataset = 'lsm_prod/lsm_300min_100K_unimpute'\n",
        "  # config.dataset_configs.num_classes = NUM_CLASSES\n",
        "  config.dataset_configs.train_split = 'train'\n",
        "  config.dataset_configs.val_split = 'test'\n",
        "\n",
        "  # NOTE: Can inject augmentations / preprocessing of dataset here\n",
        "  # using config.dataset_configs.pp_train and config.dataset_configs.pp_eval.\n",
        "  # Refer to scenic.dataset_lib.big_transfer.bit for how this is done on\n",
        "  # Cifar10.\n",
        "\n",
        "  config.dataset_configs.prefetch_to_device = 2\n",
        "  # Shuffle_buffer_size is per host, so small-ish is ok.\n",
        "  config.dataset_configs.shuffle_buffer_size = 250_000\n",
        "\n",
        "  # Model.\n",
        "  if len(VARIANT.split('/')) == 3:\n",
        "    version = VARIANT.split('/')[0]  # model variant\n",
        "    patch_h = VARIANT.split('/')[1]  # patch width\n",
        "    patch_w = VARIANT.split('/')[2]  # patch height\n",
        "  elif len(VARIANT.split('/')) == 2:\n",
        "    version = VARIANT.split('/')[0]  # model variant\n",
        "    patch_h = VARIANT.split('/')[1]  # patch width\n",
        "    patch_w = VARIANT.split('/')[1]  # patch height\n",
        "  else:\n",
        "    raise ValueError(f'Invalid model variant: {VARIANT}')\n",
        "\n",
        "  version = 'Deb' if runlocal else version\n",
        "  config.model_name = 'vit_mae'\n",
        "  config.model = ml_collections.ConfigDict()\n",
        "  # encoder\n",
        "  config.model.hidden_size = HIDDEN_SIZES[version]\n",
        "  config.model.patches = ml_collections.ConfigDict()\n",
        "  config.model.patches.size = [int(patch_h), int(patch_w)]\n",
        "  config.model.num_heads = NUM_HEADS[version]\n",
        "  config.model.mlp_dim = MLP_DIMS[version]\n",
        "  config.model.num_layers = NUM_LAYERS[version]\n",
        "  config.model.dropout_rate = 0.\n",
        "  config.model.classifier = 'none'  # Has to be \"none\" for the autoencoder\n",
        "  config.model.representation_size = None\n",
        "  config.model.positional_embedding = 'sinusoidal_2d'\n",
        "  config.model.positional_embedding_decoder = 'sinusoidal_2d'\n",
        "  # decoder\n",
        "  config.model.decoder_config = ml_collections.ConfigDict()\n",
        "  config.model.decoder_config.hidden_size = DECODER_HIDDEN_SIZES[version]\n",
        "  config.model.decoder_config.mlp_dim = DECODER_MLP_DIMS[version]\n",
        "  config.model.decoder_config.num_layers = DECODER_NUM_LAYERS[version]\n",
        "  config.model.decoder_config.num_heads = DECODER_NUM_HEADS[version]\n",
        "  config.model.decoder_config.dropout_rate = 0.\n",
        "  config.model.decoder_config.attention_dropout_rate = 0.\n",
        "\n",
        "  config.masked_feature_loss = ml_collections.ConfigDict()\n",
        "  config.masked_feature_loss.targets_type = 'rgb'\n",
        "  config.masked_feature_loss.token_mask_probability = TOKEN_MASK_PROB\n",
        "  config.masked_feature_loss.loss_only_masked_tokens = True\n",
        "  config.masked_feature_loss.loss_type = 'squared'  # 'squared' or 'absolute'\n",
        "\n",
        "  # Training.\n",
        "  config.trainer_name = 'multimask_trainer'\n",
        "  config.batch_size = 8 if runlocal else 1024\n",
        "  config.num_training_steps = NUM_TRAIN_STEPS\n",
        "  config.log_eval_steps = 100\n",
        "  config.log_summary_steps = 100\n",
        "  config.rng_seed = 42\n",
        "  sched = ml_collections.ConfigDict()\n",
        "  sched.re = '(.*)'\n",
        "  sched.lr_configs = ml_collections.ConfigDict()\n",
        "  sched.lr_configs.learning_rate_schedule = 'compound'\n",
        "  sched.lr_configs.factors = 'constant * cosine_decay * linear_warmup'\n",
        "  sched.lr_configs.total_steps = NUM_TRAIN_STEPS\n",
        "  sched.lr_configs.steps_per_cycle = sched.lr_configs.total_steps\n",
        "  sched.lr_configs.warmup_steps = 2000\n",
        "  sched.lr_configs.base_learning_rate = LRS[0]\n",
        "  config.schedule = ml_collections.ConfigDict({'all': sched})\n",
        "\n",
        "  # *Single* optimizer.\n",
        "  optim = ml_collections.ConfigDict()\n",
        "  optim.optax_name = 'scale_by_adam'\n",
        "  # optim.optax = dict(mu_dtype='bfloat16')\n",
        "  optim.optax_configs = ml_collections.ConfigDict(\n",
        "      {  # Optimizer settings.\n",
        "          'b1': 0.9,\n",
        "          'b2': 0.999,\n",
        "      })\n",
        "  config.optax = dict(mu_dtype='bfloat16')\n",
        "  optim.max_grad_norm = 1.0\n",
        "\n",
        "  optim.weight_decay = 1e-4\n",
        "  optim.weight_decay_decouple = True\n",
        "  config.optimizer = optim\n",
        "\n",
        "  # Fewshot.\n",
        "  # TODO(girishvn): This needs to be adapted to electrode dataset\n",
        "  config.fewshot = common_fewshot.get_config(\n",
        "      batch_size=config.batch_size\n",
        "  )\n",
        "  config.fewshot.datasets = {}\n",
        "  config.fewshot.walk_first = ()\n",
        "  config.fewshot.representation_layer = 'pre_logits'\n",
        "  config.fewshot.log_eval_steps = 1000\n",
        "\n",
        "  # Logging.\n",
        "  config.write_summary = True\n",
        "  config.xprof = True  # Profile using xprof.\n",
        "  config.checkpoint = True  # Do checkpointing.\n",
        "  config.checkpoint_steps = 1000\n",
        "  config.debug_train = False  # Debug mode during training.\n",
        "  config.debug_eval = False  # Debug mode during eval.\n",
        "\n",
        "  # BEGIN GOOGLE-INTERNAL\n",
        "  if runlocal:\n",
        "    # Current implementation fails with UPTC.\n",
        "    config.count_flops = False\n",
        "  # END GOOGLE-INTERNAL\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "# BEGIN GOOGLE-INTERNAL\n",
        "def get_hyper(hyper):\n",
        "  \"\"\"Defines the hyper-parameters sweeps for doing grid search.\"\"\"\n",
        "  return hyper.product([\n",
        "      hyper.sweep('config.schedule.all.lr_configs.base_learning_rate', LRS),\n",
        "  ])\n",
        "config = get_config(True)\n",
        "config.trainer_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LP39TogyGGEl"
      },
      "outputs": [],
      "source": [
        "#@title Initialize training states\n",
        "\n",
        "# Adapted from google3/third_party/py/scenic/projects/multimask/trainer.py.\n",
        "\n",
        "model_cls = get_model_cls('vit_mae_single_channel')\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = get_dataset(\n",
        "    config, data_rng\n",
        ")\n",
        "\n",
        "lead_host = jax.process_index() == 0\n",
        "# Build the loss_fn, metrics, and flax_model.\n",
        "model = model_cls(config, dataset.meta_data)\n",
        "\n",
        "# Initialize model.\n",
        "rng, params_init_rng, dropout_init_rng = jax.random.split(rng, num=3)\n",
        "init_rngs = {'params': params_init_rng, 'dropout': dropout_init_rng}\n",
        "init_batch = next(dataset.train_iter)\n",
        "(params, model_state, num_trainable_params, gflops) = (\n",
        "    train_utils.initialize_model(\n",
        "        model_def=model.flax_model,\n",
        "        input_spec=[\n",
        "            (init_batch['input_signal'].shape[1:],\n",
        "             init_batch['input_signal'].dtype)\n",
        "        ],\n",
        "        config=config,\n",
        "        rngs=init_rngs,\n",
        "        train=True,  # so that masking and decoding in MAE are initialized\n",
        "    )\n",
        ")\n",
        "\n",
        "# Create LR schedules and optimizer.\n",
        "schedule_fns = scenic_optax.make_schedule(config.get('schedule'))\n",
        "tx, _ = scenic_optax.make(config.optimizer, schedule_fns, params)\n",
        "opt_state = tx.init(params)\n",
        "\n",
        "rng, train_rng = jax.random.split(rng)\n",
        "\n",
        "# Create chrono class to track and store training statistics and metadata:\n",
        "chrono = train_utils.Chrono()\n",
        "\n",
        "train_state = train_utils.TrainState(\n",
        "    global_step=0,\n",
        "    opt_state=opt_state,\n",
        "    tx=tx,\n",
        "    params=params,\n",
        "    model_state=model_state,\n",
        "    rng=train_rng,\n",
        "    metadata={'chrono': chrono.save()},\n",
        ")\n",
        "start_step = train_state.global_step\n",
        "chrono.load(train_state.metadata['chrono'])\n",
        "train_state = train_state.replace(metadata={})\n",
        "\n",
        "# Replicate the optimzier, state, and rng.\n",
        "train_state = jax_utils.replicate(train_state)\n",
        "del params  # Do not keep a copy of the initial params.\n",
        "\n",
        "# Calculate the total number of training steps.\n",
        "# TODO(adosovitskiy): get rid of epochs?\n",
        "total_steps, steps_per_epoch = train_utils.get_num_training_steps(\n",
        "    config, dataset.meta_data\n",
        ")\n",
        "\n",
        "train_step_pmapped = jax.pmap(\n",
        "    functools.partial(\n",
        "        trainer.train_step,\n",
        "        flax_model=model.flax_model,\n",
        "        loss_fn=model.loss_function,\n",
        "        lr_fns={name: lr_fn for _, name, (lr_fn, _) in schedule_fns},\n",
        "        metrics_fn=model.get_metrics_fn('train'),\n",
        "        config=config,\n",
        "        debug=config.debug_train,\n",
        "    ),\n",
        "    axis_name='batch',\n",
        "    # We can donate both buffers of train_state and train_batch.\n",
        "    donate_argnums=(0, 1),\n",
        ")\n",
        "eval_step_pmapped = jax.pmap(\n",
        "    functools.partial(\n",
        "        trainer.eval_step,\n",
        "        flax_model=model.flax_model,\n",
        "        metrics_fn=model.get_metrics_fn('validation'),\n",
        "        config=config,\n",
        "        debug=config.debug_eval,\n",
        "    ),\n",
        "    axis_name='batch',\n",
        "    # We can donate the eval_batch's buffer.\n",
        "    donate_argnums=(1,),\n",
        ")\n",
        "\n",
        "if 'fewshot' in config:\n",
        "  representation_fn_partial = functools.partial(\n",
        "      trainer.representation_fn,\n",
        "      flax_model=model.flax_model,\n",
        "      representation_layer=config.fewshot.representation_layer,\n",
        "  )\n",
        "\n",
        "  fewshotter = fewshot_utils.FewShotEvaluator(\n",
        "      representation_fn_partial, config.fewshot\n",
        "  )\n",
        "\n",
        "log_eval_steps = config.get('log_eval_steps')\n",
        "if not log_eval_steps:\n",
        "  raise ValueError(\"'log_eval_steps' should be specified in the config.\")\n",
        "checkpoint_steps = config.get('checkpoint_steps') or log_eval_steps\n",
        "log_summary_steps = config.get('log_summary_steps') or log_eval_steps\n",
        "\n",
        "train_metrics, extra_training_logs = [], []\n",
        "train_summary, eval_summary = None, None\n",
        "\n",
        "chrono.inform(start_step, total_steps, config.batch_size, steps_per_epoch)\n",
        "logging.info('Starting training loop at step %d.', start_step + 1)\n",
        "\n",
        "\n",
        "def write_note(note):\n",
        "  if lead_host:\n",
        "    platform.work_unit().set_notes(note)\n",
        "\n",
        "\n",
        "hooks = []\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    train_state: train_utils.TrainState,\n",
        "    step: int,\n",
        "    valid_iter: Iterator[Batch],\n",
        "    num_valid_ex: int,\n",
        ") -\u003e Dict[str, Any]:\n",
        "  eval_summary = {}\n",
        "  if not isinstance(valid_iter, dict):  # Only on validation set.\n",
        "    valid_iter, num_valid_ex = {'valid': valid_iter}, {'valid': num_valid_ex}\n",
        "\n",
        "  for val_name, val_iter in valid_iter.items():\n",
        "    num_ex = num_valid_ex[val_name]\n",
        "    # Ceil rounding such that we include the last incomplete batch.\n",
        "    eval_batch_size = config.get('eval_batch_size', config.batch_size)\n",
        "    total_eval_steps = int(np.ceil(num_ex / eval_batch_size))\n",
        "    steps_per_eval = config.get('steps_per_eval') or total_eval_steps\n",
        "    eval_metrics = []\n",
        "    for _ in range(steps_per_eval):\n",
        "      eval_batch = next(val_iter)\n",
        "      e_metrics, _ = eval_step_pmapped(train_state, eval_batch)\n",
        "      eval_metrics.append(train_utils.unreplicate_and_get(e_metrics))\n",
        "      eval_summary[val_name] = eval_metrics\n",
        "  return eval_summary\n",
        "\n",
        "\n",
        "def process_valid_summary(eval_summary):\n",
        "  mae_all = []\n",
        "  mae_masked_all = []\n",
        "  mse_all = []\n",
        "  mse_masked_all = []\n",
        "  for batch_eval in eval_summary['valid']:\n",
        "    mae = float(batch_eval['mean_absolute_error_all'][0])\n",
        "    mae_masked = float(batch_eval['mean_absolute_error_masked'][0])\n",
        "    mse = float(batch_eval['mean_squared_error_all'][0])\n",
        "    mse_masked = float(batch_eval['mean_squared_error_masked'][0])\n",
        "    mae_all.append(mae)\n",
        "    mae_masked_all.append(mae_masked)\n",
        "    mse_all.append(mse)\n",
        "    mse_masked_all.append(mse_masked)\n",
        "  return (\n",
        "      sum(mae_all) / len(mae_all),\n",
        "      sum(mae_masked_all) / len(mae_masked_all),\n",
        "      sum(mse_all) / len(mse_all),\n",
        "      sum(mse_masked_all) / len(mse_masked_all),\n",
        "  )\n",
        "\n",
        "\n",
        "def smooth_data(data, smoothing_factor=0.9):\n",
        "  smoothed_data = []\n",
        "  for i, value in enumerate(data):\n",
        "    if i == 0:\n",
        "      smoothed_data.append(value)\n",
        "    else:\n",
        "      smoothed_value = (\n",
        "          smoothing_factor * smoothed_data[-1] + (1 - smoothing_factor) * value\n",
        "      )\n",
        "      smoothed_data.append(smoothed_value)\n",
        "  return smoothed_data\n",
        "\n",
        "\n",
        "def plot_steps(data, mode='train', smoothing_factor=None):\n",
        "  # Create a DataFrame from the dictionary\n",
        "  df = pd.DataFrame.from_dict(\n",
        "      data,\n",
        "      orient='index',\n",
        "      columns=['mae_all', 'mae_masked_all', 'mse_all', 'mse_masked_all'],\n",
        "  )\n",
        "  df.reset_index(inplace=True)\n",
        "  df.rename(columns={'index': 'steps'}, inplace=True)\n",
        "\n",
        "  # Apply smoothing if specified\n",
        "  if smoothing_factor is not None:\n",
        "    df['mae_all'] = smooth_data(df['mae_all'].tolist(), smoothing_factor)\n",
        "    df['mae_masked_all'] = smooth_data(\n",
        "        df['mae_masked_all'].tolist(), smoothing_factor\n",
        "    )\n",
        "    df['mse_all'] = smooth_data(df['mse_all'].tolist(), smoothing_factor)\n",
        "    df['mse_masked_all'] = smooth_data(\n",
        "        df['mse_masked_all'].tolist(), smoothing_factor\n",
        "    )\n",
        "\n",
        "  # Create the plot\n",
        "  fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "  plot_titles = [\n",
        "      f'MAE All ({mode})',\n",
        "      f'MAE Masked All ({mode})',\n",
        "      f'MSE All ({mode})',\n",
        "      f'MSE Masked All ({mode})',\n",
        "  ]\n",
        "\n",
        "  sns.lineplot(ax=axes[0], x='steps', y='mae_all', data=df)\n",
        "  axes[0].set_title(plot_titles[0])\n",
        "  axes[0].set_xlabel('Steps')\n",
        "  axes[0].set_ylabel('MAE All')\n",
        "\n",
        "  sns.lineplot(ax=axes[1], x='steps', y='mae_masked_all', data=df)\n",
        "  axes[1].set_title(plot_titles[1])\n",
        "  axes[1].set_xlabel('Steps')\n",
        "  axes[1].set_ylabel('MAE Masked All')\n",
        "\n",
        "  sns.lineplot(ax=axes[2], x='steps', y='mse_all', data=df)\n",
        "  axes[2].set_title(plot_titles[2])\n",
        "  axes[2].set_xlabel('Steps')\n",
        "  axes[2].set_ylabel('MSE All')\n",
        "\n",
        "  sns.lineplot(ax=axes[3], x='steps', y='mse_masked_all', data=df)\n",
        "  axes[3].set_title(plot_titles[3])\n",
        "  axes[3].set_xlabel('Steps')\n",
        "  axes[3].set_ylabel('MSE Masked All')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OQK_9W0VWoLf"
      },
      "outputs": [],
      "source": [
        "#@title Run training\n",
        "\n",
        "# Adapted from google3/third_party/py/scenic/projects/multimask/trainer.py.\n",
        "# and google3/third_party/py/scenic/projects/multimask/main.py\n",
        "\n",
        "train_losses = {}\n",
        "validation_losses = {}\n",
        "t_start = time.time()\n",
        "\n",
        "flax.config.update('flax_use_orbax_checkpointing', False)\n",
        "for step in tqdm.tqdm(range(start_step + 1, total_steps + 1)):\n",
        "  with jax.profiler.StepTraceAnnotation('train', step_num=step):\n",
        "    train_batch = next(dataset.train_iter)\n",
        "    train_state, t_metrics, t_logs = train_step_pmapped(\n",
        "        train_state, train_batch\n",
        "    )\n",
        "    # This will accumulate metrics in TPU memory up to the point that we log\n",
        "    # them. This is no problem for small metrics but may be a problem for\n",
        "    # large (e.g. segmentation) metrics. An alternative is to set\n",
        "    # `log_summary_steps` to a small number, or to use\n",
        "    # `train_utils.unreplicate_and_get` here instead of right before writing\n",
        "    # summaries, but that means in each step, we have data transfer between\n",
        "    # tpu and host, which might slow down the training.\n",
        "    train_metrics.append(t_metrics)\n",
        "    # Additional training logs: learning rate:\n",
        "    t_logs = jax.tree_util.tree_map(jax_utils.unreplicate, t_logs)\n",
        "    extra_training_logs.append(t_logs)\n",
        "    mae_all = round(float(t_metrics['mean_absolute_error_all'][0][0]), 2)\n",
        "    mae_masked = round(float(t_metrics['mean_absolute_error_masked'][0][0]), 2)\n",
        "    mse_all = round(float(t_metrics['mean_squared_error_all'][0][0]), 2)\n",
        "    mse_masked = round(float(t_metrics['mean_squared_error_masked'][0][0]), 2)\n",
        "    if step % 100 == 0:\n",
        "      t_current = time.time()\n",
        "      t_step = (t_current - t_start) / step\n",
        "      print(\n",
        "          'step',\n",
        "          step,\n",
        "          'mae_all:',\n",
        "          mae_all,\n",
        "          'mae_masked:',\n",
        "          mae_masked,\n",
        "          'mse_all:',\n",
        "          mse_all,\n",
        "          'mse_masked:',\n",
        "          mse_masked,\n",
        "          'time per step:',\n",
        "          f'{t_step}s\\n',\n",
        "          sep=' ',\n",
        "      )\n",
        "    train_losses[step] = (mae_all, mae_masked, mse_all, mse_masked)\n",
        "  # Quick indication that training is happening.\n",
        "  logging.log_first_n(logging.INFO, 'Finished training step %d.', 5, step)\n",
        "  for h in hooks:\n",
        "    h(step)\n",
        "  ################### EVALUATION #######################\n",
        "  if (step % log_eval_steps == 1) or (step == total_steps):\n",
        "    # chrono.pause(wait_for=(train_state.params))\n",
        "    train_state = train_utils.sync_model_state_across_replicas(train_state)\n",
        "    eval_summary = evaluate(\n",
        "          train_state,\n",
        "          step,\n",
        "          dataset.valid_iter,\n",
        "          dataset.meta_data['num_eval_examples'],\n",
        "      )\n",
        "    mae_all, mae_masked_all, mse_all, mse_masked_all = process_valid_summary(eval_summary)\n",
        "    validation_losses[step] = (mae_all, mae_masked_all, mse_all, mse_masked_all)\n",
        "\n",
        "# Time metrics\n",
        "t_end = time.time()\n",
        "t_total = (t_end - t_start)\n",
        "print(f'total training and eval time: {t_total}\\n')\n",
        "\n",
        "%matplotlib inline\n",
        "plot_steps(validation_losses, 'validation')\n",
        "plot_steps(train_losses, 'train', smoothing_factor=0.9)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "tkKKAmEbf6hJ"
      ],
      "last_runtime": {
        "build_target": "//fitbit/research/sensing/electrodes/colab:rl_colab",
        "kind": "shared"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1eOn9kpKTFjo-ZbHAKleJ4Joh_3eu1kI0",
          "timestamp": 1716677717396
        },
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/scenic_cifar_10_mae.ipynb",
          "timestamp": 1716571359843
        },
        {
          "file_id": "1N87hdhFaNHRRGsYZoAAiivaDbudNzAhE",
          "timestamp": 1716438287052
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
