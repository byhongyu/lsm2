{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEccfDHIb4Kz"
      },
      "source": [
        "## LSM Checkpoint Loading and Sample Inference Visualization.\n",
        "\n",
        "### Information:\n",
        "- Adapted from the Scenic framework.\n",
        "- Colab Kernel: `Electrodes Colab A/B/C`\n",
        "- Dataset: `Electrodes`\n",
        "\n",
        "- Grants command for Access on Demand (AoD):\n",
        "https://grants.corp.google.com/#/grants?request=20h%2Fchr-ards-electrodes-deid-colab-jobs\u0026reason=b%2F314799341\n",
        "\n",
        "### About this notebook:\n",
        "- Given a model cofig, restore a model from a checkpoint, and visualize inference on a batch (8) from the `validation` set.\n",
        "- The plots are created per sample in the batch. 1) The original input 2) The mask applied 3) The predicted reconstruction.\n",
        "- Note that the `MAE` and `MSE` on top of the reconstructed plot refer to the error between the original input image and the predicted output image (NOT the training loss, or the MAE training method).\n",
        "- This notebook provides examples of many patching strategies (`random`, `imputation`, `forecast`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14I1LdaUboeJ"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "import functools\n",
        "from typing import Any, Callable, Dict, Iterator, Tuple, Optional, Type, Union\n",
        "\n",
        "from absl import logging\n",
        "from clu import metric_writers\n",
        "from clu import periodic_actions\n",
        "from clu import platform\n",
        "\n",
        "import flax\n",
        "from flax import jax_utils\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.profiler\n",
        "\n",
        "import ml_collections\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from colabtools import adhoc_import\n",
        "with adhoc_import.Google3():\n",
        "  from scenic.dataset_lib import dataset_utils\n",
        "  from scenic.google.xm import xm_utils\n",
        "  from scenic.model_lib.base_models import base_model\n",
        "  from scenic.model_lib.base_models import model_utils\n",
        "  from scenic.model_lib.layers import nn_ops\n",
        "  from scenic.model_lib.layers import nn_layers\n",
        "  from scenic.projects.baselines import vit\n",
        "  from scenic.train_lib import optax as scenic_optax\n",
        "  from scenic.train_lib import pretrain_utils\n",
        "  from scenic.train_lib import train_utils\n",
        "\n",
        "  from scenic.projects.multimask.models import model_utils as mm_model_utils\n",
        "\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import dataset_constants\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_tiny_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.models import lsm_vit as lsm_vit_mae\n",
        "  from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils import model_constants\n",
        "  from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils import model_utils as lsm_model_utils\n",
        "  from google3.experimental.largesensormodels.scenic.trainers import lsm_mae_trainer\n",
        "\n",
        "  from google3.pyglib import gfile\n",
        "\n",
        "\n",
        "Batch = Dict[str, jnp.ndarray]\n",
        "MetricFn = Callable[\n",
        "    [jnp.ndarray, jnp.ndarray, Dict[str, jnp.ndarray]],\n",
        "    Dict[str, Tuple[float, int]],\n",
        "]\n",
        "LossFn = Callable[\n",
        "    [jnp.ndarray, Batch, Optional[jnp.ndarray], jnp.ndarray], float\n",
        "]\n",
        "LrFns = Dict[str, Callable[[jnp.ndarray], jnp.ndarray]]\n",
        "Patch = Union[Tuple[int, int], Tuple[int, int, int]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzfyGi2oEIDk"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzmyByY_ccH2"
      },
      "outputs": [],
      "source": [
        "# @title Get Model Class and Trainer\n",
        "\n",
        "def get_model_cls(model_name: str):\n",
        "  \"\"\"Get the model class for the Multimask project.\"\"\"\n",
        "  if model_name == 'lsm_vit_mae':\n",
        "    return lsm_vit_mae.ViTMAESingleChannelModel\n",
        "  else:\n",
        "    raise ValueError(f'Unrecognized model: {model_name}.')\n",
        "\n",
        "\n",
        "def get_train_fn(trainer_name):\n",
        "  if trainer_name == 'lsm_mae_trainer':\n",
        "    return lsm_mae_trainer.train\n",
        "  else:\n",
        "    raise ValueError(f'Unrecognized trainer: {trainer_name}.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUZuwnNne0eF"
      },
      "outputs": [],
      "source": [
        "# @title Visualization Functions\n",
        "\n",
        "def plot_reconstructed_image(\n",
        "    original_img: jnp.ndarray,\n",
        "    reconstructed_img: jnp.ndarray,\n",
        "    masked_tokens: jnp.ndarray,\n",
        "    step: Optional[int] = None,\n",
        "    split_name: Optional[str] = None,\n",
        "    traspose_img: bool = True\n",
        "):\n",
        "  \"\"\"Plots the original image, image mask,and the reconstructed image.\n",
        "\n",
        "  Args:\n",
        "    original_img: The original image of shape [H, W, C].\n",
        "    reconstructed_img: The reconstructed image of shape\n",
        "      [num patches, patch size (pw*ph*C)].\n",
        "    masked_tokens: masked patches of shape [num patches].\n",
        "    step: The training step (int).\n",
        "    split_name: The data split name (str).\n",
        "  \"\"\"\n",
        "  img_h, img_w, img_c = original_img.shape  # original image shape\n",
        "  ph, pw = config.model.patches.size  # get the patch shape\n",
        "  nh, nw = (img_h // ph, img_w // pw)  # get the number of patches\n",
        "\n",
        "  # Reconstruct predicted img\n",
        "  r1 = jnp.reshape(reconstructed_img, shape=(nh, nw, ph, pw, img_c))\n",
        "  r2 = jnp.transpose(r1, (0, 2, 1, 3, 4))\n",
        "  r3 = jnp.reshape(r2, shape=(nh*ph, nw*pw, img_c))\n",
        "\n",
        "  # Construct patched mask img\n",
        "  p1 = jnp.ones((nh, ph, nw, pw, img_c))\n",
        "  p2 = jnp.transpose(p1, (0, 2, 1, 3, 4))\n",
        "  p3 = jnp.reshape(p2, shape=(nh*nw, ph*pw, img_c))\n",
        "  patched_ones = jnp.reshape(p3, shape=(nh*nw, ph*pw*img_c))\n",
        "  # Apply mask\n",
        "  weights_broadcast = jax.lax.broadcast_in_dim(\n",
        "    masked_tokens,\n",
        "    shape=patched_ones.shape,\n",
        "    broadcast_dimensions=tuple(range(masked_tokens.ndim)),\n",
        "  )\n",
        "  img_mask = model_utils.apply_weights(patched_ones, weights_broadcast)\n",
        "  # Reconstruct mask image\n",
        "  rm1 = jnp.reshape(img_mask, shape=(nh, nw, ph, pw, img_c))\n",
        "  rm2 = jnp.transpose(rm1, (0, 2, 1, 3, 4))\n",
        "  mask_img_negative = jnp.reshape(rm2, shape=(nh*ph, nw*pw, img_c))\n",
        "  mask_img = 1 - mask_img_negative\n",
        "\n",
        "  # Calculate metrics\n",
        "  mse = np.mean(np.square(original_img - r3))\n",
        "  mae = np.mean(np.abs(original_img - r3))\n",
        "\n",
        "  # Plot\n",
        "  original_title = \"Original\"\n",
        "  reconstructed_title = \"Reconstructed\"\n",
        "  masked_title = \"Mask\"\n",
        "  if split_name is not None:\n",
        "    original_title = original_title + f' ({split_name})'\n",
        "    # reconstructed_title = reconstructed_title + f' ({split_name})'\n",
        "  if step is not None:\n",
        "    original_title = original_title + f' Step: {step}'\n",
        "    # reconstructed_title = reconstructed_title + f': {step}'\n",
        "\n",
        "  reconstructed_title = reconstructed_title + f' MAE: {mae:.2f}, MSE: {mse:.2f}'\n",
        "\n",
        "  if traspose_img:\n",
        "    original_plot = jnp.transpose(original_img, (1, 0, 2))\n",
        "    mask_plot = jnp.transpose(mask_img, (1, 0, 2))\n",
        "    reconstructed_plot = jnp.transpose(r3, (1, 0, 2))\n",
        "  else:\n",
        "    original_plot = original_img\n",
        "    mask_plot = mask_img\n",
        "    reconstructed_plot = r3\n",
        "\n",
        "  vmin = jnp.min(original_plot)\n",
        "  vmax = jnp.max(original_plot)\n",
        "\n",
        "  fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 8))\n",
        "  ax[0].imshow(original_plot, vmin=vmin, vmax=vmax)\n",
        "  ax[0].set_title(original_title)\n",
        "\n",
        "  ax[1].imshow(mask_plot)\n",
        "  ax[1].set_title(masked_title)\n",
        "\n",
        "  ax[2].imshow(reconstructed_plot, vmin=vmin, vmax=vmax)\n",
        "  ax[2].set_title(reconstructed_title)\n",
        "  plt.show('\\n')\n",
        "\n",
        "  return {\n",
        "      'img': original_plot,\n",
        "      'img_title': original_title,\n",
        "      'mask': mask_plot,\n",
        "      'mask_title': masked_title,\n",
        "      'reconstructed': reconstructed_plot,\n",
        "      'reconstructed_title': reconstructed_title,\n",
        "  }\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_RdcIMrfI0m"
      },
      "outputs": [],
      "source": [
        "# @title Checkpoint Restoration and Sample Inference Functions\n",
        "\n",
        "def run_eval_from_ckpt(\n",
        "    rng, config, model_cls, dataset, workdir, writer=None, step=None\n",
        "):\n",
        "\n",
        "  def visualize_eval(\n",
        "      train_state: train_utils.TrainState,\n",
        "      step: int,\n",
        "      valid_iter: Iterator[Batch],\n",
        "      num_valid_ex: int,\n",
        "      plot_sample: bool = False,\n",
        "      plot_sample_batch: bool = False,\n",
        "      rng: jax.random.PRNGKey = 42,\n",
        "  ) -\u003e Dict[str, Any]:\n",
        "    \"\"\"Run evaluation over validation sets.\n",
        "    Forked from google3/third_party/py/scenic/projects/multimask/trainer.py\n",
        "    \"\"\"\n",
        "\n",
        "    # Set up random seed for plotting random sample, and generating random masks.\n",
        "    plot_rng, eval_rng = jax.random.split(rng)\n",
        "    eval_batch_size = config.get('eval_batch_size', config.batch_size)\n",
        "    plot_dict_list = []\n",
        "\n",
        "    if not isinstance(valid_iter, dict):  # Only on validation set.\n",
        "      valid_iter, num_valid_ex = {'valid': valid_iter}, {'valid': num_valid_ex}\n",
        "\n",
        "    for val_name, val_iter in valid_iter.items():\n",
        "      eval_batch = next(val_iter)  # get eval batch\n",
        "      keys = jax.random.split(eval_rng, jax.process_count() + 1)\n",
        "      eval_rng = keys[0]\n",
        "      eval_step_rng = keys[1:]\n",
        "      # e_metrics, e_logits, e_aux = eval_step_pmapped(train_state, eval_batch, rng=eval_step_rng)\n",
        "      e_metrics, e_logits, e_aux = eval_step_pmapped(train_state, eval_batch)\n",
        "\n",
        "      masked_tokens = e_aux['token_mask']\n",
        "      for j in jnp.arange(eval_batch_size):\n",
        "        img_original = eval_batch['input_signal'][0, j, :, :, :]\n",
        "        img_reconstructed = e_logits[0, j, :, :]\n",
        "        img_mask = masked_tokens[0, j]\n",
        "        plot_dict = plot_reconstructed_image(\n",
        "            original_img=img_original,\n",
        "            reconstructed_img=img_reconstructed,\n",
        "            masked_tokens=img_mask,\n",
        "            step=step,\n",
        "            split_name=val_name,\n",
        "        )\n",
        "        plot_dict_list.append(plot_dict)\n",
        "\n",
        "    return plot_dict_list\n",
        "\n",
        "  lead_host = jax.process_index() == 0\n",
        "  # Build the loss_fn, metrics, and flax_model.\n",
        "  model = model_cls(config, dataset.meta_data)\n",
        "\n",
        "  # Initialize model.\n",
        "  rng, params_init_rng, dropout_init_rng = jax.random.split(rng, num=3)\n",
        "  init_rngs = {'params': params_init_rng, 'dropout': dropout_init_rng}\n",
        "  init_batch = next(dataset.train_iter)\n",
        "  (params, model_state, num_trainable_params, gflops) = (\n",
        "      train_utils.initialize_model(\n",
        "          model_def=model.flax_model,\n",
        "          input_spec=[\n",
        "              (init_batch['input_signal'].shape[1:], init_batch['input_signal'].dtype)\n",
        "          ],\n",
        "          config=config,\n",
        "          rngs=init_rngs,\n",
        "          train=True,  # so that masking and decoding in MAE are initialized\n",
        "      )\n",
        "  )\n",
        "\n",
        "  # Get param count\n",
        "  param_count = sum(x.size for x in jax.tree.leaves(params))\n",
        "  print(f'\\nModel Parameter Count {param_count}\\n')\n",
        "\n",
        "  # Create LR schedules and optimizer.\n",
        "  schedule_fns = scenic_optax.make_schedule(config.get('schedule'))\n",
        "  tx, _ = scenic_optax.make(config.optimizer, schedule_fns, params)\n",
        "  opt_state = tx.init(params)\n",
        "\n",
        "  rng, train_rng = jax.random.split(rng)\n",
        "\n",
        "  # Create chrono class to track and store training statistics and metadata:\n",
        "  chrono = train_utils.Chrono()\n",
        "\n",
        "  train_state = train_utils.TrainState(\n",
        "      global_step=0,\n",
        "      opt_state=opt_state,\n",
        "      tx=tx,\n",
        "      params=params,\n",
        "      model_state=model_state,\n",
        "      rng=train_rng,\n",
        "      metadata={'chrono': chrono.save()},\n",
        "  )\n",
        "  start_step = train_state.global_step\n",
        "\n",
        "  # If a checkpoint exists in the working directory.\n",
        "  if config.checkpoint:\n",
        "    train_state, start_step = train_utils.restore_checkpoint(\n",
        "        workdir, train_state, step=step\n",
        "    )\n",
        "    if start_step != 0:\n",
        "      print(f'Restoring checkpoint from train step: {start_step}\\n\\n')\n",
        "  chrono.load(train_state.metadata['chrono'])\n",
        "  train_state = train_state.replace(metadata={})\n",
        "\n",
        "  # If no checkpoint in working dir and\n",
        "  if (\n",
        "      start_step == 0  # Which means \"no\" checkpoint is restored!\n",
        "      and config.get('init_from') is not None\n",
        "  ):\n",
        "    restored_model_cfg = config.init_from.get('model_config')\n",
        "    init_checkpoint_path = config.init_from.get('checkpoint_path')\n",
        "    # BEGIN GOOGLE-INTERNAL\n",
        "    if config.init_from.get('xm'):\n",
        "      xid, wid = config.init_from.get('xm')\n",
        "      (restored_model_cfg, init_checkpoint_path) = (\n",
        "          xm_utils.get_info_from_xmanager(xid, wid)\n",
        "      )\n",
        "    # END GOOGLE-INTERNAL\n",
        "    checkpoint_format = config.init_from.get('checkpoint_format', 'scenic')\n",
        "    if init_checkpoint_path is not None:\n",
        "      if checkpoint_format == 'scenic':\n",
        "        restored_train_state = pretrain_utils.restore_pretrained_checkpoint(\n",
        "            init_checkpoint_path, train_state, assert_exist=True\n",
        "        )\n",
        "        # Load params from the init_model.\n",
        "        train_state = model.init_from_train_state(  # pytype: disable=attribute-error\n",
        "            train_state, restored_train_state, restored_model_cfg\n",
        "        )\n",
        "        del restored_train_state\n",
        "      else:\n",
        "        raise ValueError(f'Unsupported checkpoint format: {checkpoint_format}')\n",
        "\n",
        "  # Replicate the optimzier, state, and rng.\n",
        "  train_state = jax_utils.replicate(train_state)\n",
        "  del params  # Do not keep a copy of the initial params.\n",
        "\n",
        "  # Calculate the total number of training steps.\n",
        "  # TODO(adosovitskiy): get rid of epochs?\n",
        "  total_steps, steps_per_epoch = train_utils.get_num_training_steps(\n",
        "      config, dataset.meta_data\n",
        "  )\n",
        "\n",
        "  eval_step_pmapped = jax.pmap(\n",
        "      functools.partial(\n",
        "          lsm_mae_trainer.eval_step,\n",
        "          flax_model=model.flax_model,\n",
        "          metrics_fn=model.get_metrics_fn('validation'),\n",
        "          config=config,\n",
        "          debug=config.debug_eval,\n",
        "      ),\n",
        "      axis_name='batch',\n",
        "      # We can donate the eval_batch's buffer.\n",
        "      donate_argnums=(1,),\n",
        "  )\n",
        "\n",
        "\n",
        "  chrono.inform(start_step, total_steps, config.batch_size, steps_per_epoch)\n",
        "  step = start_step  # step of restored checkpoint\n",
        "  flax.config.update('flax_use_orbax_checkpointing', False)\n",
        "  ################### EVALUATION #######################\n",
        "  train_state = train_utils.sync_model_state_across_replicas(train_state)\n",
        "  rng, eval_rng = jax.random.split(rng)\n",
        "  plot_dict_list = visualize_eval(\n",
        "      train_state,\n",
        "      step,\n",
        "      dataset.valid_iter,\n",
        "      dataset.meta_data['num_val_examples'],\n",
        "      plot_sample=True,\n",
        "      plot_sample_batch=True,\n",
        "      rng=eval_rng,\n",
        "  )\n",
        "\n",
        "  return plot_dict_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cILfoUxEN6n"
      },
      "source": [
        "## Sample Inference and Patching Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BAxUOrLch3x"
      },
      "outputs": [],
      "source": [
        "# @title Base Config\n",
        "\n",
        "r\"\"\"A config to train a Base ViT MAE on LSM dataset.\n",
        "\n",
        "Forked from google3/third_party/py/scenic/projects/multimask/configs/mae_cifar10_tiny.py\n",
        "\n",
        "To run on XManager:\n",
        "gxm third_party/py/scenic/google/xm/launch_xm.py -- \\\n",
        "--binary //experimental/largesensormodels/scenic:main \\\n",
        "--config=experimental/largesensormodels/scenic/configs/mae_lsm_base.py \\\n",
        "--platform=vlp_4x8 \\\n",
        "--exp_name=lsm_mae_tier2_base_10_5_res \\\n",
        "--workdir=/cns/dz-d/home/xliucs/lsm/xm/\\{xid\\} \\\n",
        "--xm_resource_alloc=group:mobile-dynamic/h2o-ai-gqm-quota \\\n",
        "--scheduling_time_quantum=2d \\\n",
        "--priority=200\n",
        "\n",
        "To run locally:\n",
        "./third_party/py/scenic/google/runlocal.sh \\\n",
        "--uptc=\"\" \\\n",
        "--binary=//experimental/largesensormodels/scenic:main \\\n",
        "--config=$(pwd)/experimental/largesensormodels/scenic/configs/mae_lsm_base.py:runlocal\n",
        "\"\"\"\n",
        "\n",
        "from typing import Optional\n",
        "import ml_collections\n",
        "from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils import model_constants\n",
        "# from google3.experimental.largesensormodels.scenic.utils import linear_probe_config\n",
        "\n",
        "\n",
        "# To set constants.\n",
        "# 1) Dataset variables.\n",
        "DATASET_NAME = 'lsm_300min_pretraining_165K_n10'\n",
        "CACHE_DATASET = True\n",
        "TRAIN_DATA_SIZES = [1000, 10000, 100000, 750000, 1321235]\n",
        "USE_DATETIME_FEATURES = False\n",
        "USE_TRAIN_AUGMENTATIONS = [True]\n",
        "TRAIN_AUGMENTATIONS = ['stretch', 'flip', 'noise']\n",
        "SHUFFLE_SEED = 42\n",
        "\n",
        "# 2) Training / eval variables.\n",
        "BATCH_SIZE = 4096\n",
        "NUM_TRAIN_STEPS = 50000\n",
        "LRS = [5e-3]\n",
        "WEIGHT_DECAYS = [1e-4]\n",
        "\n",
        "# 3) Logging variables.\n",
        "LOG_EVAL_SUMMARY_STEPS = 500  # STEPS_PER_EPOCH\n",
        "LOG_CHECKPOINT_STEPS = 100  # LOG_EVAL_SUMMARY_STEPS * 5\n",
        "MAX_NUM_CHECKPOINTS = int(NUM_TRAIN_STEPS / LOG_CHECKPOINT_STEPS)\n",
        "\n",
        "# Model variant / patch H (time steps) / patch W (features)\n",
        "VARIANT = 'B/10/5'\n",
        "TOKEN_MASK_PROB = 'constant_0.8'\n",
        "LOSS_ONLY_MASKED_TOKENS = True\n",
        "\n",
        "# Downstream Tasks.\n",
        "# Imputation and forecast eval\n",
        "RECONSTRUCTION_HORIZONS = [0.1, 0.2, 0.4]\n",
        "\n",
        "# Linear probe eval.\n",
        "LINEAR_PROBE_USE_TRAIN_AUGMENTATIONS = False\n",
        "LINEAR_PROBE_TRAIN_AUGMENTATIONS = ['noise']\n",
        "\n",
        "\n",
        "def get_config_common_few_shot(\n",
        "    batch_size: Optional[int] = None,\n",
        "    target_resolution: int = 224,\n",
        "    resize_resolution: int = 256,\n",
        ") -\u003e ml_collections.ConfigDict:\n",
        "  \"\"\"Returns a standard-ish fewshot eval configuration.\n",
        "\n",
        "  Copied from\n",
        "  third_party/py/scenic/projects/baselines/configs/google/common/common_fewshot.py\n",
        "\n",
        "  Args:\n",
        "    batch_size: The batch size to use for fewshot evaluation.\n",
        "    target_resolution: The target resolution of the fewshot evaluation.\n",
        "    resize_resolution: The resize resolution of the fewshot evaluation.\n",
        "\n",
        "  Returns:\n",
        "    A ConfigDict with the fewshot evaluation configuration.\n",
        "  \"\"\"\n",
        "  config = ml_collections.ConfigDict()\n",
        "  config.batch_size = batch_size\n",
        "  config.representation_layer = 'pre_logits'\n",
        "  config.log_eval_steps = 25_000\n",
        "  config.datasets = {\n",
        "      'birds': ('caltech_birds2011', 'train', 'test'),\n",
        "      'caltech': ('caltech101', 'train', 'test'),\n",
        "      'cars': ('cars196:2.1.0', 'train', 'test'),\n",
        "      'cifar100': ('cifar100', 'train', 'test'),\n",
        "      'col_hist': ('colorectal_histology', 'train[:2000]', 'train[2000:]'),\n",
        "      'dtd': ('dtd', 'train', 'test'),\n",
        "      'imagenet': ('imagenet2012_subset/10pct', 'train', 'validation'),\n",
        "      'pets': ('oxford_iiit_pet', 'train', 'test'),\n",
        "      'uc_merced': ('uc_merced', 'train[:1000]', 'train[1000:]'),\n",
        "  }\n",
        "  config.pp_train = f'decode|resize({resize_resolution})|central_crop({target_resolution})|value_range(-1,1)'\n",
        "  config.pp_eval = f'decode|resize({resize_resolution})|central_crop({target_resolution})|value_range(-1,1)'\n",
        "  config.shots = [1, 5, 10, 25]\n",
        "  config.l2_regs = [2.0**i for i in range(-10, 20)]\n",
        "  config.walk_first = ('imagenet', 10)\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "def get_config(runlocal=''):\n",
        "  \"\"\"Returns the ViT experiment configuration.\"\"\"\n",
        "\n",
        "  runlocal = bool(runlocal)\n",
        "\n",
        "  # Experiment.\n",
        "  config = ml_collections.ConfigDict()\n",
        "  config.experiment_name = f'electrodes-mae-{DATASET_NAME}'\n",
        "  config.dataset_name = f'lsm_prod/{DATASET_NAME}'\n",
        "  config.shuffle_seed = SHUFFLE_SEED\n",
        "\n",
        "  # Dataset.\n",
        "  config.data_dtype_str = 'float32'\n",
        "  config.dataset_configs = ml_collections.ConfigDict()\n",
        "  config.dataset_configs.dataset = f'lsm_prod/{DATASET_NAME}'\n",
        "  config.dataset_configs.num_classes = None\n",
        "  config.dataset_configs.train_split = 'train'  # train data split\n",
        "  config.dataset_configs.train_num_samples = TRAIN_DATA_SIZES[0]  # train sample\n",
        "  # eval data split - note: this split is used for validation and test.\n",
        "  config.dataset_configs.eval_split = 'test[:64]' if runlocal else 'test'\n",
        "  config.dataset_configs.cache_dataset = CACHE_DATASET\n",
        "  config.dataset_configs.prefetch_to_device = 2\n",
        "  # Shuffle_buffer_size is per host, so small-ish is ok.\n",
        "  config.dataset_configs.shuffle_buffer_size = 250_000\n",
        "\n",
        "  # Model.\n",
        "  if len(VARIANT.split('/')) == 3:\n",
        "    version = VARIANT.split('/')[0]  # model variant\n",
        "    patch_h = VARIANT.split('/')[1]  # patch width\n",
        "    patch_w = VARIANT.split('/')[2]  # patch height\n",
        "  elif len(VARIANT.split('/')) == 2:\n",
        "    version = VARIANT.split('/')[0]  # model variant\n",
        "    patch_h = VARIANT.split('/')[1]  # patch width\n",
        "    patch_w = VARIANT.split('/')[1]  # patch height\n",
        "  else:\n",
        "    raise ValueError(f'Invalid model variant: {VARIANT}')\n",
        "\n",
        "  version = 'Deb' if runlocal else version\n",
        "  config.model_name = 'lsm_vit_mae'\n",
        "  config.model = ml_collections.ConfigDict()\n",
        "  # encoder\n",
        "  config.model.hidden_size = model_constants.HIDDEN_SIZES[version]\n",
        "  config.model.patches = ml_collections.ConfigDict()\n",
        "  config.model.patches.size = tuple([int(patch_h), int(patch_w)])\n",
        "  config.model.num_heads = model_constants.NUM_HEADS[version]\n",
        "  config.model.mlp_dim = model_constants.MLP_DIMS[version]\n",
        "  config.model.num_layers = model_constants.NUM_LAYERS[version]\n",
        "  config.model.dropout_rate = 0.0\n",
        "  config.model.classifier = 'none'  # Has to be \"none\" for the autoencoder\n",
        "  config.model.representation_size = None\n",
        "  config.model.positional_embedding = 'sinusoidal_2d'\n",
        "  config.model.positional_embedding_decoder = 'sinusoidal_2d'\n",
        "  # decoder\n",
        "  config.model.decoder_config = ml_collections.ConfigDict()\n",
        "  config.model.decoder_config.hidden_size = (\n",
        "      model_constants.DECODER_HIDDEN_SIZES[version]\n",
        "  )\n",
        "  config.model.decoder_config.mlp_dim = model_constants.DECODER_MLP_DIMS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.num_layers = model_constants.DECODER_NUM_LAYERS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.num_heads = model_constants.DECODER_NUM_HEADS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.dropout_rate = 0.0\n",
        "  config.model.decoder_config.attention_dropout_rate = 0.0\n",
        "\n",
        "  config.masked_feature_loss = ml_collections.ConfigDict()\n",
        "  config.masked_feature_loss.targets_type = 'rgb'\n",
        "  config.masked_feature_loss.token_mask_probability = TOKEN_MASK_PROB\n",
        "  config.masked_feature_loss.loss_only_masked_tokens = LOSS_ONLY_MASKED_TOKENS\n",
        "  config.masked_feature_loss.loss_type = 'squared'  # 'squared' or 'absolute'\n",
        "\n",
        "  # Datetime features.\n",
        "  config.use_datetime_features = USE_DATETIME_FEATURES\n",
        "\n",
        "  # Training.\n",
        "  config.trainer_name = 'lsm_mae_trainer'\n",
        "  config.batch_size = 8 if runlocal else BATCH_SIZE\n",
        "  config.num_training_steps = NUM_TRAIN_STEPS\n",
        "  config.log_eval_steps = LOG_EVAL_SUMMARY_STEPS\n",
        "  config.log_summary_steps = LOG_EVAL_SUMMARY_STEPS\n",
        "  config.rng_seed = 42\n",
        "  config.use_train_augmentations = USE_TRAIN_AUGMENTATIONS[0]\n",
        "  config.train_augmentations = TRAIN_AUGMENTATIONS\n",
        "  sched = ml_collections.ConfigDict()\n",
        "  sched.re = '(.*)'\n",
        "  sched.lr_configs = ml_collections.ConfigDict()\n",
        "  sched.lr_configs.learning_rate_schedule = 'compound'\n",
        "  sched.lr_configs.factors = 'constant * cosine_decay * linear_warmup'\n",
        "  sched.lr_configs.total_steps = NUM_TRAIN_STEPS\n",
        "  sched.lr_configs.steps_per_cycle = sched.lr_configs.total_steps\n",
        "  sched.lr_configs.warmup_steps = int(NUM_TRAIN_STEPS * 0.05)\n",
        "  sched.lr_configs.base_learning_rate = LRS[0]\n",
        "  config.schedule = ml_collections.ConfigDict({'all': sched})\n",
        "\n",
        "  # *Single* optimizer.\n",
        "  optim = ml_collections.ConfigDict()\n",
        "  optim.optax_name = 'scale_by_adam'\n",
        "  # optim.optax = dict(mu_dtype='bfloat16')\n",
        "  optim.optax_configs = ml_collections.ConfigDict({  # Optimizer settings.\n",
        "      'b1': 0.9,\n",
        "      'b2': 0.95,\n",
        "  })\n",
        "  config.optax = dict(mu_dtype='bfloat16')\n",
        "  optim.max_grad_norm = 1.0\n",
        "\n",
        "  optim.weight_decay = WEIGHT_DECAYS[0]\n",
        "  optim.weight_decay_decouple = True\n",
        "  config.optimizer = optim\n",
        "\n",
        "  # # Downstream Tasks.\n",
        "  # # 0) Linear Probing.\n",
        "  # config.linear_probe_gather_to_host = False if runlocal else False\n",
        "  # config.linear_probe_representation_layer = 'pre_logits'\n",
        "  # config.linear_probe_log_eval_steps = LOG_EVAL_SUMMARY_STEPS\n",
        "  # config.linear_probe = linear_probe_config.get_linear_probe_config(\n",
        "  #     log_eval_steps=LOG_EVAL_SUMMARY_STEPS,\n",
        "  #     model_config=config.model,\n",
        "  #     use_datetime_features=USE_DATETIME_FEATURES,\n",
        "  #     use_train_augmentations=LINEAR_PROBE_USE_TRAIN_AUGMENTATIONS,\n",
        "  #     train_augmentations=LINEAR_PROBE_TRAIN_AUGMENTATIONS,\n",
        "  #     cache_dataset=CACHE_DATASET,\n",
        "  #     runlocal=runlocal,\n",
        "  #     masked_feature_loss=LOSS_ONLY_MASKED_TOKENS,\n",
        "  # )\n",
        "  # # 1) Fewshot.\n",
        "  # # TODO(girishvn): This needs to be adapted to electrode dataset\n",
        "  # config.fewshot = get_config_common_few_shot(batch_size=config.batch_size)\n",
        "  # config.fewshot.datasets = {}\n",
        "  # config.fewshot.walk_first = ()\n",
        "  # config.fewshot.representation_layer = 'pre_logits'\n",
        "  # config.fewshot.log_eval_steps = LOG_EVAL_SUMMARY_STEPS\n",
        "\n",
        "  # 2) Reconstruction Eval Tasks (Forecast and Imputation).\n",
        "  config.forecast = ml_collections.ConfigDict()\n",
        "  config.forecast.horizons = RECONSTRUCTION_HORIZONS\n",
        "  config.imputation = ml_collections.ConfigDict()\n",
        "  config.imputation.horizons = RECONSTRUCTION_HORIZONS\n",
        "\n",
        "  # Logging.\n",
        "  config.write_summary = True\n",
        "  config.xprof = True  # Profile using xprof.\n",
        "  config.checkpoint = True  # Do checkpointing.\n",
        "  config.checkpoint_steps = LOG_CHECKPOINT_STEPS\n",
        "  config.debug_train = False  # Debug mode during training.\n",
        "  config.debug_eval = False  # Debug mode during eval.\n",
        "  config.max_checkpoints_to_keep = MAX_NUM_CHECKPOINTS\n",
        "  # BEGIN GOOGLE-INTERNAL\n",
        "  if runlocal:\n",
        "    # Current implementation fails with UPTC.\n",
        "    config.count_flops = False\n",
        "  # END GOOGLE-INTERNAL\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "# BEGIN GOOGLE-INTERNAL\n",
        "def get_hyper(hyper):\n",
        "  \"\"\"Defines the hyper-parameters sweeps for doing grid search.\"\"\"\n",
        "  return hyper.product([\n",
        "      hyper.sweep('config.schedule.all.lr_configs.base_learning_rate', LRS),\n",
        "      hyper.sweep('config.optimizer.weight_decay', WEIGHT_DECAYS),\n",
        "      hyper.sweep('config.dataset_configs.train_num_samples', TRAIN_DATA_SIZES),\n",
        "      hyper.sweep('config.use_train_augmentations', USE_TRAIN_AUGMENTATIONS),\n",
        "  ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4YJtwmjDMFQ"
      },
      "outputs": [],
      "source": [
        "# @title Available Compute\n",
        "\n",
        "print(\"Available devices:\", jax.devices())\n",
        "print(\"Default device:\", jax.default_backend())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bglNAdYCiOzR"
      },
      "outputs": [],
      "source": [
        "# @title 80% Random Patching\n",
        "\n",
        "# Things to set\n",
        "TOKEN_MASK_PROB = 'constant_0.8_random'  # update masking strategy\n",
        "workdir = '/cns/dz-d/home/xliucs/lsm/xm/124248847/5/'  # working directory\n",
        "step = None  # change the step to reflect the desired checkpoint step\n",
        "\n",
        "# Derived values\n",
        "work_dir_split = workdir.split('/')\n",
        "work_dir_split = [i for i in work_dir_split if i != '']\n",
        "xid = work_dir_split[-2]\n",
        "wid = work_dir_split[-1]\n",
        "\n",
        "print('Workdir', workdir)\n",
        "print('XID', xid)\n",
        "print('WID', wid)\n",
        "\n",
        "# Run pipeline\n",
        "config = get_config(runlocal=False)  # get configs\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "model_cls = get_model_cls(config.model_name)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = lsm_tiny_dataset.get_dataset(config, data_rng)\n",
        "\n",
        "output_plots = run_eval_from_ckpt(\n",
        "    rng, config, model_cls, dataset, workdir, step=step\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Mu_mtb8ENzVI"
      },
      "outputs": [],
      "source": [
        "# @title 50% Temporal Imputation Patching\n",
        "\n",
        "# Things to set\n",
        "TOKEN_MASK_PROB = 'constant_0.5_imputation_time'  # update masking strategy\n",
        "workdir = '/cns/dz-d/home/xliucs/lsm/xm/117310802/1/'  # working directory\n",
        "step = None  # change the step to reflect the desired checkpoint step\n",
        "\n",
        "# Derived values\n",
        "work_dir_split = workdir.split('/')\n",
        "work_dir_split = [i for i in work_dir_split if i != '']\n",
        "xid = work_dir_split[-2]\n",
        "wid = work_dir_split[-1]\n",
        "\n",
        "print('Workdir', workdir)\n",
        "print('XID', xid)\n",
        "print('WID', wid)\n",
        "\n",
        "# Run pipeline\n",
        "config = get_config(runlocal=False)  # get configs\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "model_cls = get_model_cls(config.model_name)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = lsm_tiny_dataset.get_dataset(config, data_rng)\n",
        "\n",
        "output_plots = run_eval_from_ckpt(\n",
        "    rng, config, model_cls, dataset, workdir, step=step\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B0ssg1MpDr6P"
      },
      "outputs": [],
      "source": [
        "# @title 25% Temporal Forecast Patching\n",
        "\n",
        "# Things to set\n",
        "TOKEN_MASK_PROB = 'constant_0.25_forecast_time'  # update masking strategy\n",
        "workdir = '/cns/dz-d/home/xliucs/lsm/xm/117310802/1/'  # working directory\n",
        "step = None  # change the step to reflect the desired checkpoint step\n",
        "\n",
        "# Derived values\n",
        "work_dir_split = workdir.split('/')\n",
        "work_dir_split = [i for i in work_dir_split if i != '']\n",
        "xid = work_dir_split[-2]\n",
        "wid = work_dir_split[-1]\n",
        "\n",
        "print('Workdir', workdir)\n",
        "print('XID', xid)\n",
        "print('WID', wid)\n",
        "\n",
        "# Run pipeline\n",
        "config = get_config(runlocal=False)  # get configs\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "model_cls = get_model_cls(config.model_name)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = lsm_tiny_dataset.get_dataset(config, data_rng)\n",
        "\n",
        "output_plots = run_eval_from_ckpt(\n",
        "    rng, config, model_cls, dataset, workdir, step=step\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gBLgb74OD0Ev"
      },
      "outputs": [],
      "source": [
        "# @title 35% Feature Imputation Patching\n",
        "\n",
        "# Things to set\n",
        "TOKEN_MASK_PROB = 'constant_0.35_imputation_feature'  # update masking strategy\n",
        "workdir = '/cns/dz-d/home/xliucs/lsm/xm/117310802/1/'  # working directory\n",
        "step = None  # change the step to reflect the desired checkpoint step\n",
        "\n",
        "# Derived values\n",
        "work_dir_split = workdir.split('/')\n",
        "work_dir_split = [i for i in work_dir_split if i != '']\n",
        "xid = work_dir_split[-2]\n",
        "wid = work_dir_split[-1]\n",
        "\n",
        "print('Workdir', workdir)\n",
        "print('XID', xid)\n",
        "print('WID', wid)\n",
        "\n",
        "# Run pipeline\n",
        "config = get_config(runlocal=False)  # get configs\n",
        "rng = jax.random.PRNGKey(config.rng_seed)\n",
        "model_cls = get_model_cls(config.model_name)\n",
        "data_rng, rng = jax.random.split(rng)\n",
        "dataset = lsm_tiny_dataset.get_dataset(config, data_rng)\n",
        "\n",
        "output_plots = run_eval_from_ckpt(\n",
        "    rng, config, model_cls, dataset, workdir, step=step\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//fitbit/research/sensing/electrodes/colab:rl_colab",
        "kind": "shared"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/experimental/largesensormodels/notebooks/[LSM]_visualize_checkpoint_inference.ipynb",
          "timestamp": 1726068608801
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
