{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bsFQ_TrmWxwF"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "import io\n",
        "import functools\n",
        "from typing import Any, Callable, Dict, Iterator, Tuple, Optional, Type, Union\n",
        "import time\n",
        "from collections import Counter\n",
        "import copy\n",
        "\n",
        "from absl import logging\n",
        "from clu import metric_writers\n",
        "from clu import periodic_actions\n",
        "from clu import platform\n",
        "\n",
        "import flax\n",
        "from flax import jax_utils\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.profiler\n",
        "\n",
        "import pandas as pd\n",
        "import ml_collections\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from colabtools import adhoc_import\n",
        "with adhoc_import.Google3():\n",
        "  from scenic.dataset_lib import dataset_utils\n",
        "  from scenic.google.xm import xm_utils\n",
        "  from scenic.model_lib.base_models import base_model\n",
        "  from scenic.model_lib.base_models import model_utils\n",
        "  from scenic.model_lib.layers import nn_ops\n",
        "  from scenic.model_lib.layers import nn_layers\n",
        "  from scenic.projects.baselines import vit\n",
        "  from scenic.train_lib import optax as scenic_optax\n",
        "  from scenic.train_lib import pretrain_utils\n",
        "  from scenic.train_lib import train_utils\n",
        "\n",
        "  from scenic.projects.multimask.models import model_utils as mm_model_utils\n",
        "\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import dataset_constants\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_activity_subset_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_mood_vs_activity_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_tiny_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_combined_pretrain_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_fewshot_mood_vs_activity_dataset\n",
        "  from google3.experimental.largesensormodels.scenic.datasets import lsm_fewshot_remapped_activity_dataset\n",
        "\n",
        "  from google3.experimental.largesensormodels.scenic.models import lsm_vit as lsm_vit_mae\n",
        "  from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils import model_constants\n",
        "  from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils import model_utils as lsm_model_utils\n",
        "  from google3.experimental.largesensormodels.scenic.trainers import lsm_mae_trainer\n",
        "\n",
        "  from google3.pyglib import gfile\n",
        "\n",
        "\n",
        "Batch = Dict[str, jnp.ndarray]\n",
        "MetricFn = Callable[\n",
        "    [jnp.ndarray, jnp.ndarray, Dict[str, jnp.ndarray]],\n",
        "    Dict[str, Tuple[float, int]],\n",
        "]\n",
        "LossFn = Callable[\n",
        "    [jnp.ndarray, Batch, Optional[jnp.ndarray], jnp.ndarray], float\n",
        "]\n",
        "LrFns = Dict[str, Callable[[jnp.ndarray], jnp.ndarray]]\n",
        "Patch = Union[Tuple[int, int], Tuple[int, int, int]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NytT8oH0XP_9"
      },
      "outputs": [],
      "source": [
        "# @title Sample Config (Base ViT MAE)\n",
        "\n",
        "r\"\"\"A config to train a Base ViT MAE on LSM dataset.\n",
        "\n",
        "Forked from google3/third_party/py/scenic/projects/multimask/configs/mae_cifar10_tiny.py\n",
        "\"\"\"\n",
        "\n",
        "from typing import Optional\n",
        "import ml_collections\n",
        "from google3.experimental.largesensormodels.scenic.models.lsm_vit_utils import model_constants\n",
        "\n",
        "\n",
        "# To set constants.\n",
        "# 1) Dataset variables.\n",
        "DATASET_NAME = 'lsm_300min_pretraining_165K_n10'\n",
        "CACHE_DATASET = True\n",
        "TRAIN_DATA_SIZES = [1321235]\n",
        "USE_DATETIME_FEATURES = False\n",
        "USE_TRAIN_AUGMENTATIONS = [True]\n",
        "TRAIN_AUGMENTATIONS = ['stretch', 'flip', 'noise']\n",
        "SHUFFLE_SEED = 42\n",
        "\n",
        "# 2) Training / eval variables.\n",
        "BATCH_SIZE = 8\n",
        "NUM_TRAIN_STEPS = 50000\n",
        "LRS = [5e-3]\n",
        "WEIGHT_DECAYS = [1e-4]\n",
        "\n",
        "# 3) Logging variables.\n",
        "LOG_EVAL_SUMMARY_STEPS = 500  # STEPS_PER_EPOCH\n",
        "LOG_CHECKPOINT_STEPS = 100  # LOG_EVAL_SUMMARY_STEPS * 5\n",
        "MAX_NUM_CHECKPOINTS = int(NUM_TRAIN_STEPS / LOG_CHECKPOINT_STEPS)\n",
        "\n",
        "# Model variant / patch H (time steps) / patch W (features)\n",
        "VARIANT = 'B/10/5'\n",
        "TOKEN_MASK_PROB = 'constant_0.8'\n",
        "LOSS_ONLY_MASKED_TOKENS = True\n",
        "\n",
        "# Downstream Tasks.\n",
        "# Imputation and forecast eval\n",
        "RECONSTRUCTION_HORIZONS = [0.1, 0.2, 0.4]\n",
        "\n",
        "# Linear probe eval.\n",
        "LINEAR_PROBE_USE_TRAIN_AUGMENTATIONS = False\n",
        "LINEAR_PROBE_TRAIN_AUGMENTATIONS = ['noise']\n",
        "\n",
        "\n",
        "def get_config(runlocal=''):\n",
        "  \"\"\"Returns the ViT experiment configuration.\"\"\"\n",
        "\n",
        "  runlocal = bool(runlocal)\n",
        "\n",
        "  # Experiment.\n",
        "  config = ml_collections.ConfigDict()\n",
        "  config.experiment_name = f'electrodes-mae-{DATASET_NAME}'\n",
        "  config.dataset_name = f'lsm_prod/{DATASET_NAME}'\n",
        "  config.shuffle_seed = SHUFFLE_SEED\n",
        "\n",
        "  # Dataset.\n",
        "  config.data_dtype_str = 'float32'\n",
        "  config.dataset_configs = ml_collections.ConfigDict()\n",
        "  config.dataset_configs.dataset = f'lsm_prod/{DATASET_NAME}'\n",
        "  config.dataset_configs.num_classes = None\n",
        "  config.dataset_configs.train_split = 'train'  # train data split\n",
        "  config.dataset_configs.train_num_samples = TRAIN_DATA_SIZES[0]  # train sample\n",
        "  # eval data split - note: this split is used for validation and test.\n",
        "  config.dataset_configs.eval_split = 'test[:64]' if runlocal else 'test'\n",
        "  config.dataset_configs.cache_dataset = CACHE_DATASET\n",
        "  config.dataset_configs.prefetch_to_device = 2\n",
        "  # Shuffle_buffer_size is per host, so small-ish is ok.\n",
        "  config.dataset_configs.shuffle_buffer_size = 250_000\n",
        "\n",
        "  # Model.\n",
        "  if len(VARIANT.split('/')) == 3:\n",
        "    version = VARIANT.split('/')[0]  # model variant\n",
        "    patch_h = VARIANT.split('/')[1]  # patch width\n",
        "    patch_w = VARIANT.split('/')[2]  # patch height\n",
        "  elif len(VARIANT.split('/')) == 2:\n",
        "    version = VARIANT.split('/')[0]  # model variant\n",
        "    patch_h = VARIANT.split('/')[1]  # patch width\n",
        "    patch_w = VARIANT.split('/')[1]  # patch height\n",
        "  else:\n",
        "    raise ValueError(f'Invalid model variant: {VARIANT}')\n",
        "\n",
        "  version = 'Deb' if runlocal else version\n",
        "  config.model_name = 'lsm_vit_mae'\n",
        "  config.model = ml_collections.ConfigDict()\n",
        "  # encoder\n",
        "  config.model.hidden_size = model_constants.HIDDEN_SIZES[version]\n",
        "  config.model.patches = ml_collections.ConfigDict()\n",
        "  config.model.patches.size = tuple([int(patch_h), int(patch_w)])\n",
        "  config.model.num_heads = model_constants.NUM_HEADS[version]\n",
        "  config.model.mlp_dim = model_constants.MLP_DIMS[version]\n",
        "  config.model.num_layers = model_constants.NUM_LAYERS[version]\n",
        "  config.model.dropout_rate = 0.0\n",
        "  config.model.classifier = 'none'  # Has to be \"none\" for the autoencoder\n",
        "  config.model.representation_size = None\n",
        "  config.model.positional_embedding = 'sinusoidal_2d'\n",
        "  config.model.positional_embedding_decoder = 'sinusoidal_2d'\n",
        "  # decoder\n",
        "  config.model.decoder_config = ml_collections.ConfigDict()\n",
        "  config.model.decoder_config.hidden_size = (\n",
        "      model_constants.DECODER_HIDDEN_SIZES[version]\n",
        "  )\n",
        "  config.model.decoder_config.mlp_dim = model_constants.DECODER_MLP_DIMS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.num_layers = model_constants.DECODER_NUM_LAYERS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.num_heads = model_constants.DECODER_NUM_HEADS[\n",
        "      version\n",
        "  ]\n",
        "  config.model.decoder_config.dropout_rate = 0.0\n",
        "  config.model.decoder_config.attention_dropout_rate = 0.0\n",
        "\n",
        "  config.masked_feature_loss = ml_collections.ConfigDict()\n",
        "  config.masked_feature_loss.targets_type = 'rgb'\n",
        "  config.masked_feature_loss.token_mask_probability = TOKEN_MASK_PROB\n",
        "  config.masked_feature_loss.loss_only_masked_tokens = LOSS_ONLY_MASKED_TOKENS\n",
        "  config.masked_feature_loss.loss_type = 'squared'  # 'squared' or 'absolute'\n",
        "\n",
        "  # Datetime features.\n",
        "  config.use_datetime_features = USE_DATETIME_FEATURES\n",
        "\n",
        "  # Training.\n",
        "  config.trainer_name = 'lsm_mae_trainer'\n",
        "  config.batch_size = 8 if runlocal else BATCH_SIZE\n",
        "  config.num_training_steps = NUM_TRAIN_STEPS\n",
        "  config.log_eval_steps = LOG_EVAL_SUMMARY_STEPS\n",
        "  config.log_summary_steps = LOG_EVAL_SUMMARY_STEPS\n",
        "  config.rng_seed = 42\n",
        "  config.use_train_augmentations = USE_TRAIN_AUGMENTATIONS[0]\n",
        "  config.train_augmentations = TRAIN_AUGMENTATIONS\n",
        "  sched = ml_collections.ConfigDict()\n",
        "  sched.re = '(.*)'\n",
        "  sched.lr_configs = ml_collections.ConfigDict()\n",
        "  sched.lr_configs.learning_rate_schedule = 'compound'\n",
        "  sched.lr_configs.factors = 'constant * cosine_decay * linear_warmup'\n",
        "  sched.lr_configs.total_steps = NUM_TRAIN_STEPS\n",
        "  sched.lr_configs.steps_per_cycle = sched.lr_configs.total_steps\n",
        "  sched.lr_configs.warmup_steps = int(NUM_TRAIN_STEPS * 0.05)\n",
        "  sched.lr_configs.base_learning_rate = LRS[0]\n",
        "  config.schedule = ml_collections.ConfigDict({'all': sched})\n",
        "\n",
        "  # *Single* optimizer.\n",
        "  optim = ml_collections.ConfigDict()\n",
        "  optim.optax_name = 'scale_by_adam'\n",
        "  # optim.optax = dict(mu_dtype='bfloat16')\n",
        "  optim.optax_configs = ml_collections.ConfigDict({  # Optimizer settings.\n",
        "      'b1': 0.9,\n",
        "      'b2': 0.95,\n",
        "  })\n",
        "  config.optax = dict(mu_dtype='bfloat16')\n",
        "  optim.max_grad_norm = 1.0\n",
        "\n",
        "  optim.weight_decay = WEIGHT_DECAYS[0]\n",
        "  optim.weight_decay_decouple = True\n",
        "  config.optimizer = optim\n",
        "\n",
        "  # Downstream Tasks.\n",
        "\n",
        "  # 2) Reconstruction Eval Tasks (Forecast and Imputation).\n",
        "  config.forecast = ml_collections.ConfigDict()\n",
        "  config.forecast.horizons = RECONSTRUCTION_HORIZONS\n",
        "  config.imputation = ml_collections.ConfigDict()\n",
        "  config.imputation.horizons = RECONSTRUCTION_HORIZONS\n",
        "\n",
        "  # Logging.\n",
        "  config.write_summary = True\n",
        "  config.xprof = True  # Profile using xprof.\n",
        "  config.checkpoint = True  # Do checkpointing.\n",
        "  config.checkpoint_steps = LOG_CHECKPOINT_STEPS\n",
        "  config.debug_train = False  # Debug mode during training.\n",
        "  config.debug_eval = False  # Debug mode during eval.\n",
        "  config.max_checkpoints_to_keep = MAX_NUM_CHECKPOINTS\n",
        "  # BEGIN GOOGLE-INTERNAL\n",
        "  if runlocal:\n",
        "    # Current implementation fails with UPTC.\n",
        "    config.count_flops = False\n",
        "  # END GOOGLE-INTERNAL\n",
        "\n",
        "  return config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Kr8dPMiDHums"
      },
      "outputs": [],
      "source": [
        "# @title Parameter Equivalence Function\n",
        "\n",
        "# Load params from the restored_train_state.\n",
        "def compare_params(params1, params2, space=0):\n",
        "  for key, value in params1.items():\n",
        "\n",
        "    if isinstance(value, dict):\n",
        "      # If value is a dict, ensure the second dict has this key and recurse.\n",
        "      if key in params2 and isinstance(params2[key], dict):\n",
        "        if space == 0: print()\n",
        "        print(f\"{' '*space*2}{key}:\")\n",
        "        compare_params(value, params2[key], space + 1)\n",
        "\n",
        "    else:\n",
        "      if key in params2:\n",
        "        if params1[key].shape != params2[key].shape:\n",
        "          print(f\"{' '*space*2}{key}: params1 shape', {params1[key].shape}\")\n",
        "          print(f\"{' '*space*2}{key}: params1 shape', {params2[key].shape}\")\n",
        "        elif not jnp.array_equal(params1[key], params2[key]):\n",
        "          print(f\"{' '*space*2}{key}: NOT EQUAL\")\n",
        "        # else:\n",
        "        #   print(f\"{' '*space*2}{key}: EQUAL\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_sbivCnAe78B"
      },
      "outputs": [],
      "source": [
        "# @title Model Init Function\n",
        "\n",
        "def init_model(config, input_shape, dtype=jnp.float32):\n",
        "  \"\"\"Initialize model.\"\"\"\n",
        "  model_cls = lsm_vit_mae.ViTMAESingleChannelModel\n",
        "  model = model_cls(config, {})\n",
        "  rng = jax.random.PRNGKey(config.rng_seed)\n",
        "  rng, params_init_rng, dropout_init_rng = jax.random.split(rng, num=3)\n",
        "  init_rngs = {'params': params_init_rng, 'dropout': dropout_init_rng}\n",
        "  (params, model_state, _, _) = (\n",
        "      train_utils.initialize_model(\n",
        "          model_def=model.flax_model,\n",
        "          input_spec=[(\n",
        "              input_shape,\n",
        "              dtype,\n",
        "          )],\n",
        "          config=config,\n",
        "          rngs=init_rngs,\n",
        "          train=True,  # so that masking and decoding in MAE are initialized\n",
        "      )\n",
        "  )\n",
        "  # NOTE: Do not delete init batch, as it may be used to init downstream models.\n",
        "\n",
        "  # Create LR schedules and optimizer.\n",
        "  schedule_fns = scenic_optax.make_schedule(config.get('schedule'))\n",
        "  tx, _ = scenic_optax.make(config.optimizer, schedule_fns, params)\n",
        "  opt_state = tx.init(params)\n",
        "\n",
        "  # Split rng for train state.\n",
        "  rng, train_rng = jax.random.split(rng)  # pylint: disable=unused-variable\n",
        "\n",
        "  # Create chrono class to track and store training statistics and metadata:\n",
        "  chrono = train_utils.Chrono()\n",
        "\n",
        "  # Create new / empty train_state.\n",
        "  train_state = train_utils.TrainState(\n",
        "      global_step=0,\n",
        "      opt_state=opt_state,\n",
        "      tx=tx,\n",
        "      params=params,\n",
        "      model_state=model_state,\n",
        "      rng=train_rng,\n",
        "      metadata={'chrono': chrono.save()},\n",
        "  )\n",
        "\n",
        "  return model, train_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tcVArQ2GUIdq"
      },
      "outputs": [],
      "source": [
        "# @title XM and Config Setup\n",
        "\n",
        "# XM job to load ckpt from\n",
        "xid, wid = (134755220, 5)\n",
        "\n",
        "# Get config\n",
        "config = get_config()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ITyvNhWznsN"
      },
      "source": [
        "## Checkpoint Restoration\n",
        "\n",
        "We explore three methods:\n",
        "\n",
        "1. This is the currently used method which does not fully restore the checkpoint state. The `output_projection` parameters are ignored.\n",
        "2. The currently implemented loading method - used for `generative_eval`.\n",
        "This method works but can be better formalized.\n",
        "3. Final resulting method - this will be implemented in mae trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jIpFT2hFZbwV"
      },
      "outputs": [],
      "source": [
        "# @title Checkpoint 1\n",
        "\n",
        "# Init model\n",
        "model, train_state = init_model(config, input_shape=[8, 300, 30, 1])\n",
        "\n",
        "# Init from XM (XID, WID)\n",
        "restored_model_cfg, init_checkpoint_path = xm_utils.get_info_from_xmanager(\n",
        "    xid, wid\n",
        ")\n",
        "restored_train_state = pretrain_utils.restore_pretrained_checkpoint(\n",
        "    init_checkpoint_path, train_state, assert_exist=True\n",
        ")\n",
        "cp_restored_train_state = copy.deepcopy(restored_train_state)\n",
        "\n",
        "# Update train state\n",
        "train_state = model.init_from_train_state(  # pytype: disable=attribute-error\n",
        "    train_state, restored_train_state, restored_model_cfg\n",
        ")\n",
        "\n",
        "# Check Same-ness\n",
        "original_params = cp_restored_train_state.params\n",
        "restored_params = train_state.params\n",
        "\n",
        "raw_params = flax.core.unfreeze(original_params)\n",
        "restored_params = flax.core.unfreeze(restored_params)\n",
        "compare_params(restored_params, raw_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "G6QrZVaoMuIC"
      },
      "outputs": [],
      "source": [
        "# @title Method 2\n",
        "\n",
        "def flatten_restored_params(raw_params, restored_params):\n",
        "  \"\"\"Removes process dimension from restored params.\"\"\"\n",
        "  for key, value in raw_params.items():\n",
        "    if isinstance(value, dict):\n",
        "      # If value is a dict, ensure the second dict has this key and recurse.\n",
        "      if key in restored_params and isinstance(restored_params[key], dict):\n",
        "        flatten_restored_params(value, restored_params[key])\n",
        "    else:\n",
        "      if key in restored_params:\n",
        "        if raw_params[key].shape != restored_params[key].shape:\n",
        "          # restored_params[key] = value[0]\n",
        "          restored_params[key] = restored_params[key][0]\n",
        "          print(key)\n",
        "\n",
        "\n",
        "# Init model\n",
        "model, train_state = init_model(config, input_shape=[8, 300, 30, 1])\n",
        "\n",
        "# Init from XM (XID, WID)\n",
        "restored_model_cfg, init_checkpoint_path = xm_utils.get_info_from_xmanager(\n",
        "    xid, wid\n",
        ")\n",
        "restored_train_state = pretrain_utils.restore_pretrained_checkpoint(\n",
        "    init_checkpoint_path, train_state, assert_exist=True\n",
        ")\n",
        "cp_restored_train_state = copy.deepcopy(restored_train_state)\n",
        "\n",
        "# Update train state\n",
        "raw_params = flax.core.unfreeze(train_state.params)\n",
        "restored_params = flax.core.unfreeze(restored_train_state.params)\n",
        "flatten_restored_params(raw_params, restored_params)\n",
        "train_state = train_state.replace(params=flax.core.freeze(restored_params))  # pytype: disable=attribute-error\n",
        "\n",
        "# Check Same-ness\n",
        "original_params = cp_restored_train_state.params\n",
        "restored_params = train_state.params\n",
        "\n",
        "raw_params = flax.core.unfreeze(original_params)\n",
        "restored_params = flax.core.unfreeze(restored_params)\n",
        "compare_params(restored_params, raw_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ioDgiV7Cie11"
      },
      "outputs": [],
      "source": [
        "# @title Method 3\n",
        "\n",
        "def _restore_params(params, restored_params):\n",
        "  \"\"\"Removes process dimension from restored params.\"\"\"\n",
        "\n",
        "  # Iterate through model parameters.\n",
        "  for key, value in params.items():\n",
        "\n",
        "    # If key value is a param dictionary.\n",
        "    if isinstance(value, dict):\n",
        "      if key in restored_params and isinstance(restored_params[key], dict):\n",
        "        _restore_params(value, restored_params[key])  # Recurse.\n",
        "\n",
        "    # If key value is a tensor.\n",
        "    else:\n",
        "      if key in restored_params:  # If key in restored params\n",
        "        params_shape = params[key].shape\n",
        "        restored_shape = restored_params[key].shape\n",
        "\n",
        "        # Transferable shape (same shape).\n",
        "        if params_shape == restored_shape:\n",
        "          params[key] = restored_params[key]\n",
        "\n",
        "        # Transferable shape (restored params have leading process dim).\n",
        "        elif params_shape == restored_shape[1:] and restored_shape[0] == 1:\n",
        "          params[key] = restored_params[key][0]\n",
        "\n",
        "        # Non-transferable shape (different shape).\n",
        "        else:\n",
        "          raise ValueError(\n",
        "              f'Unable to restore {key} from restored params of shape'\n",
        "              f'{restored_shape} to params of shape {params_shape}'\n",
        "          )\n",
        "\n",
        "\n",
        "def restore_from_train_state(train_state, restored_train_state):\n",
        "\n",
        "  # Get parameters from trainstate and unfreeze (to traverse and modify).\n",
        "  params = flax.core.unfreeze(train_state.params)\n",
        "  restored_params = flax.core.unfreeze(restored_train_state.params)\n",
        "\n",
        "  # Restore params from restored_params (done in-place).\n",
        "  _restore_params(params, restored_params)\n",
        "\n",
        "  # Update train_state parameters and return.\n",
        "  train_state = train_state.replace(params=flax.core.freeze(params))  # pytype: disable=attribute-error\n",
        "  return train_state\n",
        "\n",
        "\n",
        "# Init model\n",
        "model, train_state = init_model(config, input_shape=[8, 300, 30, 1])\n",
        "\n",
        "# Init from XM (XID, WID)\n",
        "restored_model_cfg, init_checkpoint_path = xm_utils.get_info_from_xmanager(\n",
        "    xid, wid\n",
        ")\n",
        "restored_train_state = pretrain_utils.restore_pretrained_checkpoint(\n",
        "    init_checkpoint_path, train_state, assert_exist=True\n",
        ")\n",
        "cp_restored_train_state = copy.deepcopy(restored_train_state)\n",
        "\n",
        "# Update train state\n",
        "train_state = restore_from_train_state(train_state, restored_train_state)\n",
        "\n",
        "# Check Same-ness\n",
        "original_params = cp_restored_train_state.params\n",
        "restored_params = train_state.params\n",
        "\n",
        "raw_params = flax.core.unfreeze(original_params)\n",
        "restored_params = flax.core.unfreeze(restored_params)\n",
        "compare_params(restored_params, raw_params)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
