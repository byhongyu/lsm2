{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlKtvbEuhUIK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from google3.pyglib import gfile\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyICfsundXrv"
      },
      "outputs": [],
      "source": [
        "SAMPLES_PER_DATASET = 2000\n",
        "DEFAULT_DATA_ROOT = '/namespace/fitbit-medical-sandboxes/jg/partner/encrypted/chr-ards-fitbit-prod-research/deid/exp/dmcduff/ttl=52w/lsm_v2/datasets/tfds_test'\n",
        "DATASET_NAME = 'lsm_v2_missing_balanced_20250301_valid_dataset_bounded_50p'\n",
        "DATA_CLASS = 'LsmMissingBalanced'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_pt6OouQajk"
      },
      "outputs": [],
      "source": [
        "# @title Creating concatenated_dataset\n",
        "\n",
        "def load_dataset(dataset_path: str, use_shuffle: bool, sample_size: int):\n",
        "  \"\"\"Loads, inspects, and visualizes a subset of a TensorFlow dataset.\n",
        "\n",
        "  Args:\n",
        "    dataset_path: The dataset path of the specific dataset.\n",
        "    use_shuffle: Whether to shuffle the dataset.\n",
        "    sample_size: The number of samples to visualize.\n",
        "\n",
        "  Returns:\n",
        "    valid_data: The validation data.\n",
        "  \"\"\"\n",
        "  data_valid = tfds.load(\n",
        "      'lsm',\n",
        "      data_dir=dataset_path,\n",
        "      split='valid',\n",
        "      shuffle_files=False,\n",
        "  )\n",
        "  if use_shuffle:\n",
        "    data_valid = data_valid.shuffle(100000)\n",
        "  data_valid = data_valid.take(sample_size)\n",
        "  return data_valid\n",
        "\n",
        "\n",
        "all_paths = gfile.Glob(\n",
        "    f'{DEFAULT_DATA_ROOT}/lsm_v2_test_sessions_-1_windowsize_1440_sensorfeatures_26_validonly_True_missingratio*'\n",
        ")[:3]\n",
        "all_tf_ds = []\n",
        "for path in all_paths:\n",
        "  print(path)\n",
        "  all_tf_ds.append(load_dataset(path, False, SAMPLES_PER_DATASET))\n",
        "\n",
        "concatenated_dataset = all_tf_ds[0]\n",
        "for ds in all_tf_ds[1:]:\n",
        "  concatenated_dataset = concatenated_dataset.concatenate(ds)\n",
        "print('Concated Length: ', len(concatenated_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxSltkaOQamU"
      },
      "outputs": [],
      "source": [
        "# @title Save TFDS by inheriting dataset from prior datasets\n",
        "\n",
        "# Define the output directory for the new dataset\n",
        "output_dir = f'{DEFAULT_DATA_ROOT}/{DATASET_NAME}'\n",
        "\n",
        "\n",
        "class LsmMissingBalanced(tfds.core.GeneratorBasedBuilder):\n",
        "  VERSION = tfds.core.Version('2.0.0')\n",
        "\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    # Initialize with the concatenated dataset passed as argument\n",
        "    super().__init__(*args, **kwargs)\n",
        "\n",
        "  def _info(self):\n",
        "\n",
        "    original_builder = tfds.builder('lsm', data_dir=all_paths[-1])\n",
        "    original_info = original_builder.info\n",
        "\n",
        "    updated_split_info = tfds.core.SplitInfo(\n",
        "        name='valid',\n",
        "        shard_lengths=[len(concatenated_dataset)],  # Update num_examples\n",
        "        num_bytes=original_info.splits[\n",
        "            'valid'\n",
        "        ].num_bytes,  # Preserve other fields\n",
        "    )\n",
        "\n",
        "    # Update the splits in the DatasetInfo\n",
        "    splits_dict = {}\n",
        "    splits_dict['valid'] = updated_split_info\n",
        "    # Build and return a new DatasetInfo instance with the updated splits\n",
        "    return tfds.core.DatasetInfo(\n",
        "        builder=self,\n",
        "        description=original_info.description,\n",
        "        features=original_info.features,\n",
        "        homepage=original_info.homepage,\n",
        "        split_dict=splits_dict,  # Use the SplitDict here\n",
        "        supervised_keys=original_info.supervised_keys,\n",
        "        # version=self.VERSION,\n",
        "        citation=original_info.citation,\n",
        "    )\n",
        "\n",
        "  def _split_generators(self, dl_manager):\n",
        "    # Return a split generator for the validation set (or whatever split you need)\n",
        "    return [\n",
        "        tfds.core.SplitGenerator(\n",
        "            name='valid',\n",
        "            gen_kwargs={\n",
        "                'dataset': concatenated_dataset\n",
        "            },  # Use the concatenated dataset here\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "  def _generate_examples(self, dataset):\n",
        "    # Ensure that tensors are converted to np.array compatible formats\n",
        "    for idx, example in enumerate(dataset):\n",
        "      # Convert tensors to np.array or native Python types\n",
        "      example_dict = {\n",
        "          key: self._convert_tensor_to_native(val)\n",
        "          for key, val in example.items()\n",
        "      }\n",
        "      yield idx, example_dict\n",
        "\n",
        "  def _convert_tensor_to_native(self, tensor):\n",
        "    if isinstance(tensor, tf.Tensor):\n",
        "      return tensor.numpy()  # Convert TensorFlow tensor to a NumPy array\n",
        "    elif isinstance(tensor, np.ndarray):\n",
        "      return tensor  # Already in compatible format\n",
        "    else:\n",
        "      return tensor  # If it's a native Python type (e.g., float, int), return it as is\n",
        "\n",
        "\n",
        "# Function to save the concatenated dataset using the custom builder\n",
        "def save_tfds_dataset(output_dir):\n",
        "  builder = LsmMissingBalanced(\n",
        "      data_dir=output_dir\n",
        "  )  # Pass dataset to the builder\n",
        "  builder.download_and_prepare()\n",
        "  print(f'Dataset saved at: {output_dir}')\n",
        "\n",
        "\n",
        "# Save the concatenated dataset\n",
        "\n",
        "save_tfds_dataset(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf-k5L1G_xa_"
      },
      "outputs": [],
      "source": [
        "labels = [\n",
        "    'HR',\n",
        "    'eda_level_real',\n",
        "    'leads_contact_counts',\n",
        "    'steps',\n",
        "    'jerk_auto',\n",
        "    'log_energy',\n",
        "    'covariance',\n",
        "    'log_energy_ratio',\n",
        "    'zero_crossing_std',\n",
        "    'zero_crossing_avg',\n",
        "    'axis_mean',\n",
        "    'altim_std',\n",
        "    'kurtosis',\n",
        "    'sleep_coefficient',\n",
        "    'wrist_temperatures',\n",
        "    'hrv_shannon_entropy_rr',\n",
        "    'hrv_shannon_entropy_rrd',\n",
        "    'ceda_slope_real_micro_siemens',\n",
        "    'rmssd_percentile_0595',\n",
        "    'sdnn_percentile_0595',\n",
        "    'hrv_percent_good',\n",
        "    'hrv_rr_80th_percentile_mean',\n",
        "    'hrv_rr_20th_percentile_mean',\n",
        "    'hrv_rr_median',\n",
        "    'hr_at_rest_mean',\n",
        "    'skin_temperature_slope',\n",
        "]\n",
        "\n",
        "def minutes_to_time(x):\n",
        "  \"\"\"Converts minutes to a time string in HH:MM format.\n",
        "\n",
        "  This function takes an integer representing a duration in minutes and\n",
        "  converts it into a formatted time string in the format \"HH:MM\" (hours and\n",
        "  minutes).\n",
        "\n",
        "  Args:\n",
        "    x: An integer representing the duration in minutes.\n",
        "\n",
        "  Returns:\n",
        "    A string representing the time in HH:MM format.\n",
        "  \"\"\"\n",
        "  hours = int(x // 60)\n",
        "  minutes = int(x % 60)\n",
        "  return f'{hours:02d}:{minutes:02d}'\n",
        "\n",
        "\n",
        "def inspect_dataset(root_data_path: str, dataset_name: str, data_class: str):\n",
        "  \"\"\"Loads, inspects, and visualizes a subset of a TensorFlow dataset.\n",
        "\n",
        "  This function loads a specified dataset using `tfds.load`, prints its length,\n",
        "  and then visualizes the first 5 samples. It extracts the 'mask' and\n",
        "  'input_signal' from each sample, converts them to NumPy arrays, and\n",
        "  uses the `visualize` function to display them.\n",
        "\n",
        "  Args:\n",
        "    root_data_path: The root directory where the dataset is located.\n",
        "    dataset_name: The name of the dataset to inspect.\n",
        "\n",
        "  Returns:\n",
        "    None. This function prints information and displays visualizations.\n",
        "  \"\"\"\n",
        "  print('Dataset Name:', dataset_name)\n",
        "  try:\n",
        "    data = tfds.load(\n",
        "        data_class,\n",
        "        data_dir=f'{root_data_path}/{dataset_name}',\n",
        "        split='train',\n",
        "        shuffle_files=False,\n",
        "    )\n",
        "    print('Number of Train Data Sample:', len(data))\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  data_valid = tfds.load(\n",
        "      data_class,\n",
        "      data_dir=f'{root_data_path}/{dataset_name}',\n",
        "      split='valid',\n",
        "      shuffle_files=False,\n",
        "  )\n",
        "  print('Number of Valid Data Sample:', len(data_valid))\n",
        "\n",
        "  for sample in data_valid.take(5):\n",
        "    mask = tf.io.parse_tensor(sample['mask'], out_type=tf.bool).numpy().T\n",
        "    sample = (\n",
        "        tf.io.parse_tensor(sample['input_signal'], out_type=tf.double).numpy().T\n",
        "    )\n",
        "\n",
        "    visualize(mask, cmap='Greys')\n",
        "    visualize(sample)\n",
        "\n",
        "    print('--------------------------')\n",
        "\n",
        "\n",
        "def visualize(\n",
        "    sample_signal_input,\n",
        "    figsize=(20, 5),\n",
        "    title='',\n",
        "    cmap='cool',\n",
        "    dim=None,\n",
        "    cbar=True,\n",
        "    disabletext=False,\n",
        "):\n",
        "  \"\"\"Visualizes a sample signal as a heatmap.\n",
        "\n",
        "  This function creates a heatmap visualization of the input sample signal.\n",
        "  It uses `seaborn.heatmap` to generate the heatmap and allows for customization\n",
        "  of figure size, title, colormap, and display options.\n",
        "\n",
        "  Args:\n",
        "    sample_signal_input: The input sample signal data as a NumPy array.\n",
        "    figsize: Tuple specifying the width and height of the figure (default: (20,\n",
        "      5)).\n",
        "    title: The title of the plot (default: '').\n",
        "    cmap: The colormap to use for the heatmap (default: 'cool').\n",
        "    dim: Optional dimension to select for visualization (default: None, displays\n",
        "      all dimensions).\n",
        "    cbar: Whether to display the colorbar (default: True).\n",
        "    disabletext: Whether to disable the plot title (default: False).\n",
        "\n",
        "  Returns:\n",
        "    None. This function displays the heatmap visualization.\n",
        "  \"\"\"\n",
        "\n",
        "  if dim is not None:\n",
        "    sample_signal_input = sample_signal_input[[dim], :]\n",
        "    labels_temp = []\n",
        "  else:\n",
        "    labels_temp = labels\n",
        "\n",
        "  plt.figure(figsize=figsize)\n",
        "  ax1 = plt.subplot2grid((1, 12), (0, 0), colspan=12)\n",
        "  ax1 = sns.heatmap(\n",
        "      sample_signal_input,\n",
        "      cmap=cmap,\n",
        "      cbar=cbar,\n",
        "      linewidths=0.0,\n",
        "      linecolor='black',\n",
        "      alpha=0.8,\n",
        "      ax=ax1,\n",
        "      yticklabels=labels_temp,\n",
        "  )\n",
        "\n",
        "  for tick in ax1.get_xticklabels():\n",
        "    tick.set_fontname('Ubuntu')\n",
        "  ax1.tick_params(axis='x', labelsize=10.5)\n",
        "\n",
        "  for tick in ax1.get_yticklabels():\n",
        "    tick.set_fontname('Ubuntu')\n",
        "  ax1.tick_params(axis='y', labelsize=10.5)\n",
        "\n",
        "  # Set x-axis ticks every 4 hours\n",
        "  tick_interval = 4 * 60  # 4 hours in minutes\n",
        "  xticks = np.arange(0, sample_signal_input.shape[1], tick_interval)\n",
        "  xtick_labels = [minutes_to_time(x) for x in xticks]\n",
        "  ax1.set_xticks(xticks)\n",
        "  ax1.set_xticklabels(xtick_labels, rotation=45, ha='right')\n",
        "  # ax1.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
        "\n",
        "  plt.tight_layout()\n",
        "\n",
        "  if not disabletext:\n",
        "    plt.title(title)\n",
        "\n",
        "  for i in np.arange(0, sample_signal_input.shape[1], 60):\n",
        "    if i % (60 * 24) == 0:\n",
        "      tempwidth, tempalpha = 2, 1\n",
        "    else:\n",
        "      tempwidth, tempalpha = 1, 0.4\n",
        "    ax1.axvline(x=i, color='k', alpha=tempalpha, linewidth=tempwidth)\n",
        "\n",
        "  for i in np.arange(0, sample_signal_input.shape[0] + 1, 1):\n",
        "    ax1.axhline(y=i, color='k', alpha=0.4, linewidth=1)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "inspect_dataset(DEFAULT_DATA_ROOT, DATASET_NAME, DATA_CLASS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HedmydnKC5oP"
      },
      "outputs": [],
      "source": [
        "# @title Check the distribution of missing ratio\n",
        "\n",
        "data_valid = tfds.load(\n",
        "    DATA_CLASS,\n",
        "    data_dir=f'{DEFAULT_DATA_ROOT}/{DATASET_NAME}',\n",
        "    split='valid',\n",
        "    shuffle_files=False,\n",
        ")\n",
        "print('Number of Valid Data Sample:', len(data_valid))\n",
        "\n",
        "missing_ratios_all = []\n",
        "for sample in data_valid.take(len(data_valid)):\n",
        "  missing_ratio = sample['missingness_ratio'].numpy()\n",
        "  missing_ratios_all.append(missing_ratio)\n",
        "\n",
        "bins = np.arange(0.2, 0.8, 0.1)\n",
        "sns.histplot(missing_ratios_all, bins=bins, kde=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//experimental/health_foundation_models/colab:colab_deps",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/medical/waveforms/modelling/lsm/datasets/lsm/colabs/curate_lsm_v2_missing_balanced_dataset.ipynb",
          "timestamp": 1743027868017
        },
        {
          "file_id": "/piper/depot/google3/medical/waveforms/modelling/lsm/datasets/lsm/colabs/curate_lsm_v2_missing_balanced_dataset.ipynb",
          "timestamp": 1741632559510
        },
        {
          "file_id": "1a56JmOHvyMpAuXJ3U3qMfE1RLYoo7aZ9",
          "timestamp": 1740887365138
        },
        {
          "file_id": "1IMOd0dwEfxoZ4i1m4GoH4w_uHQoAgbS_",
          "timestamp": 1740785710922
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
