{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ojx9U3lSg9k"
      },
      "source": [
        "1. Please do [AoD](https://grants.corp.google.com/#/grants?request=20h%2Fchr-ards-fitbit-prod-research-deid-eng-team:r\u0026reason=%22b%2F285178698%22) before running this colab.\n",
        "2. Use any Borg runtime kernels named after `Fitbit Prod Research`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95s6ictnHEZO"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "from google3.pyglib import gfile\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "from scipy.signal import spectrogram\n",
        "from collections import defaultdict\n",
        "from statsmodels.tsa.stattools import acf\n",
        "import pandas as pd\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRcQ9Dq5Qevs"
      },
      "outputs": [],
      "source": [
        "DEFAULT_DATA_ROOT = '/namespace/fitbit-medical-sandboxes/jg/partner/encrypted/chr-ards-fitbit-prod-research/deid/exp/dmcduff/ttl=52w/lsm_v2/datasets/tfds'  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvhALc2d2wh9"
      },
      "outputs": [],
      "source": [
        "#@title Check the paths\n",
        "\n",
        "latest_dataset_paths = gfile.Glob(f'{DEFAULT_DATA_ROOT}/*')\n",
        "latest_dataset_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PVo2BK5R-hu"
      },
      "outputs": [],
      "source": [
        "# @title Utils\n",
        "\n",
        "# for the V2 dataset, these are the modality labels\n",
        "labels = [\n",
        "    'HR',\n",
        "    'eda_level_real',\n",
        "    'leads_contact_counts',\n",
        "    'steps',\n",
        "    'jerk_auto',\n",
        "    'step_count',\n",
        "    'log_energy',\n",
        "    'covariance',\n",
        "    'log_energy_ratio',\n",
        "    'zero_crossing_std',\n",
        "    'zero_crossing_avg',\n",
        "    'axis_mean',\n",
        "    'altim_std',\n",
        "    'kurtosis',\n",
        "    'sleep_coefficient',\n",
        "    'wrist_temperatures',\n",
        "    'hrv_shannon_entropy_rr',\n",
        "    'hrv_shannon_entropy_rrd',\n",
        "    'hrv_percentage_of_nn_30',\n",
        "    'ceda_magnitude_real_micro_siemens',\n",
        "    'ceda_slope_real_micro_siemens',\n",
        "    'rmssd_percentile_0595',\n",
        "    'sdnn_percentile_0595',\n",
        "    'msa_probability',\n",
        "    'hrv_percent_good',\n",
        "    'hrv_rr_80th_percentile_mean',\n",
        "    'hrv_rr_20th_percentile_mean',\n",
        "    'hrv_rr_median',\n",
        "    'hrv_rr_mean',\n",
        "    'hr_at_rest_mean',\n",
        "    'skin_temperature_magnitude',\n",
        "    'skin_temperature_slope',\n",
        "]\n",
        "\n",
        "# These are each of the new missingness groups for the v2 dataset\n",
        "# arbitrary set one item of the group to be the key\n",
        "# the value is another dictionary, with name being name of the missingness group\n",
        "# and members being the name of the modalities inside of group\n",
        "missgroup_dict = {\n",
        "    'HR': {'name': 'HR', 'members': ['HR']},\n",
        "    'steps': {'name': 'steps', 'members': ['steps']},\n",
        "    'wrist_temperatures': {\n",
        "        'name': 'wrist_temperatures',\n",
        "        'members': ['wrist_temperatures'],\n",
        "    },\n",
        "    'sleep_coefficient': {\n",
        "        'name': 'sleep_coefficient',\n",
        "        'members': ['sleep_coefficient', 'is_on_wrist'],\n",
        "    },\n",
        "    'eda_level_real': {\n",
        "        'name': 'EDA Sensor',\n",
        "        'members': [\n",
        "            'eda_level_real',\n",
        "            'eda_level_imaginary',\n",
        "            'eda_slope_real',\n",
        "            'eda_slope_imaginary',\n",
        "            'leads_contact_counts',\n",
        "        ],\n",
        "    },\n",
        "    'jerk_auto': {\n",
        "        'name': 'ACC Sensor',\n",
        "        'members': [\n",
        "            'jerk_auto',\n",
        "            'step_count',\n",
        "            'log_energy',\n",
        "            'covariance',\n",
        "            'log_energy_ratio',\n",
        "            'zero_crossing_std',\n",
        "            'zero_crossing_avg',\n",
        "            'axis_mean',\n",
        "            'altim_std',\n",
        "            'kurtosis',\n",
        "        ],\n",
        "    },\n",
        "    'hrv_shannon_entropy_rr': {\n",
        "        'name': 'HRV',\n",
        "        'members': [\n",
        "            'hrv_shannon_entropy_rr',\n",
        "            'hrv_shannon_entropy_rrd',\n",
        "            'hrv_percentage_of_nn_30',\n",
        "            'rmssd_percentile_0595',\n",
        "            'sdnn_percentile_0595',\n",
        "            'hrv_rr_80th_percentile_mean',\n",
        "            'hrv_rr_20th_percentile_mean',\n",
        "            'hrv_rr_median',\n",
        "            'hrv_rr_mean',\n",
        "        ],\n",
        "    },\n",
        "    'ceda_magnitude_real_micro_siemens': {\n",
        "        'name': 'CEDA Magnitude',\n",
        "        'members': [\n",
        "            'ceda_magnitude_real_micro_siemens',\n",
        "            'hrv_percent_good',\n",
        "            'skin_temperature_magnitude',\n",
        "        ],\n",
        "    },\n",
        "    'ceda_slope_real_micro_siemens': {\n",
        "        'name': 'ceda_slope_real_micro_siemens',\n",
        "        'members': [\n",
        "            'msa_probability',\n",
        "        ],\n",
        "    },\n",
        "    'msa_probability': {\n",
        "        'name': 'msa_probability',\n",
        "        'members': ['msa_probability'],\n",
        "    },\n",
        "    'hr_at_rest_mean': {\n",
        "        'name': 'hr_at_rest_mean',\n",
        "        'members': ['hr_at_rest_mean'],\n",
        "    },\n",
        "    'skin_temperature_slope': {\n",
        "        'name': 'skin_temperature_slope',\n",
        "        'members': ['skin_temperature_slope'],\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "def check_dataset_length(root_data_path: str, dataset_name: str) -\u003e str:\n",
        "  \"\"\"Checks and prints the length of a dataset.\n",
        "\n",
        "  This function takes the root data path and the dataset name as input.\n",
        "  It then uses gfile.Glob to find all the data samples within the dataset\n",
        "  and prints the total number of samples found.\n",
        "\n",
        "  Args:\n",
        "    root_data_path: The root directory where the dataset is located.\n",
        "    dataset_name: The name of the dataset to check.\n",
        "\n",
        "  Returns:\n",
        "      None. This function prints the dataset name and the number of data\n",
        "      samples.\n",
        "  \"\"\"\n",
        "  print('Dataset Name:', dataset_name)\n",
        "  print(\n",
        "      'Number of Data Sample:',\n",
        "      len(gfile.Glob(f'{root_data_path}/{dataset_name}/lsm/*/*')),\n",
        "  )\n",
        "\n",
        "\n",
        "def inspect_dataset(root_data_path: str, dataset_name: str):\n",
        "  \"\"\"Loads, inspects, and visualizes a subset of a TensorFlow dataset.\n",
        "\n",
        "  This function loads a specified dataset using `tfds.load`, prints its length,\n",
        "  and then visualizes the first 5 samples. It extracts the 'mask' and\n",
        "  'input_signal' from each sample, converts them to NumPy arrays, and\n",
        "  uses the `visualize` function to display them.\n",
        "\n",
        "  Args:\n",
        "    root_data_path: The root directory where the dataset is located.\n",
        "    dataset_name: The name of the dataset to inspect.\n",
        "\n",
        "  Returns:\n",
        "    None. This function prints information and displays visualizations.\n",
        "  \"\"\"\n",
        "  print('Dataset Name:', dataset_name)\n",
        "  data = tfds.load(\n",
        "      'lsm',\n",
        "      data_dir=f'{root_data_path}/{dataset_name}',\n",
        "      split='train',\n",
        "      shuffle_files=False,\n",
        "  )\n",
        "  print('Number of Data Sample:', len(data))\n",
        "  for sample in data.take(5):\n",
        "    mask = tf.io.parse_tensor(sample['mask'], out_type=tf.bool).numpy().T\n",
        "    sample = (\n",
        "        tf.io.parse_tensor(sample['input_signal'], out_type=tf.double).numpy().T\n",
        "    )\n",
        "\n",
        "    visualize(mask, cmap='Greys')\n",
        "    visualize(sample)\n",
        "\n",
        "    print('--------------------------')\n",
        "\n",
        "\n",
        "def visualize(\n",
        "    sample_signal_input,\n",
        "    figsize=(20, 5),\n",
        "    title='',\n",
        "    cmap='cool',\n",
        "    dim=None,\n",
        "    cbar=True,\n",
        "    disabletext=False,\n",
        "):\n",
        "  \"\"\"Visualizes a sample signal as a heatmap.\n",
        "\n",
        "  This function creates a heatmap visualization of the input sample signal.\n",
        "  It uses `seaborn.heatmap` to generate the heatmap and allows for customization\n",
        "  of figure size, title, colormap, and display options.\n",
        "\n",
        "  Args:\n",
        "    sample_signal_input: The input sample signal data as a NumPy array.\n",
        "    figsize: Tuple specifying the width and height of the figure (default: (20,\n",
        "      5)).\n",
        "    title: The title of the plot (default: '').\n",
        "    cmap: The colormap to use for the heatmap (default: 'cool').\n",
        "    dim: Optional dimension to select for visualization (default: None, displays\n",
        "      all dimensions).\n",
        "    cbar: Whether to display the colorbar (default: True).\n",
        "    disabletext: Whether to disable the plot title (default: False).\n",
        "\n",
        "  Returns:\n",
        "    None. This function displays the heatmap visualization.\n",
        "  \"\"\"\n",
        "\n",
        "  if dim is not None:\n",
        "    sample_signal_input = sample_signal_input[[dim], :]\n",
        "    labels_temp = []\n",
        "  else:\n",
        "    labels_temp = labels\n",
        "\n",
        "  plt.figure(figsize=figsize)\n",
        "  ax1 = plt.subplot2grid((1, 12), (0, 0), colspan=12)\n",
        "  ax1 = sns.heatmap(\n",
        "      sample_signal_input,\n",
        "      cmap=cmap,\n",
        "      cbar=cbar,\n",
        "      linewidths=0.0,\n",
        "      linecolor='black',\n",
        "      alpha=0.8,\n",
        "      ax=ax1,\n",
        "      yticklabels=labels_temp,\n",
        "  )\n",
        "\n",
        "  for tick in ax1.get_xticklabels():\n",
        "    tick.set_fontname('Ubuntu')\n",
        "  ax1.tick_params(axis='x', labelsize=10.5)\n",
        "\n",
        "  for tick in ax1.get_yticklabels():\n",
        "    tick.set_fontname('Ubuntu')\n",
        "  ax1.tick_params(axis='y', labelsize=10.5)\n",
        "\n",
        "  # Set x-axis ticks every 4 hours\n",
        "  tick_interval = 4 * 60  # 4 hours in minutes\n",
        "  xticks = np.arange(0, sample_signal_input.shape[1], tick_interval)\n",
        "  xtick_labels = [minutes_to_time(x) for x in xticks]\n",
        "  ax1.set_xticks(xticks)\n",
        "  ax1.set_xticklabels(xtick_labels, rotation=45, ha='right')\n",
        "  # ax1.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
        "\n",
        "  plt.tight_layout()\n",
        "\n",
        "  if not disabletext:\n",
        "    plt.title(title)\n",
        "\n",
        "  for i in np.arange(0, sample_signal_input.shape[1], 60):\n",
        "    if i % (60 * 24) == 0:\n",
        "      tempwidth, tempalpha = 2, 1\n",
        "    else:\n",
        "      tempwidth, tempalpha = 1, 0.4\n",
        "    ax1.axvline(x=i, color='k', alpha=tempalpha, linewidth=tempwidth)\n",
        "\n",
        "  for i in np.arange(0, sample_signal_input.shape[0] + 1, 1):\n",
        "    ax1.axhline(y=i, color='k', alpha=0.4, linewidth=1)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def consecutive_ones_lengths(mask):\n",
        "  \"\"\"Calculates the lengths of consecutive sequences of 1s in a binary mask.\n",
        "\n",
        "  This function identifies and measures the lengths of continuous stretches of\n",
        "  1s within\n",
        "  a given binary mask (an array consisting of 0s and 1s). It works by:\n",
        "\n",
        "  1. Finding the points where the mask transitions from 0 to 1 (start of a\n",
        "  sequence)\n",
        "     and from 1 to 0 (end of a sequence) using `np.diff`.\n",
        "  2. Using `np.where` to get the indices of these transitions.\n",
        "  3. Calculating the length of each sequence by subtracting the start index from\n",
        "  the end index.\n",
        "\n",
        "  It's particularly useful for analyzing patterns or gaps in data represented by\n",
        "  such masks.\n",
        "\n",
        "  Args:\n",
        "    mask: A 1-D NumPy array representing the binary mask.\n",
        "\n",
        "  Returns:\n",
        "    A 1-D NumPy array containing the lengths of each consecutive sequence of 1s\n",
        "    found\n",
        "    in the input mask.\n",
        "  \"\"\"\n",
        "\n",
        "  # Find where the mask changes value\n",
        "  diff = np.diff(mask, prepend=0, append=0)\n",
        "\n",
        "  # Start and end indices of sequences of ones\n",
        "  starts = np.where(diff == 1)[0]\n",
        "  ends = np.where(diff == -1)[0]\n",
        "\n",
        "  # Calculate lengths of each sequence of ones\n",
        "  lengths = ends - starts\n",
        "  return lengths\n",
        "\n",
        "\n",
        "def minutes_to_time(x):\n",
        "  \"\"\"Converts minutes to a time string in HH:MM format.\n",
        "\n",
        "  This function takes an integer representing a duration in minutes and\n",
        "  converts it into a formatted time string in the format \"HH:MM\" (hours and\n",
        "  minutes).\n",
        "\n",
        "  Args:\n",
        "    x: An integer representing the duration in minutes.\n",
        "\n",
        "  Returns:\n",
        "    A string representing the time in HH:MM format.\n",
        "  \"\"\"\n",
        "  hours = int(x // 60)\n",
        "  minutes = int(x % 60)\n",
        "  return f'{hours:02d}:{minutes:02d}'\n",
        "\n",
        "\n",
        "class StopExecution(Exception):\n",
        "  \"\"\"Custom exception used to halt the execution of a cell or process.\n",
        "\n",
        "  This exception is designed to stop the execution flow without displaying\n",
        "  a traceback. It is useful for scenarios where you want to terminate\n",
        "  a process prematurely but avoid cluttering the output with unnecessary\n",
        "  traceback information.\n",
        "\n",
        "  Attributes: None\n",
        "\n",
        "  Methods:\n",
        "      _render_traceback_: Overrides the default traceback rendering to\n",
        "          suppress the traceback output.\n",
        "  \"\"\"\n",
        "\n",
        "  def _render_traceback_(self):\n",
        "    return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg_ww6vvN60b"
      },
      "outputs": [],
      "source": [
        "# @title Check the Data Length\n",
        "\n",
        "check_dataset_length(DEFAULT_DATA_ROOT, 'lsm_v2_pretraining_n_200000_300m')\n",
        "check_dataset_length(DEFAULT_DATA_ROOT, 'lsm_v2_pretraining_n_200000_1440m')\n",
        "check_dataset_length(DEFAULT_DATA_ROOT, 'lsm_v2_pretraining_n_200000_10080m')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SMWkuMoHQRp"
      },
      "outputs": [],
      "source": [
        "# @title Inspect the 5-hour Data\n",
        "\n",
        "inspect_dataset(DEFAULT_DATA_ROOT, 'lsm_v2_pretraining_n_200000_300m')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAansOLZSJF-"
      },
      "outputs": [],
      "source": [
        "# @title Inspect the Daily Data\n",
        "\n",
        "inspect_dataset(DEFAULT_DATA_ROOT, 'lsm_v2_pretraining_n_200000_1440m')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c59kdHbd1ArS"
      },
      "outputs": [],
      "source": [
        "#@title Inspect the Weekly Data\n",
        "\n",
        "inspect_dataset(DEFAULT_DATA_ROOT, 'lsm_v2_pretraining_n_200000_10080m')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIpvzd15Pqz3"
      },
      "source": [
        "# Collect Aggregate Statistics for Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f-qHc88Vlnd"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = \"lsm_v2_pretraining_n_200000_10080m\" #@param\n",
        "NUM_PPL = 1_000 #@param\n",
        "TOTAL_TIME_TIMESERIES = 24*60*7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1rpIQFYWMVD"
      },
      "outputs": [],
      "source": [
        "MISSGAP_VIZ = True #@param\n",
        "SPECTROGRAM_VIZ = True #@param\n",
        "ACF_VIZ = True #@param\n",
        "MISSCORR_VIZ = True #@param\n",
        "FULLDATA_VIZ = True #@param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZIezQX0bUtn"
      },
      "outputs": [],
      "source": [
        "##### Can be killed early if you want to #####\n",
        "\n",
        "NUM_CHANNELS = len(labels)\n",
        "TIME_LEN = TOTAL_TIME_TIMESERIES\n",
        "DAYS_INWEEK = 7\n",
        "\n",
        "# dataset creation\n",
        "print('Dataset Name:', DATASET_NAME)\n",
        "data = tfds.load(\n",
        "    'lsm',\n",
        "    data_dir=f'{DEFAULT_DATA_ROOT}/{DATASET_NAME}',\n",
        "    split='train',\n",
        "    shuffle_files=False,\n",
        ")\n",
        "\n",
        "missgroupkeys_set = set(missgroup_dict.keys())\n",
        "\n",
        "\n",
        "# aggregate statistics\n",
        "missgaps_all = defaultdict(list)\n",
        "Sxx_all = np.zeros((NUM_CHANNELS, 31, 47))\n",
        "acfday_all = defaultdict(list)\n",
        "acfweek_all = defaultdict(list)\n",
        "misscorr_all = np.zeros((len(labels), len(labels)))\n",
        "validcorrcounts_all = np.zeros((len(labels), len(labels)))\n",
        "sum_all = np.zeros((NUM_CHANNELS, TIME_LEN))\n",
        "notmissamt_all = np.zeros((NUM_CHANNELS, TIME_LEN))\n",
        "\n",
        "\n",
        "for sample in tqdm(data.take(NUM_PPL)):\n",
        "  mask = (\n",
        "      tf.io.parse_tensor(sample['mask'], out_type=tf.bool)\n",
        "      .numpy()\n",
        "      .T.reshape(NUM_CHANNELS, DAYS_INWEEK, TIME_LEN // DAYS_INWEEK)\n",
        "  )\n",
        "  signal = (\n",
        "      tf.io.parse_tensor(sample['input_signal'], out_type=tf.double)\n",
        "      .numpy()\n",
        "      .T.reshape(NUM_CHANNELS, DAYS_INWEEK, TIME_LEN // DAYS_INWEEK)\n",
        "  )\n",
        "  for i in range(NUM_CHANNELS):\n",
        "    for j in range(7):\n",
        "      ### Calculate Missingness Gap Lengths for specific missgroup\n",
        "      if MISSGAP_VIZ:\n",
        "        if labels[i] in missgroupkeys_set:\n",
        "          mask_input = mask[i, j, :]\n",
        "          missgaps_all[labels[i]].append(consecutive_ones_lengths(mask_input))\n",
        "\n",
        "      signal_channel = signal[i, j, :]\n",
        "\n",
        "      ### Calculate Spectrogram for specific channel\n",
        "      if SPECTROGRAM_VIZ:\n",
        "        f, t, Sxx = spectrogram(signal_channel, nperseg=60, noverlap=30)\n",
        "        Sxx_all[i, :] += Sxx\n",
        "\n",
        "      ### Calculate ACF for day for specific channel\n",
        "      if ACF_VIZ:\n",
        "        max_lag = signal_channel.shape[-1] // 2\n",
        "        acf_result = acf(signal_channel, nlags=max_lag, fft=True)\n",
        "        # Remove NaN values from ACF results\n",
        "        valid_lags = np.isfinite(acf_result)  # Identify non-NaN values\n",
        "        acf_result = acf_result[valid_lags]\n",
        "        lags = np.arange(0, max_lag + 1)[valid_lags]\n",
        "        acfday_all[labels[i]].extend([lags, acf_result])\n",
        "\n",
        "    mask_channel = mask[i].flatten()\n",
        "    signal_channel = signal[i].flatten()\n",
        "\n",
        "    ### Calculate ACF for week for specific channel\n",
        "    if ACF_VIZ:\n",
        "      max_lag = signal_channel.shape[-1] // 2\n",
        "      acf_result = acf(signal_channel, nlags=max_lag, fft=True)\n",
        "      # Remove NaN values from ACF results\n",
        "      valid_lags = np.isfinite(acf_result)  # Identify non-NaN values\n",
        "      acf_result = acf_result[valid_lags]\n",
        "      lags = np.arange(0, max_lag + 1)[valid_lags]\n",
        "      acfweek_all[labels[i]].extend([lags, acf_result])\n",
        "\n",
        "  ### Calculate Missingness Correlations\n",
        "  if MISSCORR_VIZ:\n",
        "    misscorr_temp = (\n",
        "        pd.DataFrame(mask.reshape(NUM_CHANNELS, -1).T).corr().to_numpy()\n",
        "    )\n",
        "    valid_mask = np.isfinite(misscorr_temp)\n",
        "    misscorr_all[valid_mask] += misscorr_temp[valid_mask]\n",
        "    # Increment the count of valid updates\n",
        "    validcorrcounts_all[valid_mask] += 1\n",
        "\n",
        "  ### Calculate Full Data and Missingness Heatmap\n",
        "  if FULLDATA_VIZ:\n",
        "    signal_temp = np.copy(signal)\n",
        "    signal_temp[(mask == 1)] = 0\n",
        "    sum_all += signal_temp.reshape(NUM_CHANNELS, -1)\n",
        "    notmissamt_all += ~mask.reshape(NUM_CHANNELS, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAdkm9ydKx7r"
      },
      "outputs": [],
      "source": [
        "if MISSCORR_VIZ:\n",
        "  # Calculate Missingness Correlations (Continued)\n",
        "  # Normalization and avoid division by zero\n",
        "  with np.errstate(divide='ignore', invalid='ignore'):\n",
        "    misscorr_all = np.divide(\n",
        "        misscorr_all, validcorrcounts_all\n",
        "    )  # Element-wise division\n",
        "    misscorr_all[~np.isfinite(misscorr_all)] = (\n",
        "        0  # Replace any resulting NaNs with 0\n",
        "    )\n",
        "  # Ensure diagonals are 1 (self-correlation is always 1)\n",
        "  np.fill_diagonal(misscorr_all, 1)\n",
        "\n",
        "if FULLDATA_VIZ:\n",
        "  ### Calculate Full Data and Missingness Heatmap (Continued)\n",
        "  dataheatmap_all = sum_all / notmissamt_all\n",
        "  missheatmap_all = (1 - notmissamt_all) / NUM_PPL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUd1_4sYNTQK"
      },
      "source": [
        "# Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKPPtOs7NJuA"
      },
      "outputs": [],
      "source": [
        "# @title Heatmaps of total missingness and total data (while ignoring missing)\n",
        "\n",
        "if not FULLDATA_VIZ:\n",
        "  print(\"No full data visualization requested\")\n",
        "  raise StopExecution\n",
        "visualize(missheatmap_all, cmap=\"Greys\")\n",
        "visualize(dataheatmap_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxUFBYSnJbDN"
      },
      "outputs": [],
      "source": [
        "# @title Correlation Visualizations\n",
        "\n",
        "if not MISSCORR_VIZ:\n",
        "  print(\"No missingness correlation visualization requested\")\n",
        "  raise StopExecution\n",
        "\n",
        "# Correlation visualization\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    misscorr_all,\n",
        "    fmt=\".2f\",\n",
        "    xticklabels=labels,\n",
        "    yticklabels=labels,\n",
        "    cmap=\"coolwarm\",\n",
        "    cbar=True,\n",
        "    square=True,\n",
        "    linewidths=0.5,\n",
        ")\n",
        "plt.title(\"Correlation Matrix Visualization\")\n",
        "plt.xticks(\n",
        "    rotation=45, ha=\"right\"\n",
        ")  # Rotate x-axis labels for better readability\n",
        "plt.yticks(rotation=0)  # Keep y-axis labels horizontal\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Redo correlation visualization with perfect correlation of 1 to be black\n",
        "plt.figure(figsize=(10, 8))\n",
        "ax = sns.heatmap(\n",
        "    misscorr_all,\n",
        "    fmt=\".2f\",\n",
        "    xticklabels=labels,\n",
        "    yticklabels=labels,\n",
        "    cmap=\"coolwarm\",\n",
        "    cbar=True,\n",
        "    square=True,\n",
        "    linewidths=0.5,\n",
        ")\n",
        "plt.title(\"Correlation Matrix Visualization\")\n",
        "plt.xticks(\n",
        "    rotation=45, ha=\"right\"\n",
        ")  # Rotate x-axis labels for better readability\n",
        "plt.yticks(rotation=0)  # Keep y-axis labels horizontal\n",
        "plt.tight_layout()\n",
        "# Overlay black color for values of 1\n",
        "for (i, j), value in np.ndenumerate(misscorr_all):\n",
        "  if value == 1:\n",
        "    ax.add_patch(plt.Rectangle((j, i), 1, 1, color=\"black\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iA3fGjVmHsoB"
      },
      "outputs": [],
      "source": [
        "# @title Autocorrelation Function (ACF) Visualization\n",
        "\n",
        "if not ACF_VIZ:\n",
        "  print(\"No ACF visualization requested\")\n",
        "  raise StopExecution\n",
        "\n",
        "# ACF heatmap for week data\n",
        "plt.figure(figsize=(10, 3))\n",
        "x_vals = np.concatenate(acfweek_all['HR'][::2])\n",
        "y_vals = np.concatenate(acfweek_all['HR'][1::2])\n",
        "max_lag = TIME_LEN // 2\n",
        "plt.hist2d(x_vals, y_vals, bins=[max_lag, 50], cmap='Blues', cmin=1)\n",
        "# Label every 360 minutes (for readability)\n",
        "time_labels = [\n",
        "    minutes_to_time(i) for i in range(0, max_lag + 1, 360)\n",
        "]  # Label every hour\n",
        "plt.xticks(np.arange(0, max_lag + 1, 360), time_labels)\n",
        "# Emphasize gridlines every 24 hours\n",
        "for i in range(0, max_lag + 1, 24 * 60):\n",
        "  plt.axvline(x=i, color='gray', linestyle='--', linewidth=2)\n",
        "plt.xlabel('Time Shifted [HH:MM]')\n",
        "plt.ylabel('Autocorrelation')\n",
        "plt.title('ACF Heatmap for Week-long HR')\n",
        "plt.colorbar(label='Frequency')  # Colorbar to indicate density\n",
        "plt.grid(axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ACF heatmap for day data\n",
        "plt.figure(figsize=(10, 3))\n",
        "x_vals = np.concatenate(acfday_all['HR'][::2])\n",
        "y_vals = np.concatenate(acfday_all['HR'][1::2])\n",
        "max_lag = 24 * 60 // 2\n",
        "plt.hist2d(x_vals, y_vals, bins=[max_lag, 50], cmap='Blues', cmin=1)\n",
        "# Label every 360 minutes (for readability)\n",
        "time_labels = [\n",
        "    minutes_to_time(i) for i in range(0, max_lag + 1, 360)\n",
        "]  # Label every hour\n",
        "plt.xticks(np.arange(0, max_lag + 1, 360), time_labels)\n",
        "# Emphasize gridlines every 24 hours\n",
        "for i in range(0, max_lag + 1, 24 * 60):\n",
        "  plt.axvline(x=i, color='gray', linestyle='--', linewidth=2)\n",
        "plt.xlabel('Time Shifted [HH:MM]')\n",
        "plt.ylabel('Autocorrelation')\n",
        "plt.title('ACF Heatmap for Week-long HR')\n",
        "plt.colorbar(label='Frequency')  # Colorbar to indicate density\n",
        "plt.grid(axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOySN4yP_dP2"
      },
      "outputs": [],
      "source": [
        "#@title Channel Specific Aggregate Spectrogram\n",
        "\n",
        "if not SPECTROGRAM_VIZ:\n",
        "  print(\"No spectrogram visualization requested\")\n",
        "  raise StopExecution\n",
        "\n",
        "for i in range(len(labels)):\n",
        "  plt.figure(figsize=(10,2))\n",
        "\n",
        "  c = plt.pcolormesh(t, f, Sxx_all[i,:], shading='gouraud', vmax=np.max(Sxx_all))\n",
        "  plt.colorbar(c)\n",
        "\n",
        "  # plt.ylabel('Freq [cpm]')\n",
        "  # plt.xlabel('Time [min]')\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "\n",
        "  # Add secondary x-axis for time of day (minutes from midnight to 11:59 PM)\n",
        "  timemax = 24*60\n",
        "  tickfreq = 180\n",
        "  tickamt = timemax//tickfreq\n",
        "  # Generate hourly ticks\n",
        "  hourly_ticks = np.linspace(30, 1408, tickamt+1)[1:][:-1]  # Tick every 60 minutes\n",
        "  hourly_labels = [minutes_to_time(x) for x in np.arange(tickfreq, timemax+tickfreq, tickfreq)][:-1]\n",
        "  # Add secondary x-axis with hourly ticks\n",
        "  ax = plt.gca()\n",
        "  ax2 = ax.secondary_xaxis('bottom')\n",
        "  ax2.set_xticks(hourly_ticks)\n",
        "  ax2.set_xticklabels(hourly_labels)\n",
        "  ax2.set_xlabel('Time of Day [HH:MM]')\n",
        "\n",
        "  plt.title(f\"{labels[i]} Spectrogram\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_0_QuwO8pEW"
      },
      "outputs": [],
      "source": [
        "#@title Missingness Gaps for Each Missingness Group\n",
        "\n",
        "if not MISSGAP_VIZ:\n",
        "  print(\"No missingness gap length visualization requested\")\n",
        "  raise StopExecution\n",
        "\n",
        "xticks = np.arange(0, 1441, 180)  # Every 3 hours\n",
        "xtick_labels = [f\"{hour:02d}:00\" for hour in range(0,24,3)]  # Hourly labels\n",
        "\n",
        "\n",
        "for miss_key in missgroup_dict.keys():\n",
        "  # plt.hist(np.concatenate(missingness_gaps[i]), bins=50);\n",
        "  # Plot the histogram\n",
        "  plt.figure(figsize=(8, 3))\n",
        "  data = np.concatenate(missgaps_all[miss_key])\n",
        "  counts, bins, patches = plt.hist(data, bins=1440, color='blue')#, edgecolor='black')\n",
        "  xticks = np.arange(0, bins[-1] + 1, bins[-1] // 8)  # Example: 10 evenly spaced ticks\n",
        "  xtick_labels = [f\"{int(x // 60):02d}:{int(x % 60):02d}\" for x in xticks]  # Convert to HH:MM\n",
        "  plt.xticks(xticks, xtick_labels, rotation=45)\n",
        "\n",
        "  # Add labels and show the plot\n",
        "  plt.xlabel(\"Time [HH:MM]\")\n",
        "  plt.ylabel(\"Frequency\")\n",
        "  plt.title(f\"{missgroup_dict[miss_key]['name']} Missingness Gap Length Histograms\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWDpF32QTRjV"
      },
      "outputs": [],
      "source": [
        "xticks = np.arange(0, 1441, 180)  # Every hour\n",
        "xtick_labels = [f\"{hour:02d}:00\" for hour in range(0,24,3)]  # Hourly labels\n",
        "\n",
        "\n",
        "for miss_key in missgroup_dict.keys():\n",
        "  # plt.hist(np.concatenate(missingness_gaps[i]), bins=50);\n",
        "  # Plot the histogram\n",
        "  plt.figure(figsize=(8, 3))\n",
        "  data = np.concatenate(missgaps_all[miss_key])\n",
        "  counts, bins, patches = plt.hist(data, bins=1440, color='blue')#, edgecolor='black')\n",
        "  xticks = np.arange(0, bins[-1] + 1, bins[-1] // 96 //  3)# 8)  # Example: 10 evenly spaced ticks\n",
        "  xtick_labels = [f\"{int(x // 60):02d}:{int(x % 60):02d}\" for x in xticks]  # Convert to HH:MM\n",
        "  plt.xticks(xticks, xtick_labels, rotation=45)\n",
        "  plt.xlim(0, 60)\n",
        "\n",
        "  # Add labels and show the plot\n",
        "  plt.xlabel(\"Time [HH:MM]\")\n",
        "  plt.ylabel(\"Frequency\")\n",
        "  plt.title(f\"{missgroup_dict[miss_key]['name']} Missingness Gap Length Histograms\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J7ZnGAYSVbu"
      },
      "source": [
        "# Sanity Checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o04MQ6UnScYe"
      },
      "outputs": [],
      "source": [
        "# @title CLT for Aggregate Statistics\n",
        "DATASET_NAME = 'lsm_v2_pretraining_n_200000_10080m'  # @param\n",
        "NUM_TRIALS = 5\n",
        "NUMPPL_PERTRIAL = np.array([10, 100, 1_000, 10_000, 100_000])\n",
        "\n",
        "data = tfds.load(\n",
        "    'lsm',\n",
        "    data_dir=f'{DEFAULT_DATA_ROOT}/{DATASET_NAME}',\n",
        "    split='train',\n",
        "    shuffle_files=False,\n",
        ")\n",
        "\n",
        "# Step 1: Compute and store the summary statistic `mean_missamt` for the dataset\n",
        "mean_missamt_list = []\n",
        "for sample in tqdm(\n",
        "    data.take(np.sum(NUMPPL_PERTRIAL) * NUM_TRIALS), desc='Processing Data'\n",
        "):\n",
        "  mask = tf.io.parse_tensor(sample['mask'], out_type=tf.bool).numpy()\n",
        "  mean_missamt = np.mean(\n",
        "      mask\n",
        "  )  # Compute the summary statistic (adjust calculation as needed)\n",
        "  mean_missamt_list.append(mean_missamt)\n",
        "\n",
        "# Convert the list to a NumPy array for efficient slicing\n",
        "mean_missamt_array = np.array(mean_missamt_list)\n",
        "\n",
        "# Step 2: Perform trials for each `num_ppl` in `num_ppl_list`\n",
        "sample_stats = []\n",
        "\n",
        "current_index = 0  # To keep track of the data index\n",
        "for n in tqdm(NUMPPL_PERTRIAL, desc=f'Processing trials'):\n",
        "  for _ in range(NUM_TRIALS):\n",
        "    # Slice the next `n` samples for the current trial\n",
        "    trial_samples = mean_missamt_array[current_index : current_index + n]\n",
        "    current_index += n\n",
        "\n",
        "    # Compute and store the sample mean\n",
        "    sample_mean = np.mean(trial_samples)\n",
        "    sample_stats.append(sample_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxzuQc9vWCcE"
      },
      "outputs": [],
      "source": [
        "num_ppl_list_repeat = np.repeat(NUMPPL_PERTRIAL, repeats=NUM_TRIALS)\n",
        "\n",
        "# Assuming num_ppl_list_repeat and sample_stats are defined\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(\n",
        "    num_ppl_list_repeat,\n",
        "    sample_stats,\n",
        "    alpha=0.25,\n",
        "    color=\"blue\",\n",
        "    label=\"Sample Statistics\",\n",
        ")\n",
        "\n",
        "# Set x-axis to log scale\n",
        "plt.xscale(\"log\")\n",
        "\n",
        "# Add labels, title, and legend\n",
        "plt.xlabel(\"Number of People (log scale)\")\n",
        "plt.ylabel(\"Mean % of Missingness \")\n",
        "plt.title(\"Mean % of Missingness vs. Sample Size (Log Scale)\")\n",
        "plt.legend()\n",
        "plt.axhline(np.mean(mean_missamt_list), c=\"red\")\n",
        "plt.grid(axis=\"y\", which=\"both\")\n",
        "plt.minorticks_on()\n",
        "\n",
        "print(f\"True Mean % of Missingness {np.mean(sample_stats)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foYJybnwUSCe"
      },
      "outputs": [],
      "source": [
        "# @title Z-norm statistics per channel\n",
        "DATASET_NAME = 'lsm_v2_pretraining_n_200000_10080m'  # @param\n",
        "NUM_PPL = 100_000\n",
        "NUM_CHANNELS = 32\n",
        "\n",
        "data = tfds.load(\n",
        "    'lsm',\n",
        "    data_dir=f'{DEFAULT_DATA_ROOT}/{DATASET_NAME}',\n",
        "    split='train',\n",
        "    shuffle_files=False,\n",
        ")\n",
        "\n",
        "channelmean_all = []\n",
        "channelstd_all = []\n",
        "\n",
        "for sample in tqdm(data.take(NUM_PPL)):\n",
        "  signal = (\n",
        "      tf.io.parse_tensor(sample['input_signal'], out_type=tf.float).numpy().T\n",
        "  )  # reshape(NUM_CHANNELS, 7, 10080//7)\n",
        "  mask = (\n",
        "      tf.io.parse_tensor(sample['mask'], out_type=tf.bool).numpy().T\n",
        "  )  # reshape(NUM_CHANNELS, 7, 10080//7)\n",
        "\n",
        "  # With z-norm happening before imputation\n",
        "  signal[mask] = np.nan\n",
        "  channelmean_all.append(np.nanmean(signal, axis=0))\n",
        "  channelstd_all.append(np.nanstd(signal, axis=0))\n",
        "  ### do not uncomment below code, this is for if z-norm happened after imputation\n",
        "  # channelmean_all.append(np.mean(signal, axis=0))\n",
        "  # channelstd_all.append(np.std(signal, axis=0))\n",
        "\n",
        "  gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwmHMshgSqMZ"
      },
      "outputs": [],
      "source": [
        "for i, name in enumerate(labels):\n",
        "  print(f\"Mean of means {name}: {np.nanmean(channelmean_all[i])}\")\n",
        "\n",
        "for i, name in enumerate(labels):\n",
        "  plt.figure(figsize=(5,2))\n",
        "  plt.hist(channelmean_all[i], bins=50)\n",
        "  plt.title(f\"Histogram of Mean of {name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4txw5_IEeaRx"
      },
      "outputs": [],
      "source": [
        "for i, name in enumerate(labels):\n",
        "  print(f\"Mean of Stds {name}: {np.nanmean(channelstd_all[i])}\")\n",
        "\n",
        "for i, name in enumerate(labels):\n",
        "  plt.figure(figsize=(5,2))\n",
        "  plt.hist(channelstd_all[i], bins=50)\n",
        "  plt.title(f\"Histogram of Std of {name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6A_TsLWhmVij"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/medical/waveforms/modelling/lsm/datasets/lsm/colabs/LSM_v2_data_investigation.ipynb",
          "timestamp": 1736790202458
        },
        {
          "file_id": "/piper/depot/google3/medical/waveforms/modelling/lsm/datasets/lsm/colabs/LSM_v2_data_investigation.ipynb",
          "timestamp": 1736731703577
        },
        {
          "file_id": "1bRgne2ysRTl-pbkBWetZZVIo9W2gllHw",
          "timestamp": 1734551140091
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
